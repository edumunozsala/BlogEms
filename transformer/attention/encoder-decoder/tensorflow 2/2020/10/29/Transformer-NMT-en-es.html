<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Attention is all you need: Discovering the Transformer model | Eduardo Muñoz NLP Blog</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Attention is all you need: Discovering the Transformer model" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Demystify the relevant artifacts in the paper Attention is all you need (Vaswani, Ashish &amp; Shazeer, Noam &amp; Parmar, Niki &amp; Uszkoreit, Jakob &amp; Jones, Llion &amp; Gomez, Aidan &amp; Kaiser, Lukasz &amp; Polosukhin, Illia. (2017)) and build a Transformer that we will apply to a small-scale NMT problem (Neural Machine Translation)" />
<meta property="og:description" content="Demystify the relevant artifacts in the paper Attention is all you need (Vaswani, Ashish &amp; Shazeer, Noam &amp; Parmar, Niki &amp; Uszkoreit, Jakob &amp; Jones, Llion &amp; Gomez, Aidan &amp; Kaiser, Lukasz &amp; Polosukhin, Illia. (2017)) and build a Transformer that we will apply to a small-scale NMT problem (Neural Machine Translation)" />
<link rel="canonical" href="https://edumunozsala.github.io/BlogEms/transformer/attention/encoder-decoder/tensorflow%202/2020/10/29/Transformer-NMT-en-es.html" />
<meta property="og:url" content="https://edumunozsala.github.io/BlogEms/transformer/attention/encoder-decoder/tensorflow%202/2020/10/29/Transformer-NMT-en-es.html" />
<meta property="og:site_name" content="Eduardo Muñoz NLP Blog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-10-29T00:00:00-05:00" />
<script type="application/ld+json">
{"mainEntityOfPage":{"@type":"WebPage","@id":"https://edumunozsala.github.io/BlogEms/transformer/attention/encoder-decoder/tensorflow%202/2020/10/29/Transformer-NMT-en-es.html"},"description":"Demystify the relevant artifacts in the paper Attention is all you need (Vaswani, Ashish &amp; Shazeer, Noam &amp; Parmar, Niki &amp; Uszkoreit, Jakob &amp; Jones, Llion &amp; Gomez, Aidan &amp; Kaiser, Lukasz &amp; Polosukhin, Illia. (2017)) and build a Transformer that we will apply to a small-scale NMT problem (Neural Machine Translation)","@type":"BlogPosting","url":"https://edumunozsala.github.io/BlogEms/transformer/attention/encoder-decoder/tensorflow%202/2020/10/29/Transformer-NMT-en-es.html","headline":"Attention is all you need: Discovering the Transformer model","dateModified":"2020-10-29T00:00:00-05:00","datePublished":"2020-10-29T00:00:00-05:00","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/BlogEms/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://edumunozsala.github.io/BlogEms/feed.xml" title="Eduardo Muñoz NLP Blog" /><link rel="shortcut icon" type="image/x-icon" href="/BlogEms/images/favicon.ico"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Attention is all you need: Discovering the Transformer model | Eduardo Muñoz NLP Blog</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Attention is all you need: Discovering the Transformer model" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Demystify the relevant artifacts in the paper Attention is all you need (Vaswani, Ashish &amp; Shazeer, Noam &amp; Parmar, Niki &amp; Uszkoreit, Jakob &amp; Jones, Llion &amp; Gomez, Aidan &amp; Kaiser, Lukasz &amp; Polosukhin, Illia. (2017)) and build a Transformer that we will apply to a small-scale NMT problem (Neural Machine Translation)" />
<meta property="og:description" content="Demystify the relevant artifacts in the paper Attention is all you need (Vaswani, Ashish &amp; Shazeer, Noam &amp; Parmar, Niki &amp; Uszkoreit, Jakob &amp; Jones, Llion &amp; Gomez, Aidan &amp; Kaiser, Lukasz &amp; Polosukhin, Illia. (2017)) and build a Transformer that we will apply to a small-scale NMT problem (Neural Machine Translation)" />
<link rel="canonical" href="https://edumunozsala.github.io/BlogEms/transformer/attention/encoder-decoder/tensorflow%202/2020/10/29/Transformer-NMT-en-es.html" />
<meta property="og:url" content="https://edumunozsala.github.io/BlogEms/transformer/attention/encoder-decoder/tensorflow%202/2020/10/29/Transformer-NMT-en-es.html" />
<meta property="og:site_name" content="Eduardo Muñoz NLP Blog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-10-29T00:00:00-05:00" />
<script type="application/ld+json">
{"mainEntityOfPage":{"@type":"WebPage","@id":"https://edumunozsala.github.io/BlogEms/transformer/attention/encoder-decoder/tensorflow%202/2020/10/29/Transformer-NMT-en-es.html"},"description":"Demystify the relevant artifacts in the paper Attention is all you need (Vaswani, Ashish &amp; Shazeer, Noam &amp; Parmar, Niki &amp; Uszkoreit, Jakob &amp; Jones, Llion &amp; Gomez, Aidan &amp; Kaiser, Lukasz &amp; Polosukhin, Illia. (2017)) and build a Transformer that we will apply to a small-scale NMT problem (Neural Machine Translation)","@type":"BlogPosting","url":"https://edumunozsala.github.io/BlogEms/transformer/attention/encoder-decoder/tensorflow%202/2020/10/29/Transformer-NMT-en-es.html","headline":"Attention is all you need: Discovering the Transformer model","dateModified":"2020-10-29T00:00:00-05:00","datePublished":"2020-10-29T00:00:00-05:00","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

<link href="https://unpkg.com/@primer/css/dist/primer.css" rel="stylesheet" />
<link rel="stylesheet" href="//use.fontawesome.com/releases/v5.0.7/css/all.css"><link type="application/atom+xml" rel="alternate" href="https://edumunozsala.github.io/BlogEms/feed.xml" title="Eduardo Muñoz NLP Blog" />
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"> </script>
    <script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head><body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/BlogEms/">Eduardo Muñoz NLP Blog</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/BlogEms/about/">About Me</a><a class="page-link" href="/BlogEms/search/">Search</a><a class="page-link" href="/BlogEms/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Attention is all you need: Discovering the Transformer model</h1><p class="page-description">Demystify the relevant artifacts in the paper Attention is all you need (Vaswani, Ashish & Shazeer, Noam & Parmar, Niki & Uszkoreit, Jakob & Jones, Llion & Gomez, Aidan & Kaiser, Lukasz & Polosukhin, Illia. (2017)) and build a Transformer that we will apply to a small-scale NMT problem (Neural Machine Translation)</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2020-10-29T00:00:00-05:00" itemprop="datePublished">
        Oct 29, 2020
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      36 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/BlogEms/categories/#Transformer">Transformer</a>
        &nbsp;
      
        <a class="category-tags-link" href="/BlogEms/categories/#attention">attention</a>
        &nbsp;
      
        <a class="category-tags-link" href="/BlogEms/categories/#encoder-decoder">encoder-decoder</a>
        &nbsp;
      
        <a class="category-tags-link" href="/BlogEms/categories/#Tensorflow 2">Tensorflow 2</a>
        
      
      </p>
    

    
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h1"><a href="#Neural-machine-traslation-using-a-Transformer-model">Neural machine traslation using a Transformer model </a>
<ul>
<li class="toc-entry toc-h2"><a href="#Why-we-need-Transformer?">Why we need Transformer? </a></li>
</ul>
</li>
<li class="toc-entry toc-h1"><a href="#Loading-the-libraries">Loading the libraries </a></li>
<li class="toc-entry toc-h1"><a href="#The-dataset-and-text-processing">The dataset and text processing </a>
<ul>
<li class="toc-entry toc-h2"><a href="#Loading-the-dataset">Loading the dataset </a></li>
</ul>
</li>
<li class="toc-entry toc-h1"><a href="#Tokenize-the-text-data">Tokenize the text data </a></li>
<li class="toc-entry toc-h1"><a href="#Create-the-batch-data-generator">Create the batch data generator </a></li>
<li class="toc-entry toc-h1"><a href="#Building-a-Transformer">Building a Transformer </a>
<ul>
<li class="toc-entry toc-h2"><a href="#What-is-Transformer?">What is Transformer? </a></li>
</ul>
</li>
<li class="toc-entry toc-h1"><a href="#Self-attention:-the-fundamental-operation">Self-attention: the fundamental operation </a>
<ul>
<li class="toc-entry toc-h2"><a href="#Queries,-Keys-and-Values">Queries, Keys and Values </a></li>
<li class="toc-entry toc-h2"><a href="#Scale-dot-product-Attention">Scale dot-product Attention </a></li>
<li class="toc-entry toc-h2"><a href="#Multi-Head-Attention">Multi-Head Attention </a></li>
</ul>
</li>
<li class="toc-entry toc-h1"><a href="#Positional-Encoding">Positional Encoding </a></li>
<li class="toc-entry toc-h1"><a href="#The-Encoder">The Encoder </a></li>
<li class="toc-entry toc-h1"><a href="#The-Decoder">The Decoder </a></li>
<li class="toc-entry toc-h1"><a href="#Transformer">Transformer </a></li>
<li class="toc-entry toc-h1"><a href="#Training-the-Transformer-model">Training the Transformer model </a>
<ul>
<li class="toc-entry toc-h2"><a href="#Show-some-results-from-training">Show some results from training </a></li>
</ul>
</li>
<li class="toc-entry toc-h1"><a href="#Make-predictions">Make predictions </a></li>
</ul><!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020-10-29-Transformer-NMT-en-es.ipynb
-->

<div class="container" id="notebook-container">
        
    
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Neural-machine-traslation-using-a-Transformer-model">
<a class="anchor" href="#Neural-machine-traslation-using-a-Transformer-model" aria-hidden="true"><span class="octicon octicon-link"></span></a>Neural machine traslation using a Transformer model<a class="anchor-link" href="#Neural-machine-traslation-using-a-Transformer-model"> </a>
</h1>
<p>In this notebook we will describe and demystify the relevant artifacts in the paper "Attention is all you need" (Vaswani, Ashish &amp; Shazeer, Noam &amp; Parmar, Niki &amp; Uszkoreit, Jakob &amp; Jones, Llion &amp; Gomez, Aidan &amp; Kaiser, Lukasz &amp; Polosukhin, Illia. (2017)). This paper was a more advanced step in the use of the attention mechanism being the main basis for a model called Transformer. The most famous current models that are emerging in NLP tasks consist of dozens of transformers or some of their variants, for example, GPT-3 or BERT.</p>
<p>Attention is all you need: <a href="https://arxiv.org/pdf/1706.03762.pdf">https://arxiv.org/pdf/1706.03762.pdf</a></p>
<p>We will describe the components of this model, analyze their operation and build a simple model that we will apply to a small-scale NMT problem (Neural Machine Translation). To read more about the problem that we will address and to know how the basic attention mechanism works, I recommend you to read my previous  post <a href="https://medium.com/better-programming/a-guide-on-the-encoder-decoder-model-and-the-attention-mechanism-401c836e2cdb"> "A Guide on the Encoder-Decoder Model and the Attention Mechanism"</a> and check the <a href="https://github.com/edumunozsala/NMT-encoder-decoder-Attention"> repository</a>.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="/BlogEms/images/copied_from_nb/images/transformer.jpg" alt="Alt" title="title Picture by Vinson Tan from Pixabay"></p>
<h2 id="Why-we-need-Transformer?">
<a class="anchor" href="#Why-we-need-Transformer?" aria-hidden="true"><span class="octicon octicon-link"></span></a>Why we need Transformer?<a class="anchor-link" href="#Why-we-need-Transformer?"> </a>
</h2>
<p>In sequence-to-sequence treatment problems such as the neural machine translation, the initial proposals were based on the use of RNNs in an encoder-decoder architecture. These architectures presented a great limitation when working with long sequences, their ability to retain information from the first elements was lost when new elements were incorporated into the sequence. In the encoder, the hidden state in every step is associated with a certain word in the input sentence, the more recent. Therefore, if the decoder only accesses the last hidden state of the decoder, it will lose relevant information about the first elements of the sequence. Then to deal with this limitation, a new concept were introduced <strong>the attention mechanism</strong>.</p>
<p>Instead of paying attention to the last state of the encoder as is usually done with the RNNs, in each step of the decoder we look at all the states of the encoder, being able to access information about all the elements of the input sequence. This is what attention does, it extracts information from the whole sequence, a weighted sum of all the past encoder states. This allows the decoder to assign greater weight or importance to a certain element of the input for each element of the output. Learning in every step to focus in the right element of the input to predict the next output element.</p>
<p>But this approach continues to have an important limitation, each sequence must be treated one at a time. Both the encoder and the decoder have to wait till the completion of t-1 steps to process at 't'th step. So when dealing with huge corpus <strong>it is very time consuming and computationally inefficient</strong>.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Loading-the-libraries">
<a class="anchor" href="#Loading-the-libraries" aria-hidden="true"><span class="octicon octicon-link"></span></a>Loading the libraries<a class="anchor-link" href="#Loading-the-libraries"> </a>
</h1>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Load and install, if necessary, the python libraries we will use along this notebook. We force to import Tensorflow version 2, if it is available.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">gc</span>
<span class="kn">import</span> <span class="nn">time</span>
<span class="kn">import</span> <span class="nn">re</span>

<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="o">%</span><span class="k">matplotlib</span> inline 

<span class="kn">from</span> <span class="nn">google.colab</span> <span class="kn">import</span> <span class="n">drive</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">try</span><span class="p">:</span>
    <span class="o">%</span><span class="k">tensorflow_version</span> 2.x
<span class="k">except</span><span class="p">:</span>
    <span class="k">pass</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>

<span class="kn">from</span> <span class="nn">tensorflow.keras</span> <span class="kn">import</span> <span class="n">layers</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras</span> <span class="kn">import</span> <span class="n">backend</span> <span class="k">as</span> <span class="n">K</span>
<span class="kn">import</span> <span class="nn">tensorflow_datasets</span> <span class="k">as</span> <span class="nn">tfds</span>

<span class="kn">from</span> <span class="nn">keras.preprocessing.text</span> <span class="kn">import</span> <span class="n">Tokenizer</span>
<span class="kn">from</span> <span class="nn">keras.preprocessing.sequence</span> <span class="kn">import</span> <span class="n">pad_sequences</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Setting some parameters and hyperparameters for our model</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Parameters for our model</span>
<span class="n">INPUT_COLUMN</span> <span class="o">=</span> <span class="s1">'input'</span>
<span class="n">TARGET_COLUMN</span> <span class="o">=</span> <span class="s1">'target'</span>
<span class="n">NUM_SAMPLES</span> <span class="o">=</span> <span class="mi">80000</span> <span class="c1">#40000</span>
<span class="n">MAX_VOCAB_SIZE</span> <span class="o">=</span> <span class="mi">2</span><span class="o">**</span><span class="mi">14</span>

<span class="n">BATCH_SIZE</span> <span class="o">=</span> <span class="mi">64</span>  <span class="c1"># Batch size for training.</span>
<span class="n">EPOCHS</span> <span class="o">=</span> <span class="mi">10</span>  <span class="c1"># Number of epochs to train for.</span>
<span class="n">MAX_LENGTH</span> <span class="o">=</span> <span class="mi">15</span>

<span class="c1"># Global parameters</span>
<span class="n">root_folder</span><span class="o">=</span><span class="s1">'/content/drive'</span>
<span class="n">data_folder_name</span><span class="o">=</span><span class="s1">'My Drive/datasets/eng_spa_translations'</span>
<span class="n">checkpoint_folder</span> <span class="o">=</span> <span class="s2">"My Drive/Projects/Transformer_NMT/ckpt/"</span>
<span class="n">train_filename</span><span class="o">=</span><span class="s1">'spa.txt'</span>

<span class="c1"># Variable for data directory</span>
<span class="n">DATA_PATH</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">abspath</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">root_folder</span><span class="p">,</span> <span class="n">data_folder_name</span><span class="p">))</span>
<span class="n">train_filenamepath</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">abspath</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">DATA_PATH</span><span class="p">,</span> <span class="n">train_filename</span><span class="p">))</span>
<span class="n">checkpoint_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">abspath</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">root_folder</span><span class="p">,</span> <span class="n">checkpoint_folder</span><span class="p">))</span>

<span class="c1"># Both train and test set are in the root data directory</span>
<span class="n">train_path</span> <span class="o">=</span> <span class="n">DATA_PATH</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="The-dataset-and-text-processing">
<a class="anchor" href="#The-dataset-and-text-processing" aria-hidden="true"><span class="octicon octicon-link"></span></a>The dataset and text processing<a class="anchor-link" href="#The-dataset-and-text-processing"> </a>
</h1>
<p>For this exercise we will use pairs of simple sentences, the source in English and target in Spanish, from the Tatoeba project where people contribute adding translations every day. This is the <a href="http://www.manythings.org/anki/">link</a> to some traslations in different languages. There you can download the Spanish - English spa_eng.zip file, it contains 124457 pairs of sentences.</p>
<p>We use a list of <strong>non breaking prefixes</strong> to avoid the tokenizer to split or break words including that prefixes. Inm our example we do not want to remove some the dot for some well-konw words.You can find non breaking prefixes for many languages in the Kaggle website:</p>
<p><a href="https://www.kaggle.com/nltkdata/nonbreaking-prefixes/activity">https://www.kaggle.com/nltkdata/nonbreaking-prefixes/activity</a></p>
<p>The text sentences are almost clean, they are simple plain text, so we only need to remove dots that are not a end of sentence symbol and duplicated white spaces.</p>
<p>The following functions will apply the cleaning mentioned previously:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">preprocess_text_nonbreaking</span><span class="p">(</span><span class="n">corpus</span><span class="p">,</span> <span class="n">non_breaking_prefixes</span><span class="p">):</span>
  <span class="n">corpus_cleaned</span> <span class="o">=</span> <span class="n">corpus</span>
  <span class="c1"># Add the string $$$ before the non breaking prefixes</span>
  <span class="c1"># To avoid remove dots from some words</span>
  <span class="k">for</span> <span class="n">prefix</span> <span class="ow">in</span> <span class="n">non_breaking_prefixes</span><span class="p">:</span>
    <span class="n">corpus_cleaned</span> <span class="o">=</span> <span class="n">corpus_cleaned</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="n">prefix</span><span class="p">,</span> <span class="n">prefix</span> <span class="o">+</span> <span class="s1">'$$$'</span><span class="p">)</span>
  <span class="c1"># Remove dots not at the end of a sentence</span>
  <span class="n">corpus_cleaned</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s2">"\.(?=[0-9]|[a-z]|[A-Z])"</span><span class="p">,</span> <span class="s2">".$$$"</span><span class="p">,</span> <span class="n">corpus_cleaned</span><span class="p">)</span>
  <span class="c1"># Remove the $$$ mark</span>
  <span class="n">corpus_cleaned</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s2">"\.\$\$\$"</span><span class="p">,</span> <span class="s1">''</span><span class="p">,</span> <span class="n">corpus_cleaned</span><span class="p">)</span>
  <span class="c1"># Rmove multiple white spaces</span>
  <span class="n">corpus_cleaned</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s2">"  +"</span><span class="p">,</span> <span class="s2">" "</span><span class="p">,</span> <span class="n">corpus_cleaned</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">corpus_cleaned</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Loading-the-dataset">
<a class="anchor" href="#Loading-the-dataset" aria-hidden="true"><span class="octicon octicon-link"></span></a>Loading the dataset<a class="anchor-link" href="#Loading-the-dataset"> </a>
</h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Mount our Google Drive unit to access the datafiles from the notebook</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">drive</span><span class="o">.</span><span class="n">mount</span><span class="p">(</span><span class="s2">"/content/drive"</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Mounted at /content/drive
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Loading the list of non breaking prefixes for the english and the spanish sentences</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">DATA_PATH</span><span class="o">+</span><span class="s2">"/nonbreaking_prefix.en"</span><span class="p">,</span> 
          <span class="n">mode</span> <span class="o">=</span> <span class="s2">"r"</span><span class="p">,</span> <span class="n">encoding</span> <span class="o">=</span> <span class="s2">"utf-8"</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">non_breaking_prefix_en</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">read</span><span class="p">()</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">DATA_PATH</span><span class="o">+</span><span class="s2">"/nonbreaking_prefix.es"</span><span class="p">,</span> 
          <span class="n">mode</span> <span class="o">=</span> <span class="s2">"r"</span><span class="p">,</span> <span class="n">encoding</span> <span class="o">=</span> <span class="s2">"utf-8"</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">non_breaking_prefix_es</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">read</span><span class="p">()</span>

<span class="n">non_breaking_prefix_en</span> <span class="o">=</span> <span class="n">non_breaking_prefix_en</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">"</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>
<span class="n">non_breaking_prefix_en</span> <span class="o">=</span> <span class="p">[</span><span class="s1">' '</span> <span class="o">+</span> <span class="n">pref</span> <span class="o">+</span> <span class="s1">'.'</span> <span class="k">for</span> <span class="n">pref</span> <span class="ow">in</span> <span class="n">non_breaking_prefix_en</span><span class="p">]</span>
<span class="n">non_breaking_prefix_es</span> <span class="o">=</span> <span class="n">non_breaking_prefix_es</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">"</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>
<span class="n">non_breaking_prefix_es</span> <span class="o">=</span> <span class="p">[</span><span class="s1">' '</span> <span class="o">+</span> <span class="n">pref</span> <span class="o">+</span> <span class="s1">'.'</span> <span class="k">for</span> <span class="n">pref</span> <span class="ow">in</span> <span class="n">non_breaking_prefix_es</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Load the dataset into a pandas dataframe and apply the preprocess function to the input and target columns.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Load the dataset: sentence in english, sentence in spanish </span>
<span class="n">df</span><span class="o">=</span><span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">train_filenamepath</span><span class="p">,</span> <span class="n">sep</span><span class="o">=</span><span class="s2">"</span><span class="se">\t</span><span class="s2">"</span><span class="p">,</span> <span class="n">header</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">names</span><span class="o">=</span><span class="p">[</span><span class="n">INPUT_COLUMN</span><span class="p">,</span><span class="n">TARGET_COLUMN</span><span class="p">],</span> <span class="n">usecols</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> 
               <span class="n">nrows</span><span class="o">=</span><span class="n">NUM_SAMPLES</span><span class="p">)</span>
<span class="c1"># Preprocess the input data</span>
<span class="n">input_data</span><span class="o">=</span><span class="n">df</span><span class="p">[</span><span class="n">INPUT_COLUMN</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span> <span class="p">:</span> <span class="n">preprocess_text_nonbreaking</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">non_breaking_prefix_en</span><span class="p">))</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
<span class="c1"># Preprocess and include the end of sentence token to the target text</span>
<span class="n">target_data</span><span class="o">=</span><span class="n">df</span><span class="p">[</span><span class="n">TARGET_COLUMN</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span> <span class="p">:</span> <span class="n">preprocess_text_nonbreaking</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">non_breaking_prefix_es</span><span class="p">))</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">'Number of sentences: '</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">input_data</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">input_data</span><span class="p">[:</span><span class="mi">5</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">target_data</span><span class="p">[:</span><span class="mi">5</span><span class="p">])</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Number of sentences:  80000
['Go.', 'Go.', 'Go.', 'Go.', 'Hi.']
['Ve.', 'Vete.', 'Vaya.', 'Váyase.', 'Hola.']
</pre>
</div>
</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1">#Delete the dataframe and release the memory (if it is possible)</span>
<span class="k">del</span> <span class="n">df</span>
<span class="n">gc</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>0</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Tokenize-the-text-data">
<a class="anchor" href="#Tokenize-the-text-data" aria-hidden="true"><span class="octicon octicon-link"></span></a>Tokenize the text data<a class="anchor-link" href="#Tokenize-the-text-data"> </a>
</h1>
<p>Next, let's see how to prepare the data for our model. It is very simple and the steps are the following:</p>
<ul>
<li>Create the vocabulary from the corpus using Subword tokenization, breaking words into “subword units” - strings of characters like ing or eau - that allow the downstream model to make intelligent decisions on words it doesn’t recognize.</li>
<li>Calculate the maximum length of the input and output sequences.</li>
<li>Tokenize the data, convert the raw text into a sequence of integers. Once we define the vocabulary, we use the encode method to get the token for every word in the corpus.</li>
<li>Remove sentences longer that the max length defined.</li>
<li>Padding the sentences: we need to pad zeros at the end of the sequences so that all sequences have the same length. Otherwise, we won't be able train the model on batches</li>
</ul>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">subword_tokenize</span><span class="p">(</span><span class="n">corpus</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">max_length</span><span class="p">):</span>
  <span class="c1"># Create the vocabulary using Subword tokenization</span>
  <span class="n">tokenizer_corpus</span> <span class="o">=</span> <span class="n">tfds</span><span class="o">.</span><span class="n">deprecated</span><span class="o">.</span><span class="n">text</span><span class="o">.</span><span class="n">SubwordTextEncoder</span><span class="o">.</span><span class="n">build_from_corpus</span><span class="p">(</span>
    <span class="n">corpus</span><span class="p">,</span> <span class="n">target_vocab_size</span><span class="o">=</span><span class="n">vocab_size</span><span class="p">)</span>
  <span class="c1"># Get the final vocab size, adding the eos and sos tokens</span>
  <span class="n">num_words</span> <span class="o">=</span> <span class="n">tokenizer_corpus</span><span class="o">.</span><span class="n">vocab_size</span> <span class="o">+</span> <span class="mi">2</span>
  <span class="c1"># Set eos and sos token</span>
  <span class="n">sos_token</span> <span class="o">=</span> <span class="p">[</span><span class="n">num_words</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span>
  <span class="n">eos_token</span> <span class="o">=</span> <span class="p">[</span><span class="n">num_words</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
  <span class="c1"># Tokenize the corpus</span>
  <span class="n">sentences</span> <span class="o">=</span> <span class="p">[</span><span class="n">sos_token</span> <span class="o">+</span> <span class="n">tokenizer_corpus</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">sentence</span><span class="p">)</span> <span class="o">+</span> <span class="n">eos_token</span>
          <span class="k">for</span> <span class="n">sentence</span> <span class="ow">in</span> <span class="n">corpus</span><span class="p">]</span>
  <span class="c1"># Identify the index of the sentences longer than max length</span>
  <span class="n">idx_to_remove</span> <span class="o">=</span> <span class="p">[</span><span class="n">count</span> <span class="k">for</span> <span class="n">count</span><span class="p">,</span> <span class="n">sent</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">sentences</span><span class="p">)</span>
                 <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">sent</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">max_length</span><span class="p">]</span>
  <span class="c1">#Pad the sentences</span>
  <span class="n">sentences</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">preprocessing</span><span class="o">.</span><span class="n">sequence</span><span class="o">.</span><span class="n">pad_sequences</span><span class="p">(</span><span class="n">sentences</span><span class="p">,</span>
                                                       <span class="n">value</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                                                       <span class="n">padding</span><span class="o">=</span><span class="s1">'post'</span><span class="p">,</span>
                                                       <span class="n">maxlen</span><span class="o">=</span><span class="n">max_length</span><span class="p">)</span>
  
  <span class="k">return</span> <span class="n">sentences</span><span class="p">,</span> <span class="n">tokenizer_corpus</span><span class="p">,</span> <span class="n">num_words</span><span class="p">,</span> <span class="n">sos_token</span><span class="p">,</span> <span class="n">eos_token</span><span class="p">,</span> <span class="n">idx_to_remove</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Tokenize and pad the input sequences</span>
<span class="n">encoder_inputs</span><span class="p">,</span> <span class="n">tokenizer_inputs</span><span class="p">,</span> <span class="n">num_words_inputs</span><span class="p">,</span> <span class="n">sos_token_input</span><span class="p">,</span> <span class="n">eos_token_input</span><span class="p">,</span> <span class="n">del_idx_inputs</span><span class="o">=</span> <span class="n">subword_tokenize</span><span class="p">(</span><span class="n">input_data</span><span class="p">,</span> 
                                                                                                        <span class="n">MAX_VOCAB_SIZE</span><span class="p">,</span> <span class="n">MAX_LENGTH</span><span class="p">)</span>
<span class="c1"># Tokenize and pad the outputs sequences</span>
<span class="n">decoder_outputs</span><span class="p">,</span> <span class="n">tokenizer_outputs</span><span class="p">,</span> <span class="n">num_words_output</span><span class="p">,</span> <span class="n">sos_token_output</span><span class="p">,</span> <span class="n">eos_token_output</span><span class="p">,</span> <span class="n">del_idx_outputs</span> <span class="o">=</span> <span class="n">subword_tokenize</span><span class="p">(</span><span class="n">target_data</span><span class="p">,</span> 
                                                                                                        <span class="n">MAX_VOCAB_SIZE</span><span class="p">,</span> <span class="n">MAX_LENGTH</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Check the tokenize function</span>
<span class="nb">print</span><span class="p">(</span><span class="n">encoder_inputs</span><span class="p">[:</span><span class="mi">5</span><span class="p">],</span> <span class="n">sos_token_input</span><span class="p">,</span> <span class="n">eos_token_input</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">decoder_outputs</span><span class="p">[:</span><span class="mi">5</span><span class="p">],</span> <span class="n">sos_token_output</span><span class="p">,</span> <span class="n">eos_token_output</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>[[14050  5483 13840 14051     0     0     0     0     0     0     0     0
      0     0     0]
 [14050  5483 13840 14051     0     0     0     0     0     0     0     0
      0     0     0]
 [14050  5483 13840 14051     0     0     0     0     0     0     0     0
      0     0     0]
 [14050  5483 13840 14051     0     0     0     0     0     0     0     0
      0     0     0]
 [14050  2180 13840 14051     0     0     0     0     0     0     0     0
      0     0     0]] [14050] [14051]
[[15568 10468 15358 15569     0     0     0     0     0     0     0     0
      0     0     0]
 [15568 10468   263 15358 15569     0     0     0     0     0     0     0
      0     0     0]
 [15568 10469 15358 15569     0     0     0     0     0     0     0     0
      0     0     0]
 [15568 14581 15358 15569     0     0     0     0     0     0     0     0
      0     0     0]
 [15568  2976 15358 15569     0     0     0     0     0     0     0     0
      0     0     0]] [15568] [15569]
</pre>
</div>
</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">'Size of Input Vocabulary: '</span><span class="p">,</span> <span class="n">num_words_inputs</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'Size of Output Vocabulary: '</span><span class="p">,</span> <span class="n">num_words_output</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Size of Input Vocabulary:  14052
Size of Output Vocabulary:  15570
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Create-the-batch-data-generator">
<a class="anchor" href="#Create-the-batch-data-generator" aria-hidden="true"><span class="octicon octicon-link"></span></a>Create the batch data generator<a class="anchor-link" href="#Create-the-batch-data-generator"> </a>
</h1>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>Create a batch data generator: we want to train the model on batches, group of sentences, so we need to create a Dataset using the tf.data library and the function batch_on_slices on the input and output sequences.</li>
</ul>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Define a dataset </span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Dataset</span><span class="o">.</span><span class="n">from_tensor_slices</span><span class="p">(</span>
    <span class="p">(</span><span class="n">encoder_inputs</span><span class="p">,</span> <span class="n">decoder_outputs</span><span class="p">))</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">input_data</span><span class="p">),</span> <span class="n">reshuffle_each_iteration</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span>
    <span class="n">BATCH_SIZE</span><span class="p">,</span> <span class="n">drop_remainder</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">prefetch</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">experimental</span><span class="o">.</span><span class="n">AUTOTUNE</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Building-a-Transformer">
<a class="anchor" href="#Building-a-Transformer" aria-hidden="true"><span class="octicon octicon-link"></span></a>Building a Transformer<a class="anchor-link" href="#Building-a-Transformer"> </a>
</h1>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="What-is-Transformer?">
<a class="anchor" href="#What-is-Transformer?" aria-hidden="true"><span class="octicon octicon-link"></span></a>What is Transformer?<a class="anchor-link" href="#What-is-Transformer?"> </a>
</h2>
<p><em>"In this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output. The Transformer allows for significantly more parallelization … the Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence-aligned RNNs or convolution."</em></p>
<p>"Attention is all you need" paper</p>
<p>The Transformer model extract  features for each word using a self-attention mechanism to figure out how important all the other words in the sentence are w.r.t. to the aforementioned word. And no recurrent units are used to obtain this features, they are just weighted sum and activations, so they can be very parallelizable and efficient.</p>
<p>But We will dive deeper to understand what all this means.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Self-attention:-the-fundamental-operation">
<a class="anchor" href="#Self-attention:-the-fundamental-operation" aria-hidden="true"><span class="octicon octicon-link"></span></a>Self-attention: the fundamental operation<a class="anchor-link" href="#Self-attention:-the-fundamental-operation"> </a>
</h1>
<p><em>"Self-attention is a sequence-to-sequence operation: a sequence of vectors goes in, and a sequence of vectors comes out. Let's call the input vectors 𝐱1,𝐱2,…𝐱t and the corresponding output vectors 𝐲1,𝐲2,…,𝐲t. The vectors all have dimension k. To produce output vector 𝐲i, the self attention operation simply takes a weighted average over all the input vectors, the simplest option is the dot product."</em></p>
<p><strong>Transformers from scratch by Peter Bloem</strong></p>
<p>In the self-attention mechanism of our model we need to introduce three elements: Queries, Values and Keys</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Queries,-Keys-and-Values">
<a class="anchor" href="#Queries,-Keys-and-Values" aria-hidden="true"><span class="octicon octicon-link"></span></a>Queries, Keys and Values<a class="anchor-link" href="#Queries,-Keys-and-Values"> </a>
</h2>
<p>Every input vector is used in three different ways in the self-attention mechanism: the Query, the Key and the Value. In every role, it is compared to the others vectors to get its own output yi (Query), to get the j-th output yj (Key) and to compute each output vector once the weights have been established (Value).</p>
<p>To obtain this roles, we need three weight matrices of dimensions k x k and compute three linear transformation for each xi:</p>
<p><img src="/BlogEms/images/copied_from_nb/images/query_key_value.png" alt="Alt" title="title Transformers from scratch by Peter Bloem"></p>
<p>These three matrices are usually known as K, Q and V, three learnable weight layers that are applied to the same encoded input. Consequently, as each of these three matrices come from the same input, we can apply the attention mechanism of the input vector with itself, a "self-attention".</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Scale-dot-product-Attention">
<a class="anchor" href="#Scale-dot-product-Attention" aria-hidden="true"><span class="octicon octicon-link"></span></a>Scale dot-product Attention<a class="anchor-link" href="#Scale-dot-product-Attention"> </a>
</h2>
<p><em>The input consists of queries and keys of dimension dk, and values of dimension dv. We compute the dot products of the query with all keys, divide each by the square root of dk, and apply a softmax function to obtain the weights on the values.</em></p>
<p>"Attention is all you need" paper</p>
<p>Then we use the Q, K and V matrices to calculate the attention scores. <strong>The scores measure how much focus to place on other places or words of the input sequence w.r.t a word at a certain position</strong>. That is, the dot product of the query vector with the key vector of the respective word we're scoring. So, for position 1 we calculate the dot product (.) of q1 and k1, then q1 . k2, q1 . k3,… </p>
<p>Next we apply the "scaled" factor to have more stable gradients. The softmax function can not work properly with large values, resulting in vanishing the gradient and slow down the learning. After "softmaxing" we multiply by the Value matrix to keep the values of the words we want to focus on and minimizing or removing the values for the irrelevant words (its value in V matrix should be very small).</p>
<p>The formula for these operations is:</p>
<p>$Attention(Q, K, V ) = \text{softmax}\left(\dfrac{QK^T}{\sqrt{d_k}}\right)V $</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">scaled_dot_product_attention</span><span class="p">(</span><span class="n">queries</span><span class="p">,</span> <span class="n">keys</span><span class="p">,</span> <span class="n">values</span><span class="p">,</span> <span class="n">mask</span><span class="p">):</span>
    <span class="c1"># Calculate the dot product, QK_transpose</span>
    <span class="n">product</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">queries</span><span class="p">,</span> <span class="n">keys</span><span class="p">,</span> <span class="n">transpose_b</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="c1"># Get the scale factor</span>
    <span class="n">keys_dim</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">keys</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="c1"># Apply the scale factor to the dot product</span>
    <span class="n">scaled_product</span> <span class="o">=</span> <span class="n">product</span> <span class="o">/</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">keys_dim</span><span class="p">)</span>
    <span class="c1"># Apply masking when it is requiered</span>
    <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">scaled_product</span> <span class="o">+=</span> <span class="p">(</span><span class="n">mask</span> <span class="o">*</span> <span class="o">-</span><span class="mf">1e9</span><span class="p">)</span>
    <span class="c1"># dot product with Values</span>
    <span class="n">attention</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">scaled_product</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">),</span> <span class="n">values</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">attention</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Multi-Head-Attention">
<a class="anchor" href="#Multi-Head-Attention" aria-hidden="true"><span class="octicon octicon-link"></span></a>Multi-Head Attention<a class="anchor-link" href="#Multi-Head-Attention"> </a>
</h2>
<p>In the previous description the attention scores are focused on the whole sentence at a time, this would produce the same results even if two sentences contain the same words in a different order. Instead, we would like to attend to different segments of the words. We can give the self attention greater power of discrimination, <strong>by combining several self attention heads</strong>, dividing the words vectors into a fixed number (h, number of heads) of chunks, and then self-attention is applied on the corresponding chunks, using Q, K and V sub-matrices.</p>
<p>This produce h different output matrices of scores.</p>
<p><img src="/BlogEms/images/copied_from_nb/images/dor_product_multihead.PNG" alt="Alt" title="title From Attention is all you need paper by Vaswani, et al., 2017"></p>
<p>But the next layer (the Feed-Forward layer) is expecting just one matrix, a vector for each word, so after calculating the dot product of every head, we concat the output matrices and multiply them by an additional weights matrix $W_O$. This final matrix captures information from all the attention heads.</p>
<p>$MultihHead(Q, K, V ) = \text{Concat}(head_1,...,head_n)W^O$</p>
<p>where $head_i=Attention(QW_i^Q,QW_i^K,QW_i^V)$ and $i$ is the head index.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">MultiHeadAttention</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span><span class="p">):</span>
    
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MultiHeadAttention</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span> <span class="o">=</span> <span class="n">n_heads</span>
        
    <span class="k">def</span> <span class="nf">build</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span> <span class="o">=</span> <span class="n">input_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span> <span class="o">==</span> <span class="mi">0</span>
        <span class="c1"># Calculate the dimension of every head or projection</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">d_head</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span>
        <span class="c1"># Set the weight matrices for Q, K and V</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">query_lin</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">d_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">key_lin</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">d_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">value_lin</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">d_model</span><span class="p">)</span>
        <span class="c1"># Set the weight matrix for the output of the multi-head attention W0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">final_lin</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">d_model</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">split_proj</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span> <span class="c1"># inputs: (batch_size, seq_length, d_model)</span>
        <span class="c1"># Set the dimension of the projections</span>
        <span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span>
                 <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
                 <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span><span class="p">,</span>
                 <span class="bp">self</span><span class="o">.</span><span class="n">d_head</span><span class="p">)</span>
        <span class="c1"># Split the input vectors</span>
        <span class="n">splited_inputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="n">shape</span><span class="p">)</span> <span class="c1"># (batch_size, seq_length, nb_proj, d_proj)</span>
        <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">splited_inputs</span><span class="p">,</span> <span class="n">perm</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span> <span class="c1"># (batch_size, nb_proj, seq_length, d_proj)</span>
    
    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">queries</span><span class="p">,</span> <span class="n">keys</span><span class="p">,</span> <span class="n">values</span><span class="p">,</span> <span class="n">mask</span><span class="p">):</span>
        <span class="c1"># Get the batch size</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">queries</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        <span class="c1"># Set the Query, Key and Value matrices</span>
        <span class="n">queries</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">query_lin</span><span class="p">(</span><span class="n">queries</span><span class="p">)</span>
        <span class="n">keys</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">key_lin</span><span class="p">(</span><span class="n">keys</span><span class="p">)</span>
        <span class="n">values</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">value_lin</span><span class="p">(</span><span class="n">values</span><span class="p">)</span>
        <span class="c1"># Split Q, K y V between the heads or projections</span>
        <span class="n">queries</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">split_proj</span><span class="p">(</span><span class="n">queries</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>
        <span class="n">keys</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">split_proj</span><span class="p">(</span><span class="n">keys</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>
        <span class="n">values</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">split_proj</span><span class="p">(</span><span class="n">values</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>
        <span class="c1"># Apply the scaled dot product</span>
        <span class="n">attention</span> <span class="o">=</span> <span class="n">scaled_dot_product_attention</span><span class="p">(</span><span class="n">queries</span><span class="p">,</span> <span class="n">keys</span><span class="p">,</span> <span class="n">values</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>
        <span class="c1"># Get the attention scores</span>
        <span class="n">attention</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">attention</span><span class="p">,</span> <span class="n">perm</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
        <span class="c1"># Concat the h heads or projections</span>
        <span class="n">concat_attention</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">attention</span><span class="p">,</span>
                                      <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span><span class="p">))</span>
        <span class="c1"># Apply W0 to get the output of the multi-head attention</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">final_lin</span><span class="p">(</span><span class="n">concat_attention</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">outputs</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Positional-Encoding">
<a class="anchor" href="#Positional-Encoding" aria-hidden="true"><span class="octicon octicon-link"></span></a>Positional Encoding<a class="anchor-link" href="#Positional-Encoding"> </a>
</h1>
<p>We mentioned briefly that the order of the words in the sentence is an issue to solve in this model, because the network and the self-attention mechanism is permutation invariant. If we shuffle up the words in the input sentence, we get the same solutions. We need to create a representation of the position of the word in the sentence and add it to the word embedding.</p>
<p><em>To this end, we add "positional encodings" to the input embeddings at the bottoms of the encoder and decoder stacks. The positional encodings have the same dimension as the embeddings, so that the two can be summed. There are many choices of positional encodings.</em></p>
<p>"Attention is all you need" paper</p>
<p>So, we apply a function to map the position in the sentence to real valued vector. The network will learn how to use this information. Another approach would be to use a position embedding, similar to word embedding, coding every known position with a vector. It would requiere sentences of all accepted positions during training but positional encoding allow the model to extrapolate to sequence lengths longer than the ones encountered.</p>
<p>In the paper a sinusoidal function is applied:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>$PE_{(pos,2i)} =\sin(pos/10000^{2i/dmodel})$</p>
<p>$PE_{(pos,2i+1)} =\cos(pos/10000^{2i/dmodel})$</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">PositionalEncoding</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span><span class="p">):</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">PositionalEncoding</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    
    <span class="k">def</span> <span class="nf">get_angles</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pos</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">d_model</span><span class="p">):</span> <span class="c1"># pos: (seq_length, 1) i: (1, d_model)</span>
        <span class="n">angles</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">power</span><span class="p">(</span><span class="mf">10000.</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="p">(</span><span class="n">i</span><span class="o">//</span><span class="mi">2</span><span class="p">))</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="n">d_model</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">pos</span> <span class="o">*</span> <span class="n">angles</span> <span class="c1"># (seq_length, d_model)</span>

    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
        <span class="c1"># input shape batch_size, seq_length, d_model</span>
        <span class="n">seq_length</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">as_list</span><span class="p">()[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span>
        <span class="n">d_model</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">as_list</span><span class="p">()[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="c1"># Calculate the angles given the input</span>
        <span class="n">angles</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_angles</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">seq_length</span><span class="p">)[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">],</span>
                                 <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">d_model</span><span class="p">)[</span><span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">,</span> <span class="p">:],</span>
                                 <span class="n">d_model</span><span class="p">)</span>
        <span class="c1"># Calculate the positional encodings</span>
        <span class="n">angles</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">angles</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">::</span><span class="mi">2</span><span class="p">])</span>
        <span class="n">angles</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">angles</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">])</span>
        <span class="c1"># Expand the encodings with a new dimension</span>
        <span class="n">pos_encoding</span> <span class="o">=</span> <span class="n">angles</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">,</span> <span class="o">...</span><span class="p">]</span>
        
        <span class="k">return</span> <span class="n">inputs</span> <span class="o">+</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">pos_encoding</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="The-Encoder">
<a class="anchor" href="#The-Encoder" aria-hidden="true"><span class="octicon octicon-link"></span></a>The Encoder<a class="anchor-link" href="#The-Encoder"> </a>
</h1>
<p>Now that all the main pieces of the model have been described we can introduce the encoder components. </p>
<ul>
<li>
<p>Positional encoding: Add the position encoding to the input embedding (our input words are transformed to embedding vectors). <em>"The same weight matrix is shared between the two embedding layers (encoder and decoder) and the pre-softmax linear transformation. In the embedding layers, we multiply those weights by square root of the model dimension"</em> [1], ${\sqrt{d_{model}}}$.</p>
</li>
<li>
<p>N = 6, identical layers, containing two sub-layers: a <strong>multi-head self-attention mechanism</strong>, and a <strong>fully connected feed-forward network</strong>. This FC layer is applied to each position separately and identically and consists of two linear transformations with a ReLU activation in between. But it is applied position-wise to the input, which means that the same neural network is applied to every single "token" vector belonging to the sentence sequence.</p>
</li>
</ul>
<p>
$$FFN(x)= max(0,xW_1+b_1)W_2+b_2$$
</p>
<ul>
<li>There is a residual connection around each sub-layer (attention and FC network) followed by a layer normalization.</li>
</ul>
<p><em>Normalization and residual connections are standard tricks used to help deep neural networks train faster and more accurately. The layer normalization is applied over the embedding dimension only.</em></p>
<p><strong>Peter Bloem, "Transformers from scratch"</strong></p>
<p>The next figure will show the components detailed:</p>
<p><img src="/BlogEms/images/copied_from_nb/images/encoder.PNG" alt="Alt" title="title The Ilustrated Transformer by Jay Alammar"></p>
<p>Keep in mind that <strong>only the vector from the last layer (6-th) is sent to the decoder</strong>.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">EncoderLayer</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span><span class="p">):</span>
    
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">FFN_units</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">,</span> <span class="n">dropout_rate</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">EncoderLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="c1"># Hidden units of the feed forward component</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">FFN_units</span> <span class="o">=</span> <span class="n">FFN_units</span>
        <span class="c1"># Set the number of projectios or heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span> <span class="o">=</span> <span class="n">n_heads</span>
        <span class="c1"># Dropout rate</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout_rate</span> <span class="o">=</span> <span class="n">dropout_rate</span>
    
    <span class="k">def</span> <span class="nf">build</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span> <span class="o">=</span> <span class="n">input_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="c1"># Build the multihead layer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">multi_head_attention</span> <span class="o">=</span> <span class="n">MultiHeadAttention</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout_1</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">rate</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dropout_rate</span><span class="p">)</span>
        <span class="c1"># Layer Normalization</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">norm_1</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">LayerNormalization</span><span class="p">(</span><span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">)</span>
        <span class="c1"># Fully connected feed forward layer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ffn1_relu</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">FFN_units</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">"relu"</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ffn2</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">d_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout_2</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">rate</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dropout_rate</span><span class="p">)</span>
        <span class="c1"># Layer normalization</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">norm_2</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">LayerNormalization</span><span class="p">(</span><span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">training</span><span class="p">):</span>
        <span class="c1"># Forward pass of the multi-head attention</span>
        <span class="n">attention</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">multi_head_attention</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span>
                                              <span class="n">inputs</span><span class="p">,</span>
                                              <span class="n">inputs</span><span class="p">,</span>
                                              <span class="n">mask</span><span class="p">)</span>
        <span class="n">attention</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout_1</span><span class="p">(</span><span class="n">attention</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="n">training</span><span class="p">)</span>
        <span class="c1"># Call to the residual connection and layer normalization</span>
        <span class="n">attention</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm_1</span><span class="p">(</span><span class="n">attention</span> <span class="o">+</span> <span class="n">inputs</span><span class="p">)</span>
        <span class="c1"># Call to the FC layer</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ffn1_relu</span><span class="p">(</span><span class="n">attention</span><span class="p">)</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ffn2</span><span class="p">(</span><span class="n">outputs</span><span class="p">)</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout_2</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="n">training</span><span class="p">)</span>
        <span class="c1"># Call to residual connection and the layer normalization</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm_2</span><span class="p">(</span><span class="n">outputs</span> <span class="o">+</span> <span class="n">attention</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">outputs</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">Encoder</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span><span class="p">):</span>
    
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">n_layers</span><span class="p">,</span>
                 <span class="n">FFN_units</span><span class="p">,</span>
                 <span class="n">n_heads</span><span class="p">,</span>
                 <span class="n">dropout_rate</span><span class="p">,</span>
                 <span class="n">vocab_size</span><span class="p">,</span>
                 <span class="n">d_model</span><span class="p">,</span>
                 <span class="n">name</span><span class="o">=</span><span class="s2">"encoder"</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Encoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_layers</span> <span class="o">=</span> <span class="n">n_layers</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span> <span class="o">=</span> <span class="n">d_model</span>
        <span class="c1"># The embedding layer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="c1"># Positional encoding layer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pos_encoding</span> <span class="o">=</span> <span class="n">PositionalEncoding</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">rate</span><span class="o">=</span><span class="n">dropout_rate</span><span class="p">)</span>
        <span class="c1"># Stack of n layers of multi-head attention and FC</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">enc_layers</span> <span class="o">=</span> <span class="p">[</span><span class="n">EncoderLayer</span><span class="p">(</span><span class="n">FFN_units</span><span class="p">,</span>
                                        <span class="n">n_heads</span><span class="p">,</span>
                                        <span class="n">dropout_rate</span><span class="p">)</span> 
                           <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_layers</span><span class="p">)]</span>
    
    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">training</span><span class="p">):</span>
        <span class="c1"># Get the embedding vectors</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
        <span class="c1"># Scale the embeddings by sqrt of d_model</span>
        <span class="n">outputs</span> <span class="o">*=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">d_model</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
        <span class="c1"># Positional encodding</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pos_encoding</span><span class="p">(</span><span class="n">outputs</span><span class="p">)</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">training</span><span class="p">)</span>
        <span class="c1"># Call the stacked layers</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_layers</span><span class="p">):</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">enc_layers</span><span class="p">[</span><span class="n">i</span><span class="p">](</span><span class="n">outputs</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">training</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">outputs</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="The-Decoder">
<a class="anchor" href="#The-Decoder" aria-hidden="true"><span class="octicon octicon-link"></span></a>The Decoder<a class="anchor-link" href="#The-Decoder"> </a>
</h1>
<p>The decoder share some components with the encoder but they are used in a different way to take into account the encoder output.</p>
<ul>
<li>Positional encoding: Similar that the one in the encoder</li>
<li>
<p>N=6 identical layers, containing 3 three sublayers. First, the Masked Multi-head attention or <strong>masked causal attention</strong> to prevent positions from attending to subsequent positions, hiding those features that belong to future states of the sequence. "This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position i can depend only on the known outputs at positions less than i" [1]. It is implemented setting to −∞ the values corresponding to the forbidden states in the softmax layer of the dot-product attention modules. The second component or <strong>"encoder-decoder attention"</strong> performs multi-head attention over the output of the decoder, the Key and Value vectors come from the output of the encoder but the queries come from the previous decoder layer. <em>This allows every position in the decoder to attend over all positions in the input sequence</em> [1]. And finally the fully-connected network.</p>
</li>
<li>
<p>The residual connection and layer normalization around each sub-layer, similar to the encoder.</p>
</li>
</ul>
<p><img src="/BlogEms/images/copied_from_nb/images/decoder.PNG" alt="Alt" title="title The Ilustrated Transformer by Jay Alammar"></p>
<p>At the end of the N stacked decoders, the <strong>linear layer</strong>, a fully-connected network, transforms the stacked outputs to a much larger vector, the <em>logits</em>. The <strong>softmax layer</strong> then turns those scores (logits) into probabilities (all positive, all add up to 1.0). The cell with the highest probability is chosen, and the word associated with it is produced as the output for this time step.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">DecoderLayer</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span><span class="p">):</span>
    
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">FFN_units</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">,</span> <span class="n">dropout_rate</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">DecoderLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">FFN_units</span> <span class="o">=</span> <span class="n">FFN_units</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span> <span class="o">=</span> <span class="n">n_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout_rate</span> <span class="o">=</span> <span class="n">dropout_rate</span>
    
    <span class="k">def</span> <span class="nf">build</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span> <span class="o">=</span> <span class="n">input_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        
        <span class="c1"># Self multi head attention, causal attention</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">multi_head_causal_attention</span> <span class="o">=</span> <span class="n">MultiHeadAttention</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout_1</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">rate</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dropout_rate</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">norm_1</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">LayerNormalization</span><span class="p">(</span><span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">)</span>
        
        <span class="c1"># Multi head attention, encoder-decoder attention </span>
        <span class="bp">self</span><span class="o">.</span><span class="n">multi_head_enc_dec_attention</span> <span class="o">=</span> <span class="n">MultiHeadAttention</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout_2</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">rate</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dropout_rate</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">norm_2</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">LayerNormalization</span><span class="p">(</span><span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">)</span>
        
        <span class="c1"># Feed foward</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ffn1_relu</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">FFN_units</span><span class="p">,</span>
                                    <span class="n">activation</span><span class="o">=</span><span class="s2">"relu"</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ffn2</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">d_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout_3</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">rate</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dropout_rate</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">norm_3</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">LayerNormalization</span><span class="p">(</span><span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">enc_outputs</span><span class="p">,</span> <span class="n">mask_1</span><span class="p">,</span> <span class="n">mask_2</span><span class="p">,</span> <span class="n">training</span><span class="p">):</span>
        <span class="c1"># Call the masked causal attention</span>
        <span class="n">attention</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">multi_head_causal_attention</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span>
                                                <span class="n">inputs</span><span class="p">,</span>
                                                <span class="n">inputs</span><span class="p">,</span>
                                                <span class="n">mask_1</span><span class="p">)</span>
        <span class="n">attention</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout_1</span><span class="p">(</span><span class="n">attention</span><span class="p">,</span> <span class="n">training</span><span class="p">)</span>
        <span class="c1"># Residual connection and layer normalization</span>
        <span class="n">attention</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm_1</span><span class="p">(</span><span class="n">attention</span> <span class="o">+</span> <span class="n">inputs</span><span class="p">)</span>
        <span class="c1"># Call the encoder-decoder attention</span>
        <span class="n">attention_2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">multi_head_enc_dec_attention</span><span class="p">(</span><span class="n">attention</span><span class="p">,</span>
                                                  <span class="n">enc_outputs</span><span class="p">,</span>
                                                  <span class="n">enc_outputs</span><span class="p">,</span>
                                                  <span class="n">mask_2</span><span class="p">)</span>
        <span class="n">attention_2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout_2</span><span class="p">(</span><span class="n">attention_2</span><span class="p">,</span> <span class="n">training</span><span class="p">)</span>
        <span class="c1"># Residual connection and layer normalization</span>
        <span class="n">attention_2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm_2</span><span class="p">(</span><span class="n">attention_2</span> <span class="o">+</span> <span class="n">attention</span><span class="p">)</span>
        <span class="c1"># Call the Feed forward</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ffn1_relu</span><span class="p">(</span><span class="n">attention_2</span><span class="p">)</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ffn2</span><span class="p">(</span><span class="n">outputs</span><span class="p">)</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout_3</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">training</span><span class="p">)</span>
        <span class="c1"># Residual connection and layer normalization</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm_3</span><span class="p">(</span><span class="n">outputs</span> <span class="o">+</span> <span class="n">attention_2</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">outputs</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">Decoder</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span><span class="p">):</span>
    
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">n_layers</span><span class="p">,</span>
                 <span class="n">FFN_units</span><span class="p">,</span>
                 <span class="n">n_heads</span><span class="p">,</span>
                 <span class="n">dropout_rate</span><span class="p">,</span>
                 <span class="n">vocab_size</span><span class="p">,</span>
                 <span class="n">d_model</span><span class="p">,</span>
                 <span class="n">name</span><span class="o">=</span><span class="s2">"decoder"</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Decoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span> <span class="o">=</span> <span class="n">d_model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_layers</span> <span class="o">=</span> <span class="n">n_layers</span>
        <span class="c1"># Embedding layer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="c1"># Positional encoding layer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pos_encoding</span> <span class="o">=</span> <span class="n">PositionalEncoding</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">rate</span><span class="o">=</span><span class="n">dropout_rate</span><span class="p">)</span>
        <span class="c1"># Stacked layers of multi-head attention and feed forward</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dec_layers</span> <span class="o">=</span> <span class="p">[</span><span class="n">DecoderLayer</span><span class="p">(</span><span class="n">FFN_units</span><span class="p">,</span>
                                        <span class="n">n_heads</span><span class="p">,</span>
                                        <span class="n">dropout_rate</span><span class="p">)</span> 
                           <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_layers</span><span class="p">)]</span>
    
    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">enc_outputs</span><span class="p">,</span> <span class="n">mask_1</span><span class="p">,</span> <span class="n">mask_2</span><span class="p">,</span> <span class="n">training</span><span class="p">):</span>
        <span class="c1"># Get the embedding vectors</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
        <span class="c1"># Scale by sqrt of d_model</span>
        <span class="n">outputs</span> <span class="o">*=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">d_model</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
        <span class="c1"># Positional encodding</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pos_encoding</span><span class="p">(</span><span class="n">outputs</span><span class="p">)</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">training</span><span class="p">)</span>
        <span class="c1"># Call the stacked layers</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_layers</span><span class="p">):</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dec_layers</span><span class="p">[</span><span class="n">i</span><span class="p">](</span><span class="n">outputs</span><span class="p">,</span>
                                         <span class="n">enc_outputs</span><span class="p">,</span>
                                         <span class="n">mask_1</span><span class="p">,</span>
                                         <span class="n">mask_2</span><span class="p">,</span>
                                         <span class="n">training</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">outputs</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Transformer">
<a class="anchor" href="#Transformer" aria-hidden="true"><span class="octicon octicon-link"></span></a>Transformer<a class="anchor-link" href="#Transformer"> </a>
</h1>
<p>Once we have defined our components and created the encoder, the decoder and the linear-softmax final layer, we join the pieces to form our model, the Transformer.</p>
<p><img src="/BlogEms/images/copied_from_nb/images/transformer_architecture.PNG" alt="Alt" title="title Attention is all you need paper"></p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">Transformer</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Model</span><span class="p">):</span>
    
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">vocab_size_enc</span><span class="p">,</span>
                 <span class="n">vocab_size_dec</span><span class="p">,</span>
                 <span class="n">d_model</span><span class="p">,</span>
                 <span class="n">n_layers</span><span class="p">,</span>
                 <span class="n">FFN_units</span><span class="p">,</span>
                 <span class="n">n_heads</span><span class="p">,</span>
                 <span class="n">dropout_rate</span><span class="p">,</span>
                 <span class="n">name</span><span class="o">=</span><span class="s2">"transformer"</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Transformer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
        <span class="c1"># Build the encoder</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">Encoder</span><span class="p">(</span><span class="n">n_layers</span><span class="p">,</span>
                               <span class="n">FFN_units</span><span class="p">,</span>
                               <span class="n">n_heads</span><span class="p">,</span>
                               <span class="n">dropout_rate</span><span class="p">,</span>
                               <span class="n">vocab_size_enc</span><span class="p">,</span>
                               <span class="n">d_model</span><span class="p">)</span>
        <span class="c1"># Build the decoder</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="n">Decoder</span><span class="p">(</span><span class="n">n_layers</span><span class="p">,</span>
                               <span class="n">FFN_units</span><span class="p">,</span>
                               <span class="n">n_heads</span><span class="p">,</span>
                               <span class="n">dropout_rate</span><span class="p">,</span>
                               <span class="n">vocab_size_dec</span><span class="p">,</span>
                               <span class="n">d_model</span><span class="p">)</span>
        <span class="c1"># build the linear transformation and softmax function</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">last_linear</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="n">vocab_size_dec</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">"lin_ouput"</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">create_padding_mask</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">seq</span><span class="p">):</span> <span class="c1">#seq: (batch_size, seq_length)</span>
        <span class="c1"># Create the mask for padding</span>
        <span class="n">mask</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">seq</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">mask</span><span class="p">[:,</span> <span class="n">tf</span><span class="o">.</span><span class="n">newaxis</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">newaxis</span><span class="p">,</span> <span class="p">:]</span>

    <span class="k">def</span> <span class="nf">create_look_ahead_mask</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">seq</span><span class="p">):</span>
        <span class="c1"># Create the mask for the causal attention</span>
        <span class="n">seq_len</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">seq</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">look_ahead_mask</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">tf</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">band_part</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">seq_len</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">)),</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">look_ahead_mask</span>
    
    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">enc_inputs</span><span class="p">,</span> <span class="n">dec_inputs</span><span class="p">,</span> <span class="n">training</span><span class="p">):</span>
        <span class="c1"># Create the padding mask for the encoder</span>
        <span class="n">enc_mask</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">create_padding_mask</span><span class="p">(</span><span class="n">enc_inputs</span><span class="p">)</span>
        <span class="c1"># Create the mask for the causal attention</span>
        <span class="n">dec_mask_1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">create_padding_mask</span><span class="p">(</span><span class="n">dec_inputs</span><span class="p">),</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">create_look_ahead_mask</span><span class="p">(</span><span class="n">dec_inputs</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="c1"># Create the mask for the encoder-decoder attention</span>
        <span class="n">dec_mask_2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">create_padding_mask</span><span class="p">(</span><span class="n">enc_inputs</span><span class="p">)</span>
        <span class="c1"># Call the encoder</span>
        <span class="n">enc_outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">enc_inputs</span><span class="p">,</span> <span class="n">enc_mask</span><span class="p">,</span> <span class="n">training</span><span class="p">)</span>
        <span class="c1"># Call the decoder</span>
        <span class="n">dec_outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="p">(</span><span class="n">dec_inputs</span><span class="p">,</span>
                                   <span class="n">enc_outputs</span><span class="p">,</span>
                                   <span class="n">dec_mask_1</span><span class="p">,</span>
                                   <span class="n">dec_mask_2</span><span class="p">,</span>
                                   <span class="n">training</span><span class="p">)</span>
        <span class="c1"># Call the Linear and Softmax functions</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_linear</span><span class="p">(</span><span class="n">dec_outputs</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">outputs</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>It is worth mentioning that we create 3 masks, each of which will allow us:</p>
<ul>
<li>Encoder mask: It is a padding mask to discard the pad tokens from the attention calculation.</li>
<li>Decoder mask 1: this mask is a union of the padding mask and the look ahead mask which will help the causal attention to discard the tokens "in the future". We take the maximum value between the padding mask and the look ahead one.</li>
<li>Decoder mask 2: it is the padding mask and is applied in the encoder-decoder attention layer.</li>
</ul>
<p>As you can see then we call the encoder, the decoder and the final linear-softmax layer to get the predicted output from our Transformer model.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Training-the-Transformer-model">
<a class="anchor" href="#Training-the-Transformer-model" aria-hidden="true"><span class="octicon octicon-link"></span></a>Training the Transformer model<a class="anchor-link" href="#Training-the-Transformer-model"> </a>
</h1>
<p>Now that we have described in detail the components in the paper we are ready to implement them and train a transformer model on a NMT problem. It is a toy problem for educational purposes.</p>
<p>We need to create a custom loss function to mask the padding tokens and we define the Adam optimizer described in the paper, with beta1 = 0.9, beta2 = 0.98 and epsilon= 10e-9. And then we create a scheduler to vary the learning rate over the training process according to:</p>
<p>$lrate = d_{model}^{-0.5}*min(step\_num^{-0.5}, step\_num*warmup\_steps^{-1.5})$</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">loss_function</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="n">pred</span><span class="p">):</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">logical_not</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
    <span class="n">loss_</span> <span class="o">=</span> <span class="n">loss_object</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="n">pred</span><span class="p">)</span>
    
    <span class="n">mask</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">loss_</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">loss_</span> <span class="o">*=</span> <span class="n">mask</span>
    
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">loss_</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">CustomSchedule</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">schedules</span><span class="o">.</span><span class="n">LearningRateSchedule</span><span class="p">):</span>
    
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">warmup_steps</span><span class="o">=</span><span class="mi">4000</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">CustomSchedule</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">warmup_steps</span> <span class="o">=</span> <span class="n">warmup_steps</span>
    
    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">step</span><span class="p">):</span>
        <span class="n">arg1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">rsqrt</span><span class="p">(</span><span class="n">step</span><span class="p">)</span>
        <span class="n">arg2</span> <span class="o">=</span> <span class="n">step</span> <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">warmup_steps</span><span class="o">**-</span><span class="mf">1.5</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">rsqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">d_model</span><span class="p">)</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">minimum</span><span class="p">(</span><span class="n">arg1</span><span class="p">,</span> <span class="n">arg2</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>And that's all, we have all the necessary elements to train our model using an usual loop for sequence-to-sequence tasks:</p>
<ul>
<li>For every iteration on the batch generator that produce batch size inputs and outputs</li>
<li>Get the input sequence from 0 to length-1 and the actual outputs from 1 to length, the next word expected at every sequence step.</li>
<li>Call the transformer to get the predictions</li>
<li>Calculate the loss function between the real outputs and the predictions</li>
<li>Apply the gradients to update the weights in the model</li>
<li>Calculate the mean loss and the accuracy for the batch data</li>
<li>Show some results and save the model in every epoch</li>
</ul>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">main_train</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">transformer</span><span class="p">,</span> <span class="n">n_epochs</span><span class="p">,</span> <span class="n">print_every</span><span class="o">=</span><span class="mi">50</span><span class="p">):</span>
  <span class="sd">''' Train the transformer model for n_epochs using the data generator dataset'''</span>
  <span class="n">losses</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="n">accuracies</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="c1"># In every epoch</span>
  <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_epochs</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"Starting epoch </span><span class="si">{}</span><span class="s2">"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="c1"># Reset the losss and accuracy calculations</span>
    <span class="n">train_loss</span><span class="o">.</span><span class="n">reset_states</span><span class="p">()</span>
    <span class="n">train_accuracy</span><span class="o">.</span><span class="n">reset_states</span><span class="p">()</span>
    <span class="c1"># Get a batch of inputs and targets</span>
    <span class="k">for</span> <span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="p">(</span><span class="n">enc_inputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">))</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">dataset</span><span class="p">):</span>
        <span class="c1"># Set the decoder inputs</span>
        <span class="n">dec_inputs</span> <span class="o">=</span> <span class="n">targets</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="c1"># Set the target outputs, right shifted</span>
        <span class="n">dec_outputs_real</span> <span class="o">=</span> <span class="n">targets</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:]</span>
        <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">tape</span><span class="p">:</span>
            <span class="c1"># Call the transformer and get the predicted output</span>
            <span class="n">predictions</span> <span class="o">=</span> <span class="n">transformer</span><span class="p">(</span><span class="n">enc_inputs</span><span class="p">,</span> <span class="n">dec_inputs</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
            <span class="c1"># Calculate the loss</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_function</span><span class="p">(</span><span class="n">dec_outputs_real</span><span class="p">,</span> <span class="n">predictions</span><span class="p">)</span>
        <span class="c1"># Update the weights and optimizer</span>
        <span class="n">gradients</span> <span class="o">=</span> <span class="n">tape</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">transformer</span><span class="o">.</span><span class="n">trainable_variables</span><span class="p">)</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">apply_gradients</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">gradients</span><span class="p">,</span> <span class="n">transformer</span><span class="o">.</span><span class="n">trainable_variables</span><span class="p">))</span>
        <span class="c1"># Save and store the metrics</span>
        <span class="n">train_loss</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
        <span class="n">train_accuracy</span><span class="p">(</span><span class="n">dec_outputs_real</span><span class="p">,</span> <span class="n">predictions</span><span class="p">)</span>
        
        <span class="k">if</span> <span class="n">batch</span> <span class="o">%</span> <span class="n">print_every</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">train_loss</span><span class="o">.</span><span class="n">result</span><span class="p">())</span>
            <span class="n">accuracies</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">train_accuracy</span><span class="o">.</span><span class="n">result</span><span class="p">())</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">"Epoch </span><span class="si">{}</span><span class="s2"> Batch </span><span class="si">{}</span><span class="s2"> Loss </span><span class="si">{:.4f}</span><span class="s2"> Accuracy </span><span class="si">{:.4f}</span><span class="s2">"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                <span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">train_loss</span><span class="o">.</span><span class="n">result</span><span class="p">(),</span> <span class="n">train_accuracy</span><span class="o">.</span><span class="n">result</span><span class="p">()))</span>
            
    <span class="c1"># Checkpoint the model on every epoch        </span>
    <span class="n">ckpt_save_path</span> <span class="o">=</span> <span class="n">ckpt_manager</span><span class="o">.</span><span class="n">save</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"Saving checkpoint for epoch </span><span class="si">{}</span><span class="s2"> in </span><span class="si">{}</span><span class="s2">"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span>
                                                        <span class="n">ckpt_save_path</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"Time for 1 epoch: </span><span class="si">{}</span><span class="s2"> secs</span><span class="se">\n</span><span class="s2">"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span><span class="p">))</span>

  <span class="k">return</span> <span class="n">losses</span><span class="p">,</span> <span class="n">accuracies</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Setting the hyperparameters and parameters of the model and training process:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Set hyperparamters for the model</span>
<span class="n">D_MODEL</span> <span class="o">=</span> <span class="mi">512</span> <span class="c1"># 512</span>
<span class="n">N_LAYERS</span> <span class="o">=</span> <span class="mi">4</span> <span class="c1"># 6</span>
<span class="n">FFN_UNITS</span> <span class="o">=</span> <span class="mi">512</span> <span class="c1"># 2048</span>
<span class="n">N_HEADS</span> <span class="o">=</span> <span class="mi">8</span> <span class="c1"># 8</span>
<span class="n">DROPOUT_RATE</span> <span class="o">=</span> <span class="mf">0.1</span> <span class="c1"># 0.1</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now we define and create all the elements to train the model and evaluate it.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Clean the session</span>
<span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">backend</span><span class="o">.</span><span class="n">clear_session</span><span class="p">()</span>
<span class="c1"># Create the Transformer model</span>
<span class="n">transformer</span> <span class="o">=</span> <span class="n">Transformer</span><span class="p">(</span><span class="n">vocab_size_enc</span><span class="o">=</span><span class="n">num_words_inputs</span><span class="p">,</span>
                          <span class="n">vocab_size_dec</span><span class="o">=</span><span class="n">num_words_output</span><span class="p">,</span>
                          <span class="n">d_model</span><span class="o">=</span><span class="n">D_MODEL</span><span class="p">,</span>
                          <span class="n">n_layers</span><span class="o">=</span><span class="n">N_LAYERS</span><span class="p">,</span>
                          <span class="n">FFN_units</span><span class="o">=</span><span class="n">FFN_UNITS</span><span class="p">,</span>
                          <span class="n">n_heads</span><span class="o">=</span><span class="n">N_HEADS</span><span class="p">,</span>
                          <span class="n">dropout_rate</span><span class="o">=</span><span class="n">DROPOUT_RATE</span><span class="p">)</span>

<span class="c1"># Define a categorical cross entropy loss</span>
<span class="n">loss_object</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">SparseCategoricalCrossentropy</span><span class="p">(</span><span class="n">from_logits</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                                                            <span class="n">reduction</span><span class="o">=</span><span class="s2">"none"</span><span class="p">)</span>
<span class="c1"># Define a metric to store the mean loss of every epoch</span>
<span class="n">train_loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">Mean</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">"train_loss"</span><span class="p">)</span>
<span class="c1"># Define a matric to save the accuracy in every epoch</span>
<span class="n">train_accuracy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">SparseCategoricalAccuracy</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">"train_accuracy"</span><span class="p">)</span>
<span class="c1"># Create the scheduler for learning rate decay</span>
<span class="n">leaning_rate</span> <span class="o">=</span> <span class="n">CustomSchedule</span><span class="p">(</span><span class="n">D_MODEL</span><span class="p">)</span>
<span class="c1"># Create the Adam optimizer</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">leaning_rate</span><span class="p">,</span>
                                     <span class="n">beta_1</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span>
                                     <span class="n">beta_2</span><span class="o">=</span><span class="mf">0.98</span><span class="p">,</span>
                                     <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-9</span><span class="p">)</span>
        
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>It is very useful to checkpoint and save our model during training. Training can take a lot of time and we can restore the model for future training or use.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1">#Create the Checkpoint </span>
<span class="n">ckpt</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">Checkpoint</span><span class="p">(</span><span class="n">transformer</span><span class="o">=</span><span class="n">transformer</span><span class="p">,</span>
                           <span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">)</span>

<span class="n">ckpt_manager</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">CheckpointManager</span><span class="p">(</span><span class="n">ckpt</span><span class="p">,</span> <span class="n">checkpoint_path</span><span class="p">,</span> <span class="n">max_to_keep</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>

<span class="k">if</span> <span class="n">ckpt_manager</span><span class="o">.</span><span class="n">latest_checkpoint</span><span class="p">:</span>
    <span class="n">ckpt</span><span class="o">.</span><span class="n">restore</span><span class="p">(</span><span class="n">ckpt_manager</span><span class="o">.</span><span class="n">latest_checkpoint</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"Last checkpoint restored."</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Train the model</span>
<span class="n">losses</span><span class="p">,</span> <span class="n">accuracies</span> <span class="o">=</span> <span class="n">main_train</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">transformer</span><span class="p">,</span> <span class="n">EPOCHS</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Starting epoch 1
Epoch 1 Batch 0 Loss 4.7359 Accuracy 0.0000
Epoch 1 Batch 100 Loss 4.6680 Accuracy 0.0530
Epoch 1 Batch 200 Loss 4.3400 Accuracy 0.0824
Epoch 1 Batch 300 Loss 3.9776 Accuracy 0.1032
Epoch 1 Batch 400 Loss 3.6847 Accuracy 0.1182
Epoch 1 Batch 500 Loss 3.4673 Accuracy 0.1302
Epoch 1 Batch 600 Loss 3.2969 Accuracy 0.1401
Epoch 1 Batch 700 Loss 3.1606 Accuracy 0.1487
Epoch 1 Batch 800 Loss 3.0452 Accuracy 0.1566
Epoch 1 Batch 900 Loss 2.9457 Accuracy 0.1637
Epoch 1 Batch 1000 Loss 2.8575 Accuracy 0.1702
Epoch 1 Batch 1100 Loss 2.7813 Accuracy 0.1757
Epoch 1 Batch 1200 Loss 2.7106 Accuracy 0.1810
Saving checkpoint for epoch 1 in /content/drive/My Drive/Projects/Transformer_NMT/ckpt/ckpt-1
Time for 1 epoch: 508.13360619544983 secs

Starting epoch 2
Epoch 2 Batch 0 Loss 1.8356 Accuracy 0.2533
Epoch 2 Batch 100 Loss 1.8220 Accuracy 0.2488
Epoch 2 Batch 200 Loss 1.7898 Accuracy 0.2517
Epoch 2 Batch 300 Loss 1.7732 Accuracy 0.2539
Epoch 2 Batch 400 Loss 1.7554 Accuracy 0.2555
Epoch 2 Batch 500 Loss 1.7384 Accuracy 0.2574
Epoch 2 Batch 600 Loss 1.7187 Accuracy 0.2592
Epoch 2 Batch 700 Loss 1.7027 Accuracy 0.2615
Epoch 2 Batch 800 Loss 1.6836 Accuracy 0.2641
Epoch 2 Batch 900 Loss 1.6639 Accuracy 0.2665
Epoch 2 Batch 1000 Loss 1.6464 Accuracy 0.2686
Epoch 2 Batch 1100 Loss 1.6307 Accuracy 0.2706
Epoch 2 Batch 1200 Loss 1.6130 Accuracy 0.2728
Saving checkpoint for epoch 2 in /content/drive/My Drive/Projects/Transformer_NMT/ckpt/ckpt-2
Time for 1 epoch: 496.16912508010864 secs

Starting epoch 3
Epoch 3 Batch 0 Loss 1.2504 Accuracy 0.3158
Epoch 3 Batch 100 Loss 1.3001 Accuracy 0.3075
Epoch 3 Batch 200 Loss 1.2939 Accuracy 0.3085
Epoch 3 Batch 300 Loss 1.2985 Accuracy 0.3088
Epoch 3 Batch 400 Loss 1.2915 Accuracy 0.3096
Epoch 3 Batch 500 Loss 1.2868 Accuracy 0.3103
Epoch 3 Batch 600 Loss 1.2827 Accuracy 0.3107
Epoch 3 Batch 700 Loss 1.2791 Accuracy 0.3117
Epoch 3 Batch 800 Loss 1.2731 Accuracy 0.3121
Epoch 3 Batch 900 Loss 1.2675 Accuracy 0.3128
Epoch 3 Batch 1000 Loss 1.2633 Accuracy 0.3134
Epoch 3 Batch 1100 Loss 1.2607 Accuracy 0.3138
Epoch 3 Batch 1200 Loss 1.2589 Accuracy 0.3141
Saving checkpoint for epoch 3 in /content/drive/My Drive/Projects/Transformer_NMT/ckpt/ckpt-3
Time for 1 epoch: 488.9535298347473 secs

Starting epoch 4
Epoch 4 Batch 0 Loss 1.3869 Accuracy 0.3181
Epoch 4 Batch 100 Loss 1.0897 Accuracy 0.3291
Epoch 4 Batch 200 Loss 1.1142 Accuracy 0.3268
Epoch 4 Batch 300 Loss 1.1265 Accuracy 0.3261
Epoch 4 Batch 400 Loss 1.1285 Accuracy 0.3256
Epoch 4 Batch 500 Loss 1.1347 Accuracy 0.3255
Epoch 4 Batch 600 Loss 1.1360 Accuracy 0.3253
Epoch 4 Batch 700 Loss 1.1365 Accuracy 0.3254
Epoch 4 Batch 800 Loss 1.1362 Accuracy 0.3258
Epoch 4 Batch 900 Loss 1.1368 Accuracy 0.3260
Epoch 4 Batch 1000 Loss 1.1357 Accuracy 0.3264
Epoch 4 Batch 1100 Loss 1.1346 Accuracy 0.3266
Epoch 4 Batch 1200 Loss 1.1334 Accuracy 0.3269
Saving checkpoint for epoch 4 in /content/drive/My Drive/Projects/Transformer_NMT/ckpt/ckpt-4
Time for 1 epoch: 479.5311484336853 secs

Starting epoch 5
Epoch 5 Batch 0 Loss 1.1191 Accuracy 0.3214
Epoch 5 Batch 100 Loss 0.9881 Accuracy 0.3438
Epoch 5 Batch 200 Loss 0.9997 Accuracy 0.3424
Epoch 5 Batch 300 Loss 1.0053 Accuracy 0.3421
Epoch 5 Batch 400 Loss 1.0054 Accuracy 0.3424
Epoch 5 Batch 500 Loss 1.0052 Accuracy 0.3422
Epoch 5 Batch 600 Loss 1.0092 Accuracy 0.3417
Epoch 5 Batch 700 Loss 1.0098 Accuracy 0.3419
Epoch 5 Batch 800 Loss 1.0125 Accuracy 0.3417
Epoch 5 Batch 900 Loss 1.0134 Accuracy 0.3417
Epoch 5 Batch 1000 Loss 1.0121 Accuracy 0.3417
Epoch 5 Batch 1100 Loss 1.0120 Accuracy 0.3420
Epoch 5 Batch 1200 Loss 1.0117 Accuracy 0.3420
Saving checkpoint for epoch 5 in /content/drive/My Drive/Projects/Transformer_NMT/ckpt/ckpt-5
Time for 1 epoch: 476.9857404232025 secs

Starting epoch 6
Epoch 6 Batch 0 Loss 0.9588 Accuracy 0.3538
Epoch 6 Batch 100 Loss 0.8943 Accuracy 0.3525
Epoch 6 Batch 200 Loss 0.8968 Accuracy 0.3547
Epoch 6 Batch 300 Loss 0.9077 Accuracy 0.3546
Epoch 6 Batch 400 Loss 0.9101 Accuracy 0.3543
Epoch 6 Batch 500 Loss 0.9142 Accuracy 0.3540
Epoch 6 Batch 600 Loss 0.9183 Accuracy 0.3535
Epoch 6 Batch 700 Loss 0.9226 Accuracy 0.3533
Epoch 6 Batch 800 Loss 0.9242 Accuracy 0.3532
Epoch 6 Batch 900 Loss 0.9246 Accuracy 0.3532
Epoch 6 Batch 1000 Loss 0.9250 Accuracy 0.3531
Epoch 6 Batch 1100 Loss 0.9268 Accuracy 0.3531
Epoch 6 Batch 1200 Loss 0.9281 Accuracy 0.3531
Saving checkpoint for epoch 6 in /content/drive/My Drive/Projects/Transformer_NMT/ckpt/ckpt-6
Time for 1 epoch: 476.99894547462463 secs

Starting epoch 7
Epoch 7 Batch 0 Loss 0.9244 Accuracy 0.3571
Epoch 7 Batch 100 Loss 0.8027 Accuracy 0.3686
Epoch 7 Batch 200 Loss 0.8253 Accuracy 0.3665
Epoch 7 Batch 300 Loss 0.8329 Accuracy 0.3649
Epoch 7 Batch 400 Loss 0.8385 Accuracy 0.3641
Epoch 7 Batch 500 Loss 0.8436 Accuracy 0.3632
Epoch 7 Batch 600 Loss 0.8465 Accuracy 0.3625
Epoch 7 Batch 700 Loss 0.8498 Accuracy 0.3623
Epoch 7 Batch 800 Loss 0.8531 Accuracy 0.3621
Epoch 7 Batch 900 Loss 0.8529 Accuracy 0.3618
Epoch 7 Batch 1000 Loss 0.8545 Accuracy 0.3618
Epoch 7 Batch 1100 Loss 0.8575 Accuracy 0.3616
Epoch 7 Batch 1200 Loss 0.8604 Accuracy 0.3616
Saving checkpoint for epoch 7 in /content/drive/My Drive/Projects/Transformer_NMT/ckpt/ckpt-7
Time for 1 epoch: 476.37826561927795 secs

Starting epoch 8
Epoch 8 Batch 0 Loss 0.6250 Accuracy 0.3906
Epoch 8 Batch 100 Loss 0.7504 Accuracy 0.3701
Epoch 8 Batch 200 Loss 0.7584 Accuracy 0.3705
Epoch 8 Batch 300 Loss 0.7593 Accuracy 0.3711
Epoch 8 Batch 400 Loss 0.7657 Accuracy 0.3713
Epoch 8 Batch 500 Loss 0.7724 Accuracy 0.3711
Epoch 8 Batch 600 Loss 0.7806 Accuracy 0.3708
Epoch 8 Batch 700 Loss 0.7855 Accuracy 0.3705
Epoch 8 Batch 800 Loss 0.7925 Accuracy 0.3701
Epoch 8 Batch 900 Loss 0.7977 Accuracy 0.3697
Epoch 8 Batch 1000 Loss 0.8015 Accuracy 0.3696
Epoch 8 Batch 1100 Loss 0.8062 Accuracy 0.3693
Epoch 8 Batch 1200 Loss 0.8082 Accuracy 0.3691
Saving checkpoint for epoch 8 in /content/drive/My Drive/Projects/Transformer_NMT/ckpt/ckpt-8
Time for 1 epoch: 476.7389974594116 secs

Starting epoch 9
Epoch 9 Batch 0 Loss 0.7808 Accuracy 0.3571
Epoch 9 Batch 100 Loss 0.6965 Accuracy 0.3805
Epoch 9 Batch 200 Loss 0.7069 Accuracy 0.3792
Epoch 9 Batch 300 Loss 0.7119 Accuracy 0.3797
Epoch 9 Batch 400 Loss 0.7249 Accuracy 0.3789
Epoch 9 Batch 500 Loss 0.7307 Accuracy 0.3789
Epoch 9 Batch 600 Loss 0.7343 Accuracy 0.3782
Epoch 9 Batch 700 Loss 0.7381 Accuracy 0.3779
Epoch 9 Batch 800 Loss 0.7425 Accuracy 0.3773
Epoch 9 Batch 900 Loss 0.7465 Accuracy 0.3770
Epoch 9 Batch 1000 Loss 0.7512 Accuracy 0.3767
Epoch 9 Batch 1100 Loss 0.7553 Accuracy 0.3762
Epoch 9 Batch 1200 Loss 0.7588 Accuracy 0.3760
Saving checkpoint for epoch 9 in /content/drive/My Drive/Projects/Transformer_NMT/ckpt/ckpt-9
Time for 1 epoch: 475.674427986145 secs

Starting epoch 10
Epoch 10 Batch 0 Loss 0.7109 Accuracy 0.3973
Epoch 10 Batch 100 Loss 0.6622 Accuracy 0.3855
Epoch 10 Batch 200 Loss 0.6659 Accuracy 0.3854
Epoch 10 Batch 300 Loss 0.6762 Accuracy 0.3844
Epoch 10 Batch 400 Loss 0.6849 Accuracy 0.3838
Epoch 10 Batch 500 Loss 0.6894 Accuracy 0.3835
Epoch 10 Batch 600 Loss 0.6959 Accuracy 0.3832
Epoch 10 Batch 700 Loss 0.7005 Accuracy 0.3827
Epoch 10 Batch 800 Loss 0.7048 Accuracy 0.3822
Epoch 10 Batch 900 Loss 0.7100 Accuracy 0.3817
Epoch 10 Batch 1000 Loss 0.7131 Accuracy 0.3815
Epoch 10 Batch 1100 Loss 0.7172 Accuracy 0.3814
Epoch 10 Batch 1200 Loss 0.7207 Accuracy 0.3812
Saving checkpoint for epoch 10 in /content/drive/My Drive/Projects/Transformer_NMT/ckpt/ckpt-10
Time for 1 epoch: 480.2911014556885 secs

</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Show-some-results-from-training">
<a class="anchor" href="#Show-some-results-from-training" aria-hidden="true"><span class="octicon octicon-link"></span></a>Show some results from training<a class="anchor-link" href="#Show-some-results-from-training"> </a>
</h2>
</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="p">(</span><span class="n">ax1</span><span class="p">,</span> <span class="n">ax2</span><span class="p">)</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>
<span class="c1"># plot some data</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">losses</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">'loss'</span><span class="p">)</span>
<span class="c1">#plt.plot(results.history['val_loss'], label='val_loss')</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">'Training Loss'</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="c1"># accuracies</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">accuracies</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">'acc'</span><span class="p">)</span>
<span class="c1">#plt.plot(results.history['val_accuracy_fn'], label='val_acc')</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">'Training Accuracy'</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,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%0A">
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Make-predictions">
<a class="anchor" href="#Make-predictions" aria-hidden="true"><span class="octicon octicon-link"></span></a>Make predictions<a class="anchor-link" href="#Make-predictions"> </a>
</h1>
<p>We have seen the scores obtained after training but what we are interested in making predictions and see how the model works with new sentences. The predict function will input a tokenize sentence to the model and return the predicted new sentence, in our example, a translation from english to spanish.</p>
<ul>
<li>Tokenize the input sentence to a sequence of tokens</li>
<li>Set the initial output sequence to the SOS token</li>
<li>Until we reach the max length or the eos token is returned by the model</li>
<li>Get the next word predicted. The model returns the logits, remember that the softmax function is applied in the loss calculation.</li>
<li>Get the index in the vocabulary of the word with the highest probability</li>
<li>Concat the next word predicted to the output sequence</li>
</ul>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="n">inp_sentence</span><span class="p">,</span> <span class="n">tokenizer_in</span><span class="p">,</span> <span class="n">tokenizer_out</span><span class="p">,</span> <span class="n">target_max_len</span><span class="p">):</span>
    <span class="c1"># Tokenize the input sequence using the tokenizer_in</span>
    <span class="n">inp_sentence</span> <span class="o">=</span> <span class="n">sos_token_input</span> <span class="o">+</span> <span class="n">tokenizer_in</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">inp_sentence</span><span class="p">)</span> <span class="o">+</span> <span class="n">eos_token_input</span>
    <span class="n">enc_input</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">inp_sentence</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

    <span class="c1"># Set the initial output sentence to sos</span>
    <span class="n">out_sentence</span> <span class="o">=</span> <span class="n">sos_token_output</span>
    <span class="c1"># Reshape the output</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">out_sentence</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

    <span class="c1"># For max target len tokens</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">target_max_len</span><span class="p">):</span>
        <span class="c1"># Call the transformer and get the logits </span>
        <span class="n">predictions</span> <span class="o">=</span> <span class="n">transformer</span><span class="p">(</span><span class="n">enc_input</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span> <span class="c1">#(1, seq_length, VOCAB_SIZE_ES)</span>
        <span class="c1"># Extract the logists of the next word</span>
        <span class="n">prediction</span> <span class="o">=</span> <span class="n">predictions</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">:,</span> <span class="p">:]</span>
        <span class="c1"># The highest probability is taken</span>
        <span class="n">predicted_id</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">prediction</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">),</span> <span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
        <span class="c1"># Check if it is the eos token</span>
        <span class="k">if</span> <span class="n">predicted_id</span> <span class="o">==</span> <span class="n">eos_token_output</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="c1"># Concat the predicted word to the output sequence</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">output</span><span class="p">,</span> <span class="n">predicted_id</span><span class="p">],</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>And finally our last function receives a sentence in english, calls the transformer to translate it to spanish and shows the result</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">translate</span><span class="p">(</span><span class="n">sentence</span><span class="p">):</span>
    <span class="c1"># Get the predicted sequence for the input sentence</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">predict</span><span class="p">(</span><span class="n">sentence</span><span class="p">,</span> <span class="n">tokenizer_inputs</span><span class="p">,</span> <span class="n">tokenizer_outputs</span><span class="p">,</span> <span class="n">MAX_LENGTH</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    <span class="c1"># Transform the sequence of tokens to a sentence</span>
    <span class="n">predicted_sentence</span> <span class="o">=</span> <span class="n">tokenizer_outputs</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span>
        <span class="p">[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">output</span> <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">sos_token_output</span><span class="p">]</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">predicted_sentence</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>First, we explore the predictions on sentences of our training dataset:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1">#Show some translations</span>
<span class="n">sentence</span> <span class="o">=</span> <span class="s2">"you should pay for it."</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Input sentence: </span><span class="si">{}</span><span class="s2">"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">sentence</span><span class="p">))</span>
<span class="n">predicted_sentence</span> <span class="o">=</span> <span class="n">translate</span><span class="p">(</span><span class="n">sentence</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Output sentence: </span><span class="si">{}</span><span class="s2">"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">predicted_sentence</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Input sentence: you should pay for it.
Output sentence: Deberías pagarlo.
</pre>
</div>
</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1">#Show some translations</span>
<span class="n">sentence</span> <span class="o">=</span> <span class="s2">"we have no extra money."</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Input sentence: </span><span class="si">{}</span><span class="s2">"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">sentence</span><span class="p">))</span>
<span class="n">predicted_sentence</span> <span class="o">=</span> <span class="n">translate</span><span class="p">(</span><span class="n">sentence</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Output sentence: </span><span class="si">{}</span><span class="s2">"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">predicted_sentence</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Input sentence: we have no extra money.
Output sentence: No tenemos dinero extra.
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Next, let's predict some new sentences on diferent topics:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1">#Show some translations</span>
<span class="n">sentence</span> <span class="o">=</span> <span class="s2">"This is a problem to deal with."</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Input sentence: </span><span class="si">{}</span><span class="s2">"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">sentence</span><span class="p">))</span>
<span class="n">predicted_sentence</span> <span class="o">=</span> <span class="n">translate</span><span class="p">(</span><span class="n">sentence</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Output sentence: </span><span class="si">{}</span><span class="s2">"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">predicted_sentence</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Input sentence: This is a problem to deal with.
Output sentence: Este problema es un problema con eso.
</pre>
</div>
</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1">#Show some translations</span>
<span class="n">sentence</span> <span class="o">=</span> <span class="s2">"This is a really powerful method!"</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Input sentence: </span><span class="si">{}</span><span class="s2">"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">sentence</span><span class="p">))</span>
<span class="n">predicted_sentence</span> <span class="o">=</span> <span class="n">translate</span><span class="p">(</span><span class="n">sentence</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Output sentence: </span><span class="si">{}</span><span class="s2">"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">predicted_sentence</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Input sentence: This is a really powerful method!
Output sentence: ¡Esto es un verdadero poderoso!
</pre>
</div>
</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1">#Show some translations</span>
<span class="n">sentence</span> <span class="o">=</span> <span class="s2">"This is an interesting course about Natural Language Processing"</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Input sentence: </span><span class="si">{}</span><span class="s2">"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">sentence</span><span class="p">))</span>
<span class="n">predicted_sentence</span> <span class="o">=</span> <span class="n">translate</span><span class="p">(</span><span class="n">sentence</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Output sentence: </span><span class="si">{}</span><span class="s2">"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">predicted_sentence</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Input sentence: This is an interesting course about Natural Language Processing
Output sentence: Esta es una buena persona en la diez.
</pre>
</div>
</div>

</div>
</div>

</div>
    

</div>



  </div><a class="u-url" href="/BlogEms/transformer/attention/encoder-decoder/tensorflow%202/2020/10/29/Transformer-NMT-en-es.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/BlogEms/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/BlogEms/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/BlogEms/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>Introducing many NLP models and task I learnt on my learning path. I hope I can find new content soon.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/edumunozsala" title="edumunozsala"><svg class="svg-icon grey"><use xlink:href="/BlogEms/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/emunozsala" title="emunozsala"><svg class="svg-icon grey"><use xlink:href="/BlogEms/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>

{
 "cells": [
  {
   "source": [
    "# \"Character-level Text Generator using Pytorch and Amazon SageMaker\"\n",
    "> \"Implementing a simple LSTM encoder-decoder model with PyTorch to familiarize ourselves with the PyTorch library and Amazon SageMaker framework. We will cover how to use Amazon Sagemaker to train a model, deploy as an endpoint service and invoke it to get some predictions\"\n",
    "\n",
    "- toc: true\n",
    "- branch: master\n",
    "- badges: false\n",
    "- comments: false\n",
    "- categories: [fastpages, jupyter, RNN, LSTM, Pytorch, SageMaker]\n",
    "- hide: false\n",
    "- search_exclude: true"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character-level text generator with Pytorch\n",
    "\n",
    "## Using PyTorch and SageMaker\n",
    "\n",
    "Some parts of the notebook have been extracted or modified from a notebook of my exercises in the Machine Learning Egineer Nanodegree. \n",
    "\n",
    "In this notebook we will be implementing a simple RNN character model with PyTorch to familiarize ourselves with the PyTorch library and get started with RNNs. The goal is to build a model that can complete your sentence based on a few characters or a word used as input. And we will use AWS Sagemaker to train the model, evaluate and deploy.\n",
    "\n",
    "\n",
    "## General Outline\n",
    "\n",
    "Recall the general outline for SageMaker projects using a notebook instance.\n",
    "\n",
    "1. Download or otherwise retrieve the data.\n",
    "2. Process / Prepare the data.\n",
    "3. Upload the processed data to S3.\n",
    "4. Train a chosen model.\n",
    "5. Test the trained model (typically using a batch transform job).\n",
    "6. Deploy the trained model.\n",
    "7. Use the deployed model.\n",
    "\n",
    "For this project, you will be following the steps in the general outline with some modifications. \n",
    "\n",
    "First, we will not be testing the model in its own step. We will still be testing the model, however, we will do it by deploying your model and then using the deployed model by sending the test data to it. One of the reasons for doing this is so that we can make sure that our deployed model is working correctly before moving forward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random as rnd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Downloading and loading the data\n",
    "\n",
    "First, we'll define the sentences that we want our model to output when fed with the first word or the first few characters. Our dataset is a text file containing Shakespeare's plays or books from which we will extract sequence of chars to use as input to our model. Then our model will learn how to complete sentences like \"Shakespeare would do\".\n",
    "\n",
    "This dataset can be downloaded from Karpathy's Github account: https://github.com/karpathy/char-rnn/blob/master/data/tinyshakespeare/input.txt.\n",
    "\n",
    "The dataset is stored in our notebook instance, it is small and easy to \"move\", so we do not need to store it in S3 or other cloud storage service.\n",
    "\n",
    "As in many of my notebooks, we set some variables to the data directory and filenames. If you want to run this code on your own enviroment you must change these values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the root folder\n",
    "root_folder='.'\n",
    "# Set the folder with the dataset\n",
    "data_folder_name='data'\n",
    "model_folder_name='model'\n",
    "# Set the filename\n",
    "filename='input.txt'\n",
    "\n",
    "# Path to the data folder\n",
    "DATA_PATH = os.path.abspath(os.path.join(root_folder, data_folder_name))\n",
    "model_dir = os.path.abspath(os.path.join(root_folder, model_folder_name))\n",
    "\n",
    "# Set the path where the text for training is stored\n",
    "train_path = os.path.join(DATA_PATH, filename)\n",
    "\n",
    "# Set a seed\n",
    "seed = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_text_data(filename, init_dialog=False):\n",
    "    ''' Load the texts from the filename, splitting by lines and removing empty strings.\n",
    "        Setting init_dialog = True will remove lines where the character who is going to speak is indicated\n",
    "    '''\n",
    "    sentences = []\n",
    "    with open(filename, 'r') as reader:\n",
    "        #sentences = reader.readlines()\n",
    "        for line in reader:\n",
    "            #if ':' not in line and line !='\\n':\n",
    "            if init_dialog or ':' not in line:\n",
    "                # Append the line to the sentences, removing the end of line character\n",
    "                sentences.append(line[:-1])\n",
    "                \n",
    "    return sentences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the input data, sentences from Shakespeare's plays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences:  29723\n",
      "['Before we proceed any further, hear me speak.', '', 'Speak, speak.', '', 'You are all resolved rather to die than to famish?', '', 'Resolved. resolved.', '', 'First, you know Caius Marcius is chief enemy to the people.', '', \"We know't, we know't.\", '', \"Let us kill him, and we'll have corn at our own price.\", \"Is't a verdict?\", '', '', 'One word, good citizens.', '', 'We are accounted poor citizens, the patricians good.', 'would yield us but the superfluity, while it were']\n"
     ]
    }
   ],
   "source": [
    "sentences = load_text_data(train_path)\n",
    "print('Number of sentences: ', len(sentences))\n",
    "print(sentences[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Preparing and Processing the data\n",
    "\n",
    "Also, we will be doing some initial data processing. The first few steps are the same as in many other notebooks that works in NLP tasks. To begin with, we will read in each of the lines and combine them into a single input structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the input data\n",
    "\n",
    "When working with text data, we usually need to perform some cleanings to prepare the data for our algorithm. This time we will start with a simple cleaning, convert to lowercase the text and that's all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(sentences, alpha=False):\n",
    "    ''' Cleaning process of the text'''\n",
    "    if alpha:\n",
    "        # Remove non alphabetic character\n",
    "        cleaned_text = [''.join([t.lower() for t in text if t.isalpha() or t.isspace()]) for text in sentences]\n",
    "    else:\n",
    "        # Simply lower the characters\n",
    "        cleaned_text = [t.lower() for t in sentences]\n",
    "    # Remove any emoty string\n",
    "    cleaned_text = [t for t in cleaned_text if t!='']\n",
    "    \n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of characters:  894876\n",
      "before we proceed any further, hear me speak. speak, speak. you are all resolved rather to die than \n"
     ]
    }
   ],
   "source": [
    "sentences = clean_text(sentences, False)\n",
    "# Join all the sentences in a one long string\n",
    "sentences = ' '.join(sentences)\n",
    "print('Number of characters: ', len(sentences))\n",
    "print(sentences[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our input data is a sequence of 900,000 characters, we will extract the label data from this sequence and split it into a train and validation dataset. But we will do this tasks after encoding the text data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the dictionary\n",
    "\n",
    "Now we'll create a dictionary out of all the characters that we have in the sentences and map them to an integer. This will allow us to convert our input characters to their respective integers (char2int) and viceversa (int2char)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharVocab: \n",
    "    ''' Create a Vocabulary for '''\n",
    "    def __init__(self, type_vocab,pad_token='<PAD>', eos_token='<EOS>', unk_token='<UNK>'): #Initialization of the type of vocabulary\n",
    "        self.type = type_vocab\n",
    "        #self.int2char ={}\n",
    "        self.int2char = []\n",
    "        if pad_token !=None:\n",
    "            self.int2char += [pad_token]\n",
    "        if eos_token !=None:\n",
    "            self.int2char += [eos_token]\n",
    "        if unk_token !=None:\n",
    "            self.int2char += [unk_token]\n",
    "        #self.int2char[1]=eos_token\n",
    "        #self.int2char[2]=unk_token\n",
    "        self.char2int = {}\n",
    "        \n",
    "    def __call__(self, text):       #When called, adds the values of parameters x_1 and x_2, prints and returns the result \n",
    "        # Join all the sentences together and extract the unique characters from the combined sentences\n",
    "        chars = set(''.join(text))\n",
    "\n",
    "        # Creating a dictionary that maps integers to the characters\n",
    "        self.int2char += list(chars)\n",
    "\n",
    "        # Creating another dictionary that maps characters to integers\n",
    "        self.char2int = {char: ind for ind, char in enumerate(self.int2char)}\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of vocabulary:  38\n",
      "Int to Char:  ['<UNK>', 'a', 'n', '-', 'h', '!', 'i', 'j', 'd', '.', 'f', 'x', 'k', 'w', '3', 'c', 'l', 'q', ' ', 'u', '&', ',', '$', 't', 'b', 'm', 'p', ';', 'z', 'g', 'r', 's', '?', \"'\", 'e', 'v', 'o', 'y']\n",
      "Char to Int:  {'<UNK>': 0, 'a': 1, 'n': 2, '-': 3, 'h': 4, '!': 5, 'i': 6, 'j': 7, 'd': 8, '.': 9, 'f': 10, 'x': 11, 'k': 12, 'w': 13, '3': 14, 'c': 15, 'l': 16, 'q': 17, ' ': 18, 'u': 19, '&': 20, ',': 21, '$': 22, 't': 23, 'b': 24, 'm': 25, 'p': 26, ';': 27, 'z': 28, 'g': 29, 'r': 30, 's': 31, '?': 32, \"'\": 33, 'e': 34, 'v': 35, 'o': 36, 'y': 37}\n"
     ]
    }
   ],
   "source": [
    "vocab = CharVocab('char',None,None,'<UNK>')\n",
    "vocab(sentences)\n",
    "print('Length of vocabulary: ', len(vocab.int2char))\n",
    "print('Int to Char: ', vocab.int2char)\n",
    "print('Char to Int: ', vocab.char2int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the dictionary\n",
    "\n",
    "In this example it is not mandatory to save the dictionary inmediately, because it is a fast and easy to reproduce task. But when dealing with a huge corpus and a large dictionary, we should save the dictionary to restore it latter when new experiments will be executed.\n",
    "\n",
    "Later on when we construct an endpoint which processes a submitted review we will need to make use of the char2int and int2char dictionaries which we have created. As such, we will save them to a file now for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check or create the directory where dictionary will be saved\n",
    "if not os.path.exists(DATA_PATH): # Make sure that the folder exists\n",
    "    os.makedirs(DATA_PATH)\n",
    "    \n",
    "# Save the dictionary to data path dir  \n",
    "with open(os.path.join(DATA_PATH, 'char_dict.pkl'), \"wb\") as f:\n",
    "    pickle.dump(vocab.char2int, f)\n",
    "    \n",
    "with open(os.path.join(DATA_PATH, 'int_dict.pkl'), \"wb\") as f:\n",
    "    pickle.dump(vocab.int2char, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the input data and labels for training\n",
    "\n",
    "As we're going to predict the next character in the sequence at each time step, we'll have to divide each sentence into:\n",
    "\n",
    "- **Input data**: The last input character should be excluded as it does not need to be fed into the model (it is the target label for the last input character)\n",
    "- **Target/Ground Truth Label**: One time-step ahead of the Input data as this will be the \"correct answer\" for the model at each time step corresponding to the input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(indices, dict_size):\n",
    "    ''' Define one hot encode matrix for our sequences'''\n",
    "    # Creating a multi-dimensional array with the desired output shape\n",
    "    # Encode every integer with its one hot representation\n",
    "    features = np.eye(dict_size, dtype=np.float32)[indices.flatten()]\n",
    "    \n",
    "    # Finally reshape it to get back to the original array\n",
    "    features = features.reshape((*indices.shape, dict_size))\n",
    "            \n",
    "    return features\n",
    "\n",
    "def encode_text(input_text, vocab, one_hot = False):\n",
    "    # Replace every char by its integer value based on the vocabulary\n",
    "    output = [vocab.char2int.get(character,0) for character in input_text]\n",
    "    \n",
    "    if one_hot:\n",
    "    # One hot encode every integer of the sequence\n",
    "        dict_size = len(vocab.char2int)\n",
    "        return one_hot_encode(output, dict_size)\n",
    "    else:\n",
    "        return np.array(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can encode our text, replacing every character by the integer value in the dictionary. When we have our dataset unified and prepared, we should do a quick check and see an example of the data our model will be trained on. This is generally a good idea as it allows you to see how each of the further processing steps affects the reviews and it also ensures that the data has been loaded correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original text:\n",
      "before we proceed any further, hear me speak. speak, speak. you are all resolved rather to die than \n",
      "\n",
      "Encoded text:\n",
      "[24 34 10 36 30 34 18 13 34 18 26 30 36 15 34 34  8 18  1  2 37 18 10 19\n",
      " 30 23  4 34 30 21 18  4 34  1 30 18 25 34 18 31 26 34  1 12  9 18 31 26\n",
      " 34  1 12 21 18 31 26 34  1 12  9 18 37 36 19 18  1 30 34 18  1 16 16 18\n",
      " 30 34 31 36 16 35 34  8 18 30  1 23  4 34 30 18 23 36 18  8  6 34 18 23\n",
      "  4  1  2 18]\n",
      "\n",
      "Input sequence:\n",
      "[24 34 10 36 30 34 18 13 34 18 26 30 36 15 34 34  8 18  1  2 37 18 10 19\n",
      " 30 23  4 34 30 21 18  4 34  1 30 18 25 34 18 31 26 34  1 12  9 18 31 26\n",
      " 34  1 12 21 18 31 26 34  1 12  9 18 37 36 19 18  1 30 34 18  1 16 16 18\n",
      " 30 34 31 36 16 35 34  8 18 30  1 23  4 34 30 18 23 36 18  8  6 34 18 23\n",
      "  4  1  2 18]\n",
      "\n",
      "Target sequence:\n",
      "[34 10 36 30 34 18 13 34 18 26 30 36 15 34 34  8 18  1  2 37 18 10 19 30\n",
      " 23  4 34 30 21 18  4 34  1 30 18 25 34 18 31 26 34  1 12  9 18 31 26 34\n",
      "  1 12 21 18 31 26 34  1 12  9 18 37 36 19 18  1 30 34 18  1 16 16 18 30\n",
      " 34 31 36 16 35 34  8 18 30  1 23  4 34 30 18 23 36 18  8  6 34 18 23  4\n",
      "  1  2 18 23]\n"
     ]
    }
   ],
   "source": [
    "# Encode the train dataset\n",
    "train_data = encode_text(sentences, vocab, one_hot = False)\n",
    "\n",
    "# Create the input sequence, from 0 to len-1\n",
    "input_seq=train_data[:-1]\n",
    "# Create the target sequence, from 1 to len. It is right-shifted one place\n",
    "target_seq=train_data[1:]\n",
    "print('\\nOriginal text:')\n",
    "print(sentences[:100])\n",
    "print('\\nEncoded text:')\n",
    "print(train_data[:100])\n",
    "print('\\nInput sequence:')\n",
    "print(input_seq[:100])\n",
    "print('\\nTarget sequence:')\n",
    "print(target_seq[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets check our one-hot-encode function that we will use later during the training phase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded characters:  [23 36]\n",
      "One-hot-encoded characters:  [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print('Encoded characters: ',train_data[100:102])\n",
    "print('One-hot-encoded characters: ',one_hot_encode(train_data[100:102], len(vocab.int2char)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Upload the data to S3\n",
    "\n",
    "Now, we will need to upload the training dataset to S3 in order for our training code to access it. For now we will save it locally and we will upload to S3 later on.\n",
    "\n",
    "### Save the processed training dataset locally\n",
    "\n",
    "It is important to note the format of the data that we are saving as we will need to know it when we write the training code. In our case, we will save the dataset as a pickle object, it is a list containing the whole dataset encoded as an integer value for every character. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the encoded text to a file\n",
    "encoded_data = os.path.join(DATA_PATH, 'input_data.pkl')\n",
    "with open(encoded_data, 'wb') as fp:\n",
    "    pickle.dump(train_data, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uploading the training data\n",
    "\n",
    "\n",
    "Next, we need to upload the training data to the SageMaker default S3 bucket so that we can provide access to it while training our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "# Get the session id \n",
    "sagemaker_session = sagemaker.Session()\n",
    "# Get the bucet, in our example the default buack\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "# Set the S3 subfolder where our data will be stored \n",
    "prefix = 'sagemaker/char_level_rnn'\n",
    "# Get the role for permission\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = sagemaker_session.upload_data(path=DATA_PATH, bucket=bucket, key_prefix=prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** The cell above uploads the entire contents of our data directory. This includes the `char_dict.pkl` and `int_dict.pkl` file. This is fortunate as we will need this later on when we create an endpoint that accepts an arbitrary input text. For now, we will just take note of the fact that it resides in the data directory (and so also in the S3 training bucket) and that we will need to make sure it gets saved in the model directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Build and Train the PyTorch Model\n",
    "\n",
    "A model in the SageMaker framework, in particular, comprises three objects:\n",
    "\n",
    " - Model Artifacts,\n",
    " - Training Code, and\n",
    " - Inference Code,\n",
    " \n",
    "each of which interact with one another.\n",
    "\n",
    "We will start by implementing our own neural network in PyTorch along with a training script. For the purposes of this project we need to provide the model object implementation in the `model.py` file, inside of the `train` folder. You can see the provided implementation by running the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m nn\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mautograd\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Variable\r\n",
      "\r\n",
      "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mRNNModel\u001b[39;49;00m(nn.Module):\r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32m__init__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, vocab_size, embedding_size, hidden_dim, n_layers, drop_rate=\u001b[34m0.2\u001b[39;49;00m):\r\n",
      "        \r\n",
      "        \u001b[36msuper\u001b[39;49;00m(RNNModel, \u001b[36mself\u001b[39;49;00m).\u001b[32m__init__\u001b[39;49;00m()\r\n",
      "\r\n",
      "        \u001b[37m# Defining some parameters\u001b[39;49;00m\r\n",
      "        \u001b[36mself\u001b[39;49;00m.hidden_dim = hidden_dim\r\n",
      "        \u001b[36mself\u001b[39;49;00m.embedding_size = embedding_size\r\n",
      "        \u001b[36mself\u001b[39;49;00m.n_layers = n_layers\r\n",
      "        \u001b[36mself\u001b[39;49;00m.vocab_size = vocab_size\r\n",
      "        \u001b[36mself\u001b[39;49;00m.drop_rate = drop_rate\r\n",
      "        \u001b[36mself\u001b[39;49;00m.char2int = \u001b[34mNone\u001b[39;49;00m\r\n",
      "        \u001b[36mself\u001b[39;49;00m.int2char = \u001b[34mNone\u001b[39;49;00m\r\n",
      "\r\n",
      "\r\n",
      "        \u001b[37m#Defining the layers\u001b[39;49;00m\r\n",
      "        \u001b[37m# Define the encoder as an Embedding layer\u001b[39;49;00m\r\n",
      "        \u001b[37m#self.encoder = nn.Embedding(vocab_size, embedding_size)\u001b[39;49;00m\r\n",
      "            \r\n",
      "        \u001b[37m# Dropout layer\u001b[39;49;00m\r\n",
      "        \u001b[36mself\u001b[39;49;00m.dropout = nn.Dropout(drop_rate)\r\n",
      "        \u001b[37m# RNN Layer\u001b[39;49;00m\r\n",
      "        \u001b[36mself\u001b[39;49;00m.rnn = nn.LSTM(embedding_size, hidden_dim, n_layers, dropout=drop_rate, batch_first = \u001b[34mTrue\u001b[39;49;00m)\r\n",
      "        \u001b[37m# Fully connected layer\u001b[39;49;00m\r\n",
      "        \u001b[36mself\u001b[39;49;00m.decoder = nn.Linear(hidden_dim, vocab_size)\r\n",
      "    \r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mforward\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, x, state):\r\n",
      "        \r\n",
      "        \u001b[37m# input shape: [batch_size, seq_len, embedding_size]\u001b[39;49;00m\r\n",
      "        \u001b[37m# Apply the embedding layer and dropout\u001b[39;49;00m\r\n",
      "        \u001b[37m#embed_seq = self.dropout(self.encoder(x))\u001b[39;49;00m\r\n",
      "            \r\n",
      "        \u001b[37m#print('Input RNN shape: ', embed_seq.shape)\u001b[39;49;00m\r\n",
      "        \u001b[37m# shape: [batch_size, seq_len, embedding_size]\u001b[39;49;00m\r\n",
      "        rnn_out, state = \u001b[36mself\u001b[39;49;00m.rnn(x, state)\r\n",
      "        \u001b[37m#print('Out RNN shape: ', rnn_out.shape)\u001b[39;49;00m\r\n",
      "        \u001b[37m# rnn_out shape: [batch_size, seq_len, rnn_size]\u001b[39;49;00m\r\n",
      "        \u001b[37m# hidden shape: [2, num_layers, batch_size, rnn_size]\u001b[39;49;00m\r\n",
      "        rnn_out = \u001b[36mself\u001b[39;49;00m.dropout(rnn_out)\r\n",
      "\r\n",
      "        \u001b[37m# shape: [seq_len, batch_size, rnn_size]\u001b[39;49;00m\r\n",
      "        \u001b[37m# Stack up LSTM outputs using view\u001b[39;49;00m\r\n",
      "        \u001b[37m# you may need to use contiguous to reshape the output\u001b[39;49;00m\r\n",
      "        rnn_out = rnn_out.contiguous().view(-\u001b[34m1\u001b[39;49;00m, \u001b[36mself\u001b[39;49;00m.hidden_dim)\r\n",
      "\r\n",
      "        logits = \u001b[36mself\u001b[39;49;00m.decoder(rnn_out)\r\n",
      "        \u001b[37m# output shape: [seq_len * batch_size, vocab_size]\u001b[39;49;00m\r\n",
      "        \u001b[37m#print('Output model shape: ', logits.shape)\u001b[39;49;00m\r\n",
      "        \u001b[34mreturn\u001b[39;49;00m logits, state\r\n",
      "    \r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32minit_state\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, device, batch_size=\u001b[34m1\u001b[39;49;00m):\r\n",
      "        \u001b[33m\"\"\"\u001b[39;49;00m\r\n",
      "\u001b[33m        initialises rnn states.\u001b[39;49;00m\r\n",
      "\u001b[33m        \"\"\"\u001b[39;49;00m\r\n",
      "        \u001b[37m#return (Variable(torch.zeros(self.n_layers, batch_size, self.hidden_dim)),\u001b[39;49;00m\r\n",
      "        \u001b[37m#        Variable(torch.zeros(self.n_layers, batch_size, self.hidden_dim)))\u001b[39;49;00m\r\n",
      "        \u001b[34mreturn\u001b[39;49;00m (torch.zeros(\u001b[36mself\u001b[39;49;00m.n_layers, batch_size, \u001b[36mself\u001b[39;49;00m.hidden_dim).to(device),\r\n",
      "                torch.zeros(\u001b[36mself\u001b[39;49;00m.n_layers, batch_size, \u001b[36mself\u001b[39;49;00m.hidden_dim).to(device))\r\n",
      "\r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mpredict\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, \u001b[36minput\u001b[39;49;00m):\r\n",
      "        \u001b[37m# input shape: [seq_len, batch_size]\u001b[39;49;00m\r\n",
      "        logits, hidden = \u001b[36mself\u001b[39;49;00m.forward(\u001b[36minput\u001b[39;49;00m)\r\n",
      "        \u001b[37m# logits shape: [seq_len * batch_size, vocab_size]\u001b[39;49;00m\r\n",
      "        \u001b[37m# hidden shape: [2, num_layers, batch_size, rnn_size]\u001b[39;49;00m\r\n",
      "        probs = F.softmax(logits)\r\n",
      "        \u001b[37m# shape: [seq_len * batch_size, vocab_size]\u001b[39;49;00m\r\n",
      "        probs = probs.view(\u001b[36minput\u001b[39;49;00m.size(\u001b[34m0\u001b[39;49;00m), \u001b[36minput\u001b[39;49;00m.size(\u001b[34m1\u001b[39;49;00m), probs.size(\u001b[34m1\u001b[39;49;00m))\r\n",
      "        \u001b[37m# output shape: [seq_len, batch_size, vocab_size]\u001b[39;49;00m\r\n",
      "        \u001b[34mreturn\u001b[39;49;00m probs, hidden\r\n"
     ]
    }
   ],
   "source": [
    "!pygmentize train/model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a batch data generator\n",
    "\n",
    "When training on the dataset, we need to extract a batch size examples from the inputs and targets, forward and backward the RNN on them and then repite the iteration with another batch size examples. A batch generator will help us to extract a batch size examples from our datasets.\n",
    "\n",
    "The next code defines our batch generator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator_sequence(features_seq, label_seq, batch_size, seq_len):\n",
    "    \"\"\"Generator function that yields batches of data (input and target)\n",
    "\n",
    "    Args:\n",
    "        batch_size (int): number of examples (in this case, sentences) per batch.\n",
    "        max_length (int): maximum length of the output tensor.\n",
    "        NOTE: max_length includes the end-of-sentence character that will be added\n",
    "                to the tensor.  \n",
    "                Keep in mind that the length of the tensor is always 1 + the length\n",
    "                of the original line of characters.\n",
    "        input_lines (list): list of the input data to group into batches.\n",
    "        target_lines (list): list of the target data to group into batches.\n",
    "        shuffle (bool, optional): True if the generator should generate random batches of data. Defaults to True.\n",
    "\n",
    "    Yields:\n",
    "        tuple: two copies of the batch and the mask \n",
    "    \"\"\"\n",
    "    # calculate the number of batches we can supply\n",
    "    num_batches = len(features_seq) // (batch_size * seq_len)\n",
    "    if num_batches == 0:\n",
    "        raise ValueError(\"No batches created. Use smaller batch size or sequence length.\")\n",
    "    # calculate effective length of text to use\n",
    "    rounded_len = num_batches * batch_size * seq_len\n",
    "    # Reshape the features matrix in batch size x num_batches * seq_len\n",
    "    x = np.reshape(features_seq[: rounded_len], [batch_size, num_batches * seq_len])\n",
    "    \n",
    "    # Reshape the target matrix in batch size x num_batches * seq_len\n",
    "    y = np.reshape(label_seq[: rounded_len], [batch_size, num_batches * seq_len])\n",
    "    \n",
    "    epoch = 0\n",
    "    while True:\n",
    "        # roll so that no need to reset rnn states over epochs\n",
    "        x_epoch = np.split(np.roll(x, -epoch, axis=0), num_batches, axis=1)\n",
    "        y_epoch = np.split(np.roll(y, -epoch, axis=0), num_batches, axis=1)\n",
    "        for batch in range(num_batches):\n",
    "            yield x_epoch[batch], y_epoch[batch]\n",
    "        epoch += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing the training method\n",
    "\n",
    "Next we need to write the training code itself. This should be very similar to training methods that you have written before to train PyTorch models. We will leave any difficult aspects such as model saving / loading and parameter loading until a little later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_main(model, optimizer, loss_fn, batch_data, num_batches, val_batches, batch_size, seq_len, n_epochs, clip_norm, device):\n",
    "    # Training Run\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        start_time = time.time()\n",
    "        # Store the loss in every batch iteration\n",
    "        epoch_losses =[]\n",
    "        # Init the hidden state\n",
    "        hidden = model.init_state(device, batch_size)\n",
    "        # Train all the batches in every epoch\n",
    "        for i in range(num_batches-val_batches):\n",
    "            #print('Batch :', i)\n",
    "            # Get the next batch data for input and target\n",
    "            input_batch, target_batch = next(batch_data)\n",
    "            # Onr hot encode the input data\n",
    "            input_batch = one_hot_encode(input_batch, model.vocab_size)\n",
    "            # Tranform to tensor\n",
    "            input_data = torch.from_numpy(input_batch)\n",
    "            target_data = torch.from_numpy(target_batch)\n",
    "            # Create a new variable for the hidden state, necessary to calculate the gradients\n",
    "            hidden = tuple(([Variable(var.data) for var in hidden]))\n",
    "            # Move the input data to the device\n",
    "            input_data = input_data.to(device)\n",
    "            #print('Input shape: ', input_data.shape)\n",
    "            #print('Hidden shape: ', hidden[0].shape, hidden[1].shape)\n",
    "            # Set the model to train and prepare the gradients\n",
    "            model.train()\n",
    "            optimizer.zero_grad() # Clears existing gradients from previous epoch\n",
    "            # Pass Fordward the RNN\n",
    "            output, hidden = model(input_data, hidden)\n",
    "            #print('Output shape: ', output.shape)\n",
    "            output = output.to(device)\n",
    "            #print('Output shape: ', output.shape)\n",
    "            #print('Target shape; ', target_data.shape)\n",
    "            # Move the target data to the device\n",
    "            target_data = target_data.to(device)\n",
    "            #print('Target shape; ', target_data.shape)\n",
    "            target_data = torch.reshape(target_data, (batch_size*seq_len,))\n",
    "            #print('Target shape; ', target_data.shape)\n",
    "            loss = loss_fn(output, target_data.view(batch_size*seq_len))\n",
    "            #print(loss)\n",
    "            # Save the loss\n",
    "            epoch_losses.append(loss.item()) #data[0]\n",
    "        \n",
    "            loss.backward() # Does backpropagation and calculates gradients\n",
    "            # clip gradient norm\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), clip_norm)\n",
    "            \n",
    "            optimizer.step() # Updates the weights accordingly\n",
    "    \n",
    "        # Now, when epoch is finished, evaluate the model on validation data\n",
    "        model.eval()\n",
    "        val_hidden = model.init_state(device, batch_size)\n",
    "        val_losses = []\n",
    "        for i in range(val_batches):\n",
    "            # Get the next batch data for input and target\n",
    "            input_batch, target_batch = next(batch_data)\n",
    "            # Onr hot encode the input data\n",
    "            input_batch = one_hot_encode(input_batch, model.vocab_size)\n",
    "            # Tranform to tensor\n",
    "            input_data = torch.from_numpy(input_batch)\n",
    "            target_data = torch.from_numpy(target_batch)\n",
    "            # Create a new variable for the hidden state, necessary to calculate the gradients\n",
    "            hidden = tuple(([Variable(var.data) for var in val_hidden]))\n",
    "            # Move the input data to the device\n",
    "            input_data = input_data.to(device)\n",
    "            # Pass Fordward the RNN\n",
    "            output, hidden = model(input_data, hidden)\n",
    "            #print('Output shape: ', output.shape)\n",
    "            output = output.to(device)\n",
    "            #print('Output shape: ', output.shape)\n",
    "            #print('Target shape; ', target_data.shape)\n",
    "            # Move the target data to the device\n",
    "            target_data = target_data.to(device)\n",
    "            #print('Target shape; ', target_data.shape)\n",
    "            target_data = torch.reshape(target_data, (batch_size*seq_len,))\n",
    "            #print('Target shape; ', target_data.shape)\n",
    "            loss = loss_fn(output, target_data.view(batch_size*seq_len))\n",
    "            #print(loss)\n",
    "            # Save the loss\n",
    "            val_losses.append(loss.item()) #data[0]\n",
    "\n",
    "        model.train()                  \n",
    "        #if epoch%2 == 0:\n",
    "        print('Epoch: {}/{}.............'.format(epoch, n_epochs), end=' ')\n",
    "        print('Time: {:.4f}'.format(time.time() - start_time), end=' ')\n",
    "        print(\"Train Loss: {:.4f}\".format(np.mean(epoch_losses)), end=' ')\n",
    "        print(\"Val Loss: {:.4f}\".format(np.mean(val_losses)))\n",
    "        \n",
    "    return epoch_losses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supposing we have the training method above, we will test that it is working by writing a bit of code in the notebook that executes our training method on a small sample training set. Because we are not using a GPU and we are just testing the training code we take 50,000 characters from the input data. The reason for doing this in the notebook is so that we have an opportunity to fix any errors that arise early when they are easier to diagnose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/5............. Time: 0.3447 Train Loss: 3.2359 Val Loss: 2.9530\n",
      "Epoch: 2/5............. Time: 0.3499 Train Loss: 2.9400 Val Loss: 2.8528\n",
      "Epoch: 3/5............. Time: 0.3474 Train Loss: 2.7989 Val Loss: 2.6553\n",
      "Epoch: 4/5............. Time: 0.3487 Train Loss: 2.6213 Val Loss: 2.4888\n",
      "Epoch: 5/5............. Time: 0.3474 Train Loss: 2.5100 Val Loss: 2.3986\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from train.model import RNNModel\n",
    "\n",
    "# Set a seed to reproduce experiments\n",
    "torch.manual_seed(seed)\n",
    "# Set the device for training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Create the model\n",
    "model = RNNModel(38, 38, 16, 1).to(device)\n",
    "\n",
    "# Define Loss, Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Limit the size of our input sequence for this simple test\n",
    "input_seq = input_seq[:50000]\n",
    "target_seq = target_seq[:50000]\n",
    "\n",
    "# Calculate the number of batches to train\n",
    "batch_size=32\n",
    "maxlen=64\n",
    "num_batches = len(input_seq) // (batch_size*maxlen)\n",
    "# Calculate the validation batches\n",
    "val_frac = 0.1\n",
    "val_batches = int(num_batches*val_frac)\n",
    "\n",
    "# Create the batch data generator\n",
    "batch_data = batch_generator_sequence(input_seq, target_seq, batch_size, maxlen)\n",
    "# Train the model\n",
    "losses = train_main(model, optimizer, criterion, batch_data, num_batches, val_batches, batch_size, maxlen, 5, 5, device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to construct a PyTorch model using SageMaker we must provide SageMaker with a training script. We may optionally include a directory which will be copied to the container and from which our training code will be run. When the training container is executed it will check the uploaded directory (if there is one) for a `requirements.txt` file and install any required Python libraries, after which the training script will be run.\n",
    "\n",
    "In this example, we only requiere the numpy package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model\n",
    "\n",
    "When a PyTorch model is constructed in SageMaker, an entry point must be specified. This is the Python file which will be executed when the model is trained. Inside of the `train` directory is a file called `train.py` which contains most of the necessary code to train our model. \n",
    "\n",
    "**NOTICE**: The `train()` method written above and has been pasted into the `train/train.py` file where required.\n",
    "\n",
    "The way that SageMaker passes hyperparameters to the training script is by way of arguments. These arguments can then be parsed and used in the training script. To see how this is done take a look at the provided `train/train.py` file.\n",
    "\n",
    "First, we need to set which type of instance will run our training:\n",
    "- Local: We do not launch a real compute instance, just a container where our scripts will run. This scenario is very useful to test that the train script is working fine because it is faster to run a container than an compute instance. But finally, when we confirm that everything is working we must change the instance type for a \"real\" training instance.\n",
    "- ml.m4.4xlarge: It is a CPU instance\n",
    "- ml.p2.xlarge: A GPU instance to use when managing a big volume of data to train on.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the type of instance to use for training\n",
    "#instance_type='ml.m4.4xlarge' # CPU instance\n",
    "instance_type='ml.p2.xlarge' # GPU instance\n",
    "#instance_type='local'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "estimator = PyTorch(entry_point=\"train.py\",\n",
    "                    source_dir=\"train\",\n",
    "                    role=role,\n",
    "                    framework_version='0.4.0',\n",
    "                    train_instance_count=1,\n",
    "                    train_instance_type=instance_type,\n",
    "                    hyperparameters={\n",
    "                        'epochs': 50,\n",
    "                        'hidden_dim': 512,\n",
    "                        'n_layers': 2,\n",
    "                    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'create_image_uri' will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n",
      "'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n",
      "'create_image_uri' will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-09-04 18:09:13 Starting - Starting the training job...\n",
      "2020-09-04 18:09:15 Starting - Launching requested ML instances......\n",
      "2020-09-04 18:10:30 Starting - Preparing the instances for training.........\n",
      "2020-09-04 18:12:11 Downloading - Downloading input data......\n",
      "2020-09-04 18:13:01 Training - Downloading the training image..\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2020-09-04 18:13:23,643 sagemaker-containers INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2020-09-04 18:13:23,671 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2020-09-04 18:13:26,690 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2020-09-04 18:13:27,188 sagemaker-containers INFO     Module train does not provide a setup.py. \u001b[0m\n",
      "\u001b[34mGenerating setup.py\u001b[0m\n",
      "\u001b[34m2020-09-04 18:13:27,189 sagemaker-containers INFO     Generating setup.cfg\u001b[0m\n",
      "\u001b[34m2020-09-04 18:13:27,189 sagemaker-containers INFO     Generating MANIFEST.in\u001b[0m\n",
      "\u001b[34m2020-09-04 18:13:27,189 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[34m/usr/bin/python -m pip install -U . -r requirements.txt\u001b[0m\n",
      "\u001b[34mProcessing /opt/ml/code\u001b[0m\n",
      "\u001b[34mCollecting numpy (from -r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34m  Downloading https://files.pythonhosted.org/packages/b5/36/88723426b4ff576809fec7d73594fe17a35c27f8d01f93637637a29ae25b/numpy-1.18.5-cp35-cp35m-manylinux1_x86_64.whl (19.9MB)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: train\n",
      "  Running setup.py bdist_wheel for train: started\u001b[0m\n",
      "\u001b[34m  Running setup.py bdist_wheel for train: finished with status 'done'\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-7uiumycw/wheels/35/24/16/37574d11bf9bde50616c67372a334f94fa8356bc7164af8ca3\u001b[0m\n",
      "\u001b[34mSuccessfully built train\u001b[0m\n",
      "\u001b[34mInstalling collected packages: numpy, train\n",
      "  Found existing installation: numpy 1.15.4\n",
      "    Uninstalling numpy-1.15.4:\u001b[0m\n",
      "\u001b[34m      Successfully uninstalled numpy-1.15.4\u001b[0m\n",
      "\n",
      "2020-09-04 18:13:22 Training - Training image download completed. Training in progress.\u001b[34mSuccessfully installed numpy-1.18.5 train-1.0.0\u001b[0m\n",
      "\u001b[34mYou are using pip version 18.1, however version 20.2.2 is available.\u001b[0m\n",
      "\u001b[34mYou should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[34m2020-09-04 18:13:34,112 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"num_gpus\": 1,\n",
      "    \"resource_config\": {\n",
      "        \"network_interface_name\": \"eth0\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"current_host\": \"algo-1\"\n",
      "    },\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-223817798831/sagemaker-pytorch-2020-09-04-18-09-12-678/source/sourcedir.tar.gz\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"num_cpus\": 4,\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"log_level\": 20,\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"user_entry_point\": \"train.py\",\n",
      "    \"job_name\": \"sagemaker-pytorch-2020-09-04-18-09-12-678\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"hyperparameters\": {\n",
      "        \"epochs\": 50,\n",
      "        \"hidden_dim\": 512,\n",
      "        \"n_layers\": 2\n",
      "    },\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=50\u001b[0m\n",
      "\u001b[34mSM_HP_N_LAYERS=2\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":50,\"hidden_dim\":512,\"n_layers\":2},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"job_name\":\"sagemaker-pytorch-2020-09-04-18-09-12-678\",\"log_level\":20,\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-223817798831/sagemaker-pytorch-2020-09-04-18-09-12-678/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/usr/local/bin:/usr/lib/python35.zip:/usr/lib/python3.5:/usr/lib/python3.5/plat-x86_64-linux-gnu:/usr/lib/python3.5/lib-dynload:/usr/local/lib/python3.5/dist-packages:/usr/lib/python3/dist-packages\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HPS={\"epochs\":50,\"hidden_dim\":512,\"n_layers\":2}\u001b[0m\n",
      "\u001b[34mSM_HP_HIDDEN_DIM=512\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-223817798831/sagemaker-pytorch-2020-09-04-18-09-12-678/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--epochs\",\"50\",\"--hidden_dim\",\"512\",\"--n_layers\",\"2\"]\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/usr/bin/python -m train --epochs 50 --hidden_dim 512 --n_layers 2\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34mUsing device cuda.\u001b[0m\n",
      "\u001b[34mGet train data loader.\u001b[0m\n",
      "\u001b[34mModel loaded with embedding_dim 38, hidden_dim 512, vocab_size 38.\u001b[0m\n",
      "\u001b[34mEpoch: 1/50............. Time: 14.4431 Train Loss: 2.9868 Val Loss: 2.9684\u001b[0m\n",
      "\u001b[34mEpoch: 2/50............. Time: 14.4539 Train Loss: 2.8127 Val Loss: 2.5652\u001b[0m\n",
      "\u001b[34mEpoch: 3/50............. Time: 14.5240 Train Loss: 2.3785 Val Loss: 2.2880\u001b[0m\n",
      "\u001b[34mEpoch: 4/50............. Time: 14.6011 Train Loss: 2.1525 Val Loss: 2.0916\u001b[0m\n",
      "\u001b[34mEpoch: 5/50............. Time: 14.6164 Train Loss: 1.9958 Val Loss: 1.9362\u001b[0m\n",
      "\u001b[34mEpoch: 6/50............. Time: 14.6468 Train Loss: 1.8589 Val Loss: 1.8146\u001b[0m\n",
      "\u001b[34mEpoch: 7/50............. Time: 14.6547 Train Loss: 1.7433 Val Loss: 1.7126\u001b[0m\n",
      "\u001b[34mEpoch: 8/50............. Time: 14.6899 Train Loss: 1.6541 Val Loss: 1.6359\u001b[0m\n",
      "\u001b[34mEpoch: 9/50............. Time: 14.6877 Train Loss: 1.5832 Val Loss: 1.5861\u001b[0m\n",
      "\u001b[34mEpoch: 10/50............. Time: 14.9234 Train Loss: 1.5350 Val Loss: 1.5421\u001b[0m\n",
      "\u001b[34mEpoch: 11/50............. Time: 14.7145 Train Loss: 1.6048 Val Loss: 1.5478\u001b[0m\n",
      "\u001b[34mEpoch: 12/50............. Time: 14.8416 Train Loss: 1.4783 Val Loss: 1.4964\u001b[0m\n",
      "\u001b[34mEpoch: 13/50............. Time: 14.6023 Train Loss: 1.4303 Val Loss: 1.4738\u001b[0m\n",
      "\u001b[34mEpoch: 14/50............. Time: 14.6174 Train Loss: 1.3992 Val Loss: 1.4667\u001b[0m\n",
      "\u001b[34mEpoch: 15/50............. Time: 14.8158 Train Loss: 1.3759 Val Loss: 1.4611\u001b[0m\n",
      "\u001b[34mEpoch: 16/50............. Time: 15.3926 Train Loss: 1.3576 Val Loss: 1.4520\u001b[0m\n",
      "\u001b[34mEpoch: 17/50............. Time: 15.4225 Train Loss: 1.3431 Val Loss: 1.4551\u001b[0m\n",
      "\u001b[34mEpoch: 18/50............. Time: 15.7483 Train Loss: 1.3301 Val Loss: 1.4477\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mEpoch: 19/50............. Time: 15.5521 Train Loss: 1.3183 Val Loss: 1.4493\u001b[0m\n",
      "\u001b[34mEpoch: 20/50............. Time: 15.5506 Train Loss: 1.3071 Val Loss: 1.4519\u001b[0m\n",
      "\u001b[34mEpoch: 21/50............. Time: 15.8857 Train Loss: 1.2994 Val Loss: 1.4560\u001b[0m\n",
      "\u001b[34mEpoch: 22/50............. Time: 15.8872 Train Loss: 1.2918 Val Loss: 1.4624\u001b[0m\n",
      "\u001b[34mEpoch: 23/50............. Time: 15.5845 Train Loss: 1.2837 Val Loss: 1.4590\u001b[0m\n",
      "\u001b[34mEpoch: 24/50............. Time: 15.4095 Train Loss: 1.2769 Val Loss: 1.4608\u001b[0m\n",
      "\u001b[34mEpoch: 25/50............. Time: 15.3529 Train Loss: 1.2723 Val Loss: 1.4590\u001b[0m\n",
      "\u001b[34mEpoch: 26/50............. Time: 15.4680 Train Loss: 1.2655 Val Loss: 1.4589\u001b[0m\n",
      "\u001b[34mEpoch: 27/50............. Time: 15.2459 Train Loss: 1.2594 Val Loss: 1.4576\u001b[0m\n",
      "\u001b[34mEpoch: 28/50............. Time: 14.8407 Train Loss: 1.2541 Val Loss: 1.4611\u001b[0m\n",
      "\u001b[34mEpoch: 29/50............. Time: 14.8493 Train Loss: 1.2491 Val Loss: 1.4626\u001b[0m\n",
      "\u001b[34mEpoch: 30/50............. Time: 14.8030 Train Loss: 1.2439 Val Loss: 1.4673\u001b[0m\n",
      "\u001b[34mEpoch: 31/50............. Time: 14.8256 Train Loss: 1.2406 Val Loss: 1.4672\u001b[0m\n",
      "\u001b[34mEpoch: 32/50............. Time: 14.8147 Train Loss: 1.2360 Val Loss: 1.4802\u001b[0m\n",
      "\u001b[34mEpoch: 33/50............. Time: 14.8162 Train Loss: 1.2309 Val Loss: 1.4683\u001b[0m\n",
      "\u001b[34mEpoch: 34/50............. Time: 14.7752 Train Loss: 1.2273 Val Loss: 1.4689\u001b[0m\n",
      "\u001b[34mEpoch: 35/50............. Time: 14.8036 Train Loss: 1.2238 Val Loss: 1.4734\u001b[0m\n",
      "\u001b[34mEpoch: 36/50............. Time: 14.9146 Train Loss: 1.2201 Val Loss: 1.4775\u001b[0m\n",
      "\u001b[34mEpoch: 37/50............. Time: 14.7848 Train Loss: 1.2188 Val Loss: 1.4706\u001b[0m\n",
      "\u001b[34mEpoch: 38/50............. Time: 14.8986 Train Loss: 1.2141 Val Loss: 1.4750\u001b[0m\n",
      "\u001b[34mEpoch: 39/50............. Time: 15.3889 Train Loss: 1.2104 Val Loss: 1.4759\u001b[0m\n",
      "\u001b[34mEpoch: 40/50............. Time: 15.4683 Train Loss: 1.2065 Val Loss: 1.4744\u001b[0m\n",
      "\u001b[34mEpoch: 41/50............. Time: 14.8328 Train Loss: 1.2049 Val Loss: 1.4741\u001b[0m\n",
      "\u001b[34mEpoch: 42/50............. Time: 14.8112 Train Loss: 1.2023 Val Loss: 1.4774\u001b[0m\n",
      "\u001b[34mEpoch: 43/50............. Time: 14.8114 Train Loss: 1.1987 Val Loss: 1.4803\u001b[0m\n",
      "\u001b[34mEpoch: 44/50............. Time: 14.7854 Train Loss: 1.1956 Val Loss: 1.4837\u001b[0m\n",
      "\u001b[34mEpoch: 45/50............. Time: 14.8066 Train Loss: 1.1929 Val Loss: 1.4888\u001b[0m\n",
      "\u001b[34mEpoch: 46/50............. Time: 14.9014 Train Loss: 1.1889 Val Loss: 1.4856\u001b[0m\n",
      "\u001b[34mEpoch: 47/50............. Time: 14.7856 Train Loss: 1.1877 Val Loss: 1.4897\u001b[0m\n",
      "\u001b[34mEpoch: 48/50............. Time: 14.7771 Train Loss: 1.1857 Val Loss: 1.4911\u001b[0m\n",
      "\u001b[34mEpoch: 49/50............. Time: 15.2428 Train Loss: 1.1826 Val Loss: 1.4963\u001b[0m\n",
      "\n",
      "2020-09-04 18:26:23 Uploading - Uploading generated training model\n",
      "2020-09-04 18:26:23 Completed - Training job completed\n",
      "\u001b[34mEpoch: 50/50............. Time: 15.3424 Train Loss: 1.1790 Val Loss: 1.4912\u001b[0m\n",
      "\u001b[34m2020-09-04 18:26:12,138 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "Training seconds: 852\n",
      "Billable seconds: 852\n"
     ]
    }
   ],
   "source": [
    "estimator.fit({'training': input_data})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Testing the model\n",
    "\n",
    "As mentioned at the top of this notebook, we will be testing this model by first deploying it and then sending the testing data to the deployed endpoint. We will do this so that we can make sure that the deployed model is working correctly.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6 - Deploy the model for inference\n",
    "\n",
    "Now that our model is trained, it's time to create some custom inference code so that we can send the model a initial string which has not been processed and determine the next caracters on the string.\n",
    "\n",
    "By default the estimator which we created, when deployed, will use the entry script and directory which we provided when creating the model. However, since we wish to accept a string as input and our model expects a processed review, we need to write some custom inference code.\n",
    "\n",
    "We will store the code that we write in the `serve` directory. Provided in this directory is the `model.py` file that we used to construct our model, a `utils.py` file which contains the `one-hot-encode` and `encode_text` pre-processing functions which we used during the initial data processing, and `predict.py`, the file which will contain our custom inference code. Note also that `requirements.txt` is present which will tell SageMaker what Python libraries are required by our custom inference code.\n",
    "\n",
    "When deploying a PyTorch model in SageMaker, you are expected to provide four functions which the SageMaker inference container will use.\n",
    " - `model_fn`: This function is the same function that we used in the training script and it tells SageMaker how to load our model. This function must be called `model_fn()` and takes as its only parameter a path to the directory where the model artifacts are stored. This function must also be present in the python file which we specified as the entry point. It also reads the saved dictionaries because they could be used during the inference process.\n",
    " - `input_fn`: This function receives the raw serialized input that has been sent to the model's endpoint and its job is to de-serialize and make the input available for the inference code. Latter we will mention what our input_fn function is doing.\n",
    " - `output_fn`: This function takes the output of the inference code and its job is to serialize this output and return it to the caller of the model's endpoint.\n",
    " - `predict_fn`: The heart of the inference script, this is where the actual prediction is done and is the function which you will need to complete.\n",
    "\n",
    "For the simple example that we are constructing during this project, the `input_fn` and `output_fn` methods are relatively straightforward. We require being able to accept a string as input, *composed by the desired length of the output and the initial string*. And we expect to return a single string as output, the new text generated. You might imagine though that in a more complex application the input or output may be image data or some other binary data which would require some effort to serialize.\n",
    "\n",
    "### Writing inference code\n",
    "\n",
    "Before writing our custom inference code, we will begin by taking a look at the code which has been provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpickle\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msys\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msagemaker_containers\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mnn\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnn\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mmodel\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m RNNModel\r\n",
      "\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mutils\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m clean_text, encode_text, one_hot_encode\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mmodel_fn\u001b[39;49;00m(model_dir):\r\n",
      "    \u001b[33m\"\"\"Load the PyTorch model from the `model_dir` directory.\"\"\"\u001b[39;49;00m\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mLoading model.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "\r\n",
      "    \u001b[37m# First, load the parameters used to create the model.\u001b[39;49;00m\r\n",
      "    model_info = {}\r\n",
      "    model_info_path = os.path.join(model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mmodel_info.pth\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(model_info_path, \u001b[33m'\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\r\n",
      "        model_info = torch.load(f)\r\n",
      "\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mmodel_info: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(model_info))\r\n",
      "\r\n",
      "    \u001b[37m# Determine the device and construct the model.\u001b[39;49;00m\r\n",
      "    device = torch.device(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m torch.cuda.is_available() \u001b[34melse\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    model = RNNModel(model_info[\u001b[33m'\u001b[39;49;00m\u001b[33mvocab_size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], model_info[\u001b[33m'\u001b[39;49;00m\u001b[33membedding_dim\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], model_info[\u001b[33m'\u001b[39;49;00m\u001b[33mhidden_dim\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], model_info[\u001b[33m'\u001b[39;49;00m\u001b[33mn_layers\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], model_info[\u001b[33m'\u001b[39;49;00m\u001b[33mdrop_rate\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\r\n",
      "\r\n",
      "    \u001b[37m# Load the stored model parameters.\u001b[39;49;00m\r\n",
      "    model_path = os.path.join(model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mmodel.pth\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(model_path, \u001b[33m'\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\r\n",
      "        model.load_state_dict(torch.load(f, map_location=\u001b[34mlambda\u001b[39;49;00m storage, loc: storage))\r\n",
      "\r\n",
      "    \u001b[37m# Load the saved word_dict.\u001b[39;49;00m\r\n",
      "    word_dict_path = os.path.join(model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mchar_dict.pkl\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(word_dict_path, \u001b[33m'\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\r\n",
      "        model.char2int = pickle.load(f)\r\n",
      "\r\n",
      "    word_dict_path = os.path.join(model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mint_dict.pkl\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(word_dict_path, \u001b[33m'\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\r\n",
      "        model.int2char = pickle.load(f)\r\n",
      "\r\n",
      "    model.to(device).eval()\r\n",
      "\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mDone loading model.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m model\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32minput_fn\u001b[39;49;00m(serialized_input_data, content_type):\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mDeserializing the input data.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \u001b[34mif\u001b[39;49;00m content_type == \u001b[33m'\u001b[39;49;00m\u001b[33mtext/plain\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\r\n",
      "        data = serialized_input_data.decode(\u001b[33m'\u001b[39;49;00m\u001b[33mutf-8\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "        \u001b[37m# Extract the desired length of the output string\u001b[39;49;00m\r\n",
      "        sep_pos= data.find(\u001b[33m'\u001b[39;49;00m\u001b[33m-\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "        \u001b[34mif\u001b[39;49;00m sep_pos > \u001b[34m0\u001b[39;49;00m:\r\n",
      "            length = data[:sep_pos]\r\n",
      "            initial_string = data[sep_pos+\u001b[34m1\u001b[39;49;00m:]\r\n",
      "        \u001b[34mreturn\u001b[39;49;00m (length,initial_string)\r\n",
      "    \u001b[34mraise\u001b[39;49;00m \u001b[36mException\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mRequested unsupported ContentType in content_type: \u001b[39;49;00m\u001b[33m'\u001b[39;49;00m + content_type)\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32moutput_fn\u001b[39;49;00m(prediction_output, accept):\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mSerializing the generated output.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m \u001b[36mstr\u001b[39;49;00m(prediction_output)\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32msample_from_probs\u001b[39;49;00m(probs, top_n=\u001b[34m10\u001b[39;49;00m):\r\n",
      "    \u001b[33m\"\"\"\u001b[39;49;00m\r\n",
      "\u001b[33m    truncated weighted random choice.\u001b[39;49;00m\r\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\r\n",
      "    _, indices = torch.sort(probs)\r\n",
      "    \u001b[37m# set probabilities after top_n to 0\u001b[39;49;00m\r\n",
      "    probs[indices.data[:-top_n]] = \u001b[34m0\u001b[39;49;00m\r\n",
      "    \u001b[37m#print(probs.shape)\u001b[39;49;00m\r\n",
      "    sampled_index = torch.multinomial(probs, \u001b[34m1\u001b[39;49;00m)\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m sampled_index\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mpredict_probs\u001b[39;49;00m(model, hidden, character, vocab, device):\r\n",
      "    \u001b[37m# One-hot encoding our input to fit into the model\u001b[39;49;00m\r\n",
      "    character = np.array([[vocab[c] \u001b[34mfor\u001b[39;49;00m c \u001b[35min\u001b[39;49;00m character]])\r\n",
      "    character = one_hot_encode(character, \u001b[36mlen\u001b[39;49;00m(vocab))\r\n",
      "    character = torch.from_numpy(character)\r\n",
      "    character = character.to(device)\r\n",
      "    \r\n",
      "    \u001b[34mwith\u001b[39;49;00m torch.no_grad():\r\n",
      "        out, hidden = model(character, hidden)\r\n",
      "\r\n",
      "    prob = nn.functional.softmax(out[-\u001b[34m1\u001b[39;49;00m], dim=\u001b[34m0\u001b[39;49;00m).data\r\n",
      "\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m prob, hidden\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mpredict_old\u001b[39;49;00m(input_data, model):\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mInferring sentiment of input data.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "    device = torch.device(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m torch.cuda.is_available() \u001b[34melse\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    \r\n",
      "    \u001b[34mif\u001b[39;49;00m model.char2int \u001b[35mis\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m:\r\n",
      "        \u001b[34mraise\u001b[39;49;00m \u001b[36mException\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mModel has not been loaded properly, no word_dict.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \r\n",
      "    model.eval() \u001b[37m# eval mode\u001b[39;49;00m\r\n",
      "    start = input_data.lower()\r\n",
      "    \u001b[37m# Clean the text as the text used in training \u001b[39;49;00m\r\n",
      "    start = clean_text(start, \u001b[34mTrue\u001b[39;49;00m)\r\n",
      "    \u001b[37m# Encode the text\u001b[39;49;00m\r\n",
      "    \u001b[37m#encode_text(start, model.vocab, one_hot = False):\u001b[39;49;00m\r\n",
      "    \u001b[37m# First off, run through the starting characters\u001b[39;49;00m\r\n",
      "    chars = [ch \u001b[34mfor\u001b[39;49;00m ch \u001b[35min\u001b[39;49;00m start]\r\n",
      "\r\n",
      "    state = model.init_state(device, \u001b[34m1\u001b[39;49;00m)\r\n",
      "    probs, state = predict_probs(model, state, chars, model.vocab, device)\r\n",
      "    \u001b[37m#print(probs.shape)\u001b[39;49;00m\r\n",
      "    \u001b[37m#probs = torch.transpose(probs, 0, 1)\u001b[39;49;00m\r\n",
      "    next_index = sample_from_probs(probs[-\u001b[34m1\u001b[39;49;00m].squeeze(), top_n=\u001b[34m3\u001b[39;49;00m)\r\n",
      "    \r\n",
      "    \u001b[34mreturn\u001b[39;49;00m next_index.data[\u001b[34m0\u001b[39;49;00m].item()\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mpredict_fn\u001b[39;49;00m(input_data, model):\r\n",
      "\r\n",
      "    device = torch.device(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m torch.cuda.is_available() \u001b[34melse\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    \r\n",
      "    \u001b[34mif\u001b[39;49;00m model.char2int \u001b[35mis\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m:\r\n",
      "        \u001b[34mraise\u001b[39;49;00m \u001b[36mException\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mModel has not been loaded properly, no word_dict.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \r\n",
      "    \u001b[37m# TODO: Process input_data so that it is ready to be sent to our model.\u001b[39;49;00m\r\n",
      "    \u001b[37m#       You should produce two variables:\u001b[39;49;00m\r\n",
      "    \u001b[37m#         data_X   - A sequence of length 500 which represents the converted review\u001b[39;49;00m\r\n",
      "    \u001b[37m#         data_len - The length of the review\u001b[39;49;00m\r\n",
      "    \u001b[37m# Extract the input data and the desired length\u001b[39;49;00m\r\n",
      "    out_len, start = input_data\r\n",
      "    out_len = \u001b[36mint\u001b[39;49;00m(out_len)\r\n",
      "\r\n",
      "    model.eval() \u001b[37m# eval mode\u001b[39;49;00m\r\n",
      "    start = start.lower()\r\n",
      "    \u001b[37m# Clean the text as the text used in training \u001b[39;49;00m\r\n",
      "    start = clean_text(start, \u001b[34mTrue\u001b[39;49;00m)\r\n",
      "    \u001b[37m# First off, run through the starting characters\u001b[39;49;00m\r\n",
      "    chars = [ch \u001b[34mfor\u001b[39;49;00m ch \u001b[35min\u001b[39;49;00m start]\r\n",
      "    size = out_len - \u001b[36mlen\u001b[39;49;00m(chars)\r\n",
      "    \u001b[37m# Init the hidden state\u001b[39;49;00m\r\n",
      "    state = model.init_state(device, \u001b[34m1\u001b[39;49;00m)\r\n",
      "\r\n",
      "    \u001b[37m# Warm up the initial state, predicting on the initial string\u001b[39;49;00m\r\n",
      "    \u001b[34mfor\u001b[39;49;00m ch \u001b[35min\u001b[39;49;00m chars:\r\n",
      "        \u001b[37m#char, state = predict(model, ch, state, top_n=top_k)\u001b[39;49;00m\r\n",
      "        probs, state = predict_probs(model, state, ch, model.char2int, device)\r\n",
      "        next_index = sample_from_probs(probs, \u001b[34m5\u001b[39;49;00m)\r\n",
      "\r\n",
      "    \u001b[37m# Include the last char predicted to the predicted output\u001b[39;49;00m\r\n",
      "    chars.append(model.int2char[next_index.data[\u001b[34m0\u001b[39;49;00m]])   \r\n",
      "    \u001b[37m# Now pass in the previous characters and get a new one\u001b[39;49;00m\r\n",
      "    \u001b[34mfor\u001b[39;49;00m ii \u001b[35min\u001b[39;49;00m \u001b[36mrange\u001b[39;49;00m(size-\u001b[34m1\u001b[39;49;00m):\r\n",
      "        \u001b[37m#char, h = predict_char(model, chars, vocab)\u001b[39;49;00m\r\n",
      "        probs, state = predict_probs(model, state, chars[-\u001b[34m1\u001b[39;49;00m], model.char2int, device)\r\n",
      "        next_index = sample_from_probs(probs, \u001b[34m5\u001b[39;49;00m)\r\n",
      "        \u001b[37m# append to sequence\u001b[39;49;00m\r\n",
      "        chars.append(model.int2char[next_index.data[\u001b[34m0\u001b[39;49;00m]])\r\n",
      "\r\n",
      "    \u001b[37m# Join all the chars    \u001b[39;49;00m\r\n",
      "    \u001b[37m#chars = chars.decode('utf-8')\u001b[39;49;00m\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m \u001b[33m'\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.join(chars)\r\n",
      "    \r\n"
     ]
    }
   ],
   "source": [
    "!pygmentize serve/predict.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned earlier, the `model_fn` method is the same as the one provided in the training code and the `input_fn` and `output_fn` methods are very simple. Finally we must build a `predict_fn` method that will receive the input string, encode it (char2int), one-hot-encode and send it to the model. Every output will be decode (int2char) and appended to the final output string. \n",
    "\n",
    "Make sure that you save the completed file as `predict.py` in the `serve` directory.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploying the model\n",
    "\n",
    "Now that the custom inference code has been written, we will create and deploy our model. To begin with, we need to construct a new PyTorchModel object which points to the model artifacts created during training and also points to the inference code that we wish to use. Then we can call the deploy method to launch the deployment container.\n",
    "\n",
    "**NOTE**: The default behaviour for a deployed PyTorch model is to assume that any input passed to the predictor is a `numpy` array. In our case we want to send a string so we need to construct a simple wrapper around the `RealTimePredictor` class to accomodate simple strings. In a more complicated situation you may want to provide a serialization object, for example if you wanted to sent image data.\n",
    "\n",
    "**NOTE:** When deploying a model you are asking SageMaker to launch an compute instance that will wait for data to be sent to it. As a result, this compute instance will continue to run until *you* shut it down. This is important to know since the cost of a deployed endpoint depends on how long it has been running for.\n",
    "\n",
    "In other words **If you are no longer using a deployed endpoint, shut it down!**\n",
    "\n",
    "Now, we can deploy our trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading a previously trained model\n",
    "\n",
    "In many situations, you have trained the model in another execution of this notebook or you shutdown the notebook while the model was training. So the estimator variable is empty or undefined and then, you want to restore or use a previous training job and deploy it. In the next cell, we attach that trained model to the estimator variable and continue the necessary steps to launch and deploy the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-09-02 20:00:18 Starting - Preparing the instances for training\n",
      "2020-09-02 20:00:18 Downloading - Downloading input data\n",
      "2020-09-02 20:00:18 Training - Training image download completed. Training in progress.\n",
      "2020-09-02 20:00:18 Uploading - Uploading generated training model\n",
      "2020-09-02 20:00:18 Completed - Training job completed\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2020-09-02 19:53:51,432 sagemaker-containers INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2020-09-02 19:53:51,457 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2020-09-02 19:53:51,461 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2020-09-02 19:53:51,761 sagemaker-containers INFO     Module train does not provide a setup.py. \u001b[0m\n",
      "\u001b[34mGenerating setup.py\u001b[0m\n",
      "\u001b[34m2020-09-02 19:53:51,761 sagemaker-containers INFO     Generating setup.cfg\u001b[0m\n",
      "\u001b[34m2020-09-02 19:53:51,761 sagemaker-containers INFO     Generating MANIFEST.in\u001b[0m\n",
      "\u001b[34m2020-09-02 19:53:51,761 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[34m/usr/bin/python -m pip install -U . -r requirements.txt\u001b[0m\n",
      "\u001b[34mProcessing /opt/ml/code\u001b[0m\n",
      "\u001b[34mCollecting pandas (from -r requirements.txt (line 1))\n",
      "  Downloading https://files.pythonhosted.org/packages/74/24/0cdbf8907e1e3bc5a8da03345c23cbed7044330bb8f73bb12e711a640a00/pandas-0.24.2-cp35-cp35m-manylinux1_x86_64.whl (10.0MB)\u001b[0m\n",
      "\u001b[34mCollecting numpy (from -r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34m  Downloading https://files.pythonhosted.org/packages/b5/36/88723426b4ff576809fec7d73594fe17a35c27f8d01f93637637a29ae25b/numpy-1.18.5-cp35-cp35m-manylinux1_x86_64.whl (19.9MB)\u001b[0m\n",
      "\u001b[34mCollecting pytz>=2011k (from pandas->-r requirements.txt (line 1))\n",
      "  Downloading https://files.pythonhosted.org/packages/4f/a4/879454d49688e2fad93e59d7d4efda580b783c745fd2ec2a3adf87b0808d/pytz-2020.1-py2.py3-none-any.whl (510kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied, skipping upgrade: python-dateutil>=2.5.0 in /usr/local/lib/python3.5/dist-packages (from pandas->-r requirements.txt (line 1)) (2.7.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied, skipping upgrade: six>=1.5 in /usr/local/lib/python3.5/dist-packages (from python-dateutil>=2.5.0->pandas->-r requirements.txt (line 1)) (1.11.0)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: train\n",
      "  Running setup.py bdist_wheel for train: started\u001b[0m\n",
      "\u001b[34m  Running setup.py bdist_wheel for train: finished with status 'done'\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-9xt478wn/wheels/35/24/16/37574d11bf9bde50616c67372a334f94fa8356bc7164af8ca3\u001b[0m\n",
      "\u001b[34mSuccessfully built train\u001b[0m\n",
      "\u001b[34mInstalling collected packages: pytz, numpy, pandas, train\n",
      "  Found existing installation: numpy 1.15.4\n",
      "    Uninstalling numpy-1.15.4:\u001b[0m\n",
      "\u001b[34m      Successfully uninstalled numpy-1.15.4\u001b[0m\n",
      "\u001b[34mSuccessfully installed numpy-1.18.5 pandas-0.24.2 pytz-2020.1 train-1.0.0\u001b[0m\n",
      "\u001b[34mYou are using pip version 18.1, however version 20.2.2 is available.\u001b[0m\n",
      "\u001b[34mYou should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[34m2020-09-02 19:54:01,620 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"log_level\": 20,\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"job_name\": \"sagemaker-pytorch-2020-09-02-19-49-57-475\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"user_entry_point\": \"train.py\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-223817798831/sagemaker-pytorch-2020-09-02-19-49-57-475/source/sourcedir.tar.gz\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"hyperparameters\": {\n",
      "        \"epochs\": 3,\n",
      "        \"hidden_dim\": 64\n",
      "    },\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"resource_config\": {\n",
      "        \"network_interface_name\": \"eth0\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"current_host\": \"algo-1\"\n",
      "    },\n",
      "    \"num_gpus\": 1,\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"module_name\": \"train\",\n",
      "    \"current_host\": \"algo-1\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=3\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_HPS={\"epochs\":3,\"hidden_dim\":64}\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":3,\"hidden_dim\":64},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"job_name\":\"sagemaker-pytorch-2020-09-02-19-49-57-475\",\"log_level\":20,\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-223817798831/sagemaker-pytorch-2020-09-02-19-49-57-475/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/usr/local/bin:/usr/lib/python35.zip:/usr/lib/python3.5:/usr/lib/python3.5/plat-x86_64-linux-gnu:/usr/lib/python3.5/lib-dynload:/usr/local/lib/python3.5/dist-packages:/usr/lib/python3/dist-packages\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--epochs\",\"3\",\"--hidden_dim\",\"64\"]\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-223817798831/sagemaker-pytorch-2020-09-02-19-49-57-475/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_HP_HIDDEN_DIM=64\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/usr/bin/python -m train --epochs 3 --hidden_dim 64\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34mUsing device cuda.\u001b[0m\n",
      "\u001b[34mGet train data loader.\u001b[0m\n",
      "\u001b[34mModel loaded with embedding_dim 28, hidden_dim 64, vocab_size 28.\u001b[0m\n",
      "\u001b[34mEpoch: 1/3............. Loss: 1.5655\u001b[0m\n",
      "\u001b[34mEpoch: 2/3............. Loss: 1.5143\u001b[0m\n",
      "\u001b[34mEpoch: 3/3............. Loss: 1.5086\u001b[0m\n",
      "\u001b[34m2020-09-02 20:00:09,820 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "Training seconds: 453\n",
      "Billable seconds: 453\n"
     ]
    }
   ],
   "source": [
    "# Attach the estimator to a oreviously trained job\n",
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "my_training_job_name = 'sagemaker-pytorch-2020-09-02-19-49-57-475'\n",
    "\n",
    "estimator = PyTorch.attach(my_training_job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter image will be renamed to image_uri in SageMaker Python SDK v2.\n",
      "'create_image_uri' will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------!"
     ]
    }
   ],
   "source": [
    "from sagemaker.predictor import RealTimePredictor\n",
    "from sagemaker.pytorch import PyTorchModel\n",
    "\n",
    "class StringPredictor(RealTimePredictor):\n",
    "    def __init__(self, endpoint_name, sagemaker_session):\n",
    "        super(StringPredictor, self).__init__(endpoint_name, sagemaker_session, content_type='text/plain')\n",
    "\n",
    "model = PyTorchModel(model_data=estimator.model_data,\n",
    "                     role = role,\n",
    "                     framework_version='0.4.0',\n",
    "                     entry_point='predict.py',\n",
    "                     source_dir='serve',\n",
    "                     predictor_cls=StringPredictor)\n",
    "predictor = model.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7 - Use the model for testing\n",
    "\n",
    "Now that we have deployed our model with the custom inference code, we should test it to see if everything is working. Here we test our model by creating an initial string and send it to the endpoint, then collect the result. But we also want to tell the inference how long should be the expected output. \n",
    "\n",
    "It means that we need to send to our predict function not only the initial string but the the length of the output. As the expected input by the deserialized function, input_fn, is a string and we are looking for a simple solution, **our input data will be a string composed by: the length of the output+'-'+initial string.**\n",
    "\n",
    "Now, it is time to test our model, sending a very common initial string: `you are `.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you are think too the starry a my seens weeped as he to be then that tonight we was shall be wear my\n"
     ]
    }
   ],
   "source": [
    "test_text = '100-you are '\n",
    "new_text = predictor.predict(test_text).decode('utf-8')\n",
    "print(new_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another example to try out is to send the model a text included in the training dataset and see what the model predicts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text:  he did content to say it was for his country he did it to please his mother and to be partly proud; which he is, even till the altitude of his virtue. what he cannot help in his nature,\n",
      "Init text:  he did content to say it was for his country he did it to\n",
      "he did content to say it was for his country he did it to please his mother and to be partly proud which he is even till the altitude of his virtue what he cannot help in his nature of \n"
     ]
    }
   ],
   "source": [
    "print('Text: ',sentences[963:1148])\n",
    "init_text = sentences[963:1148]\n",
    "print('Init text: ', sentences[963:1020])\n",
    "test_text = str(len(init_text))+'-'+init_text\n",
    "new_text = predictor.predict(test_text).decode('utf-8')\n",
    "print(new_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check that the model \"remember\" the texts during the training, it can mostly reproduce the original text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know our endpoint is working as expected, we can set up a web page or app that will interact with it.\n",
    "\n",
    "**Make sure to skip down to the end of this notebook and shut down your endpoint. You can deploy it again when you come back.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete the endpoint\n",
    "\n",
    "Remember to always shut down your endpoint if you are no longer using it. You are charged for the length of time that the endpoint is running so if you forget and leave it on you could end up with an unexpectedly large bill."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
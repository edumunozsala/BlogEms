<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Intro to the Encoder-Decoder model and the Attention mechanism | fastpages</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Intro to the Encoder-Decoder model and the Attention mechanism" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Implementing an encoder-decoder model using RNNs model with Tensorflow 2, then describe the Attention mechanism and finally build an decoder with the Luong’s attention. we will apply this encoder-decoder with attention to a neural machine translation problem, translating texts from English to Spanish" />
<meta property="og:description" content="Implementing an encoder-decoder model using RNNs model with Tensorflow 2, then describe the Attention mechanism and finally build an decoder with the Luong’s attention. we will apply this encoder-decoder with attention to a neural machine translation problem, translating texts from English to Spanish" />
<link rel="canonical" href="https://edumunozsala.github.io/BlogEms/fastpages/jupyter/encoder-decoder/lstm/attention/tensorflow%202/2020/10/07/Intro-seq2seq-Encoder-Decoder-ENG-SPA-translator-tf2.html" />
<meta property="og:url" content="https://edumunozsala.github.io/BlogEms/fastpages/jupyter/encoder-decoder/lstm/attention/tensorflow%202/2020/10/07/Intro-seq2seq-Encoder-Decoder-ENG-SPA-translator-tf2.html" />
<meta property="og:site_name" content="fastpages" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-10-07T00:00:00-05:00" />
<script type="application/ld+json">
{"headline":"Intro to the Encoder-Decoder model and the Attention mechanism","description":"Implementing an encoder-decoder model using RNNs model with Tensorflow 2, then describe the Attention mechanism and finally build an decoder with the Luong’s attention. we will apply this encoder-decoder with attention to a neural machine translation problem, translating texts from English to Spanish","datePublished":"2020-10-07T00:00:00-05:00","dateModified":"2020-10-07T00:00:00-05:00","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://edumunozsala.github.io/BlogEms/fastpages/jupyter/encoder-decoder/lstm/attention/tensorflow%202/2020/10/07/Intro-seq2seq-Encoder-Decoder-ENG-SPA-translator-tf2.html"},"url":"https://edumunozsala.github.io/BlogEms/fastpages/jupyter/encoder-decoder/lstm/attention/tensorflow%202/2020/10/07/Intro-seq2seq-Encoder-Decoder-ENG-SPA-translator-tf2.html","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/BlogEms/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://edumunozsala.github.io/BlogEms/feed.xml" title="fastpages" /><link rel="shortcut icon" type="image/x-icon" href="/BlogEms/images/favicon.ico"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Intro to the Encoder-Decoder model and the Attention mechanism | fastpages</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Intro to the Encoder-Decoder model and the Attention mechanism" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Implementing an encoder-decoder model using RNNs model with Tensorflow 2, then describe the Attention mechanism and finally build an decoder with the Luong’s attention. we will apply this encoder-decoder with attention to a neural machine translation problem, translating texts from English to Spanish" />
<meta property="og:description" content="Implementing an encoder-decoder model using RNNs model with Tensorflow 2, then describe the Attention mechanism and finally build an decoder with the Luong’s attention. we will apply this encoder-decoder with attention to a neural machine translation problem, translating texts from English to Spanish" />
<link rel="canonical" href="https://edumunozsala.github.io/BlogEms/fastpages/jupyter/encoder-decoder/lstm/attention/tensorflow%202/2020/10/07/Intro-seq2seq-Encoder-Decoder-ENG-SPA-translator-tf2.html" />
<meta property="og:url" content="https://edumunozsala.github.io/BlogEms/fastpages/jupyter/encoder-decoder/lstm/attention/tensorflow%202/2020/10/07/Intro-seq2seq-Encoder-Decoder-ENG-SPA-translator-tf2.html" />
<meta property="og:site_name" content="fastpages" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-10-07T00:00:00-05:00" />
<script type="application/ld+json">
{"headline":"Intro to the Encoder-Decoder model and the Attention mechanism","description":"Implementing an encoder-decoder model using RNNs model with Tensorflow 2, then describe the Attention mechanism and finally build an decoder with the Luong’s attention. we will apply this encoder-decoder with attention to a neural machine translation problem, translating texts from English to Spanish","datePublished":"2020-10-07T00:00:00-05:00","dateModified":"2020-10-07T00:00:00-05:00","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://edumunozsala.github.io/BlogEms/fastpages/jupyter/encoder-decoder/lstm/attention/tensorflow%202/2020/10/07/Intro-seq2seq-Encoder-Decoder-ENG-SPA-translator-tf2.html"},"url":"https://edumunozsala.github.io/BlogEms/fastpages/jupyter/encoder-decoder/lstm/attention/tensorflow%202/2020/10/07/Intro-seq2seq-Encoder-Decoder-ENG-SPA-translator-tf2.html","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

<link href="https://unpkg.com/@primer/css/dist/primer.css" rel="stylesheet" />
<link rel="stylesheet" href="//use.fontawesome.com/releases/v5.0.7/css/all.css"><link type="application/atom+xml" rel="alternate" href="https://edumunozsala.github.io/BlogEms/feed.xml" title="fastpages" />

<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head><body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/BlogEms/">fastpages</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/BlogEms/about/">About Me</a><a class="page-link" href="/BlogEms/search/">Search</a><a class="page-link" href="/BlogEms/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Intro to the Encoder-Decoder model and the Attention mechanism</h1><p class="page-description">Implementing an encoder-decoder model using RNNs model with Tensorflow 2, then describe the Attention mechanism and finally build an decoder with the Luong's attention. we will apply this encoder-decoder with attention to a neural machine translation problem, translating texts from English to Spanish</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2020-10-07T00:00:00-05:00" itemprop="datePublished">
        Oct 7, 2020
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      35 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/BlogEms/categories/#fastpages">fastpages</a>
        &nbsp;
      
        <a class="category-tags-link" href="/BlogEms/categories/#jupyter">jupyter</a>
        &nbsp;
      
        <a class="category-tags-link" href="/BlogEms/categories/#encoder-decoder">encoder-decoder</a>
        &nbsp;
      
        <a class="category-tags-link" href="/BlogEms/categories/#LSTM">LSTM</a>
        &nbsp;
      
        <a class="category-tags-link" href="/BlogEms/categories/#attention">attention</a>
        &nbsp;
      
        <a class="category-tags-link" href="/BlogEms/categories/#Tensorflow 2">Tensorflow 2</a>
        
      
      </p>
    

    
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h1"><a href="#Intro-to-the-Encoder-Decoder-model-and-the-Attention-mechanism">Intro to the Encoder-Decoder model and the Attention mechanism </a></li>
<li class="toc-entry toc-h1"><a href="#A-neural-machine-translator-from-english-to-spanish-short-sentences-in-tf2">A neural machine translator from english to spanish short sentences in tf2 </a>
<ul>
<li class="toc-entry toc-h3"><a href="#What-is-Neural-Machine-Translation?">What is Neural Machine Translation? </a></li>
<li class="toc-entry toc-h3"><a href="#A-basic-approach-to-the-Encoder-Decoder-model">A basic approach to the Encoder-Decoder model </a></li>
<li class="toc-entry toc-h3"><a href="#Importing-the-libraries-and-initialize-global-variables">Importing the libraries and initialize global variables </a></li>
<li class="toc-entry toc-h2"><a href="#The-dataset-and-text-processing">The dataset and text processing </a>
<ul>
<li class="toc-entry toc-h3"><a href="#Preprocess-the-text-data">Preprocess the text data </a></li>
<li class="toc-entry toc-h3"><a href="#Loading-the-datasets">Loading the datasets </a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#Tokenize-and-process-the-text-data">Tokenize and process the text data </a></li>
<li class="toc-entry toc-h2"><a href="#Create-the-vocabularies">Create the vocabularies </a></li>
<li class="toc-entry toc-h2"><a href="#Padding-the-sentences">Padding the sentences </a>
<ul>
<li class="toc-entry toc-h3"><a href="#Create-the-batch-data-generator">Create the batch data generator </a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#Build-an-Encoder-Decoder-model-with-Recurrent-Neural-Networks">Build an Encoder-Decoder model with Recurrent Neural Networks </a>
<ul>
<li class="toc-entry toc-h3"><a href="#Encoder-class">Encoder class </a></li>
<li class="toc-entry toc-h3"><a href="#Decoder-class">Decoder class </a></li>
<li class="toc-entry toc-h3"><a href="#Create-the-loss-function-and-metrics">Create the loss function and metrics </a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#Train-the-model">Train the model </a>
<ul>
<li class="toc-entry toc-h3"><a href="#Teacher-Forcing">Teacher Forcing </a></li>
<li class="toc-entry toc-h3"><a href="#Evaluate-the-model">Evaluate the model </a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#Make-predictions">Make predictions </a></li>
<li class="toc-entry toc-h2"><a href="#The-Attention-Mechanism">The Attention Mechanism </a>
<ul>
<li class="toc-entry toc-h3"><a href="#Loung-Attention-layer">Loung Attention layer </a></li>
<li class="toc-entry toc-h3"><a href="#Decoder-with-Attention">Decoder with Attention </a></li>
<li class="toc-entry toc-h3"><a href="#Train-step-function">Train step function </a></li>
<li class="toc-entry toc-h3"><a href="#Main-train">Main train </a></li>
<li class="toc-entry toc-h3"><a href="#Evaluate-the-model">Evaluate the model </a></li>
<li class="toc-entry toc-h3"><a href="#Prediction-or-inference">Prediction or inference </a></li>
</ul>
</li>
</ul>
</li>
</ul><!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020-10-07-Intro-seq2seq-Encoder-Decoder-ENG-SPA-translator-tf2.ipynb
-->

<div class="container" id="notebook-container">
        
    
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Intro-to-the-Encoder-Decoder-model-and-the-Attention-mechanism">
<a class="anchor" href="#Intro-to-the-Encoder-Decoder-model-and-the-Attention-mechanism" aria-hidden="true"><span class="octicon octicon-link"></span></a>Intro to the Encoder-Decoder model and the Attention mechanism<a class="anchor-link" href="#Intro-to-the-Encoder-Decoder-model-and-the-Attention-mechanism"> </a>
</h1>
<h1 id="A-neural-machine-translator-from-english-to-spanish-short-sentences-in-tf2">
<a class="anchor" href="#A-neural-machine-translator-from-english-to-spanish-short-sentences-in-tf2" aria-hidden="true"><span class="octicon octicon-link"></span></a>A neural machine translator from english to spanish short sentences in tf2<a class="anchor-link" href="#A-neural-machine-translator-from-english-to-spanish-short-sentences-in-tf2"> </a>
</h1>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We continue our journey through the world of NLP, in this post we are going to describe the <strong>basic architecture of an encoder-decoder model</strong> that we will apply to a neural <em>machine translation problem</em>, translating texts from English to Spanish. Later, we will introduce a technique that has been a great step forward in the treatment of NLP tasks: <strong>the attention mechanism</strong>. We will detail a basic processing of the attention applied to a scenario of a <em>sequence-to-sequence model</em>, "many to many" approach. But for the moment it will be a simple attention model, we will not comment on more complex models that will be discussed in future posts, when we address the subject of <em>Transformers</em>.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="What-is-Neural-Machine-Translation?">
<a class="anchor" href="#What-is-Neural-Machine-Translation?" aria-hidden="true"><span class="octicon octicon-link"></span></a>What is Neural Machine Translation?<a class="anchor-link" href="#What-is-Neural-Machine-Translation?"> </a>
</h3>
<p><em>Machine translation (MT) is the task of automatically converting source text in one language to text in another language. Given a sequence of text in a source language, there is no one single best translation of that text to another language. This is because of the natural ambiguity and flexibility of human language. This makes the challenge of automatic machine translation difficult, perhaps one of the most difficult in artificial intelligence.</em></p>
<p><em>Machine Learning Mastery, Jason Brownlee [1]</em></p>
<p>The initial approach to MT problems was the statistical machine translation based on the use of statistical models, probabilities, given an input sentence. Neural machine translation, or NMT for short, is the use of neural network models to learn a statistical model for machine translation.
The key benefit to the approach is that a single system can be trained directly on source and target text, no longer requiring the pipeline of specialized systems used in statistical machine learning.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="A-basic-approach-to-the-Encoder-Decoder-model">
<a class="anchor" href="#A-basic-approach-to-the-Encoder-Decoder-model" aria-hidden="true"><span class="octicon octicon-link"></span></a>A basic approach to the Encoder-Decoder model<a class="anchor-link" href="#A-basic-approach-to-the-Encoder-Decoder-model"> </a>
</h3>
<p>From the above we can deduce that NMT is a problem where we process an input sequence to produce an output sequence, that is, a sequence-to-sequence (seq2seq) problem. Specifically of the many-to-many type, sequence of several elements both at the input and at the output, and the encoder-decoder architecture for recurrent neural networks is the standard method.</p>
<p><img src="/BlogEms/images/copied_from_nb/images/encoder_decoder_basic.png" alt="Alt" title="title Depiction of Sutskever Encoder-Decoder Model for Text Translation Taken from Sequence to Sequence Learning with Neural Networks, 2014"></p>
<p>The seq2seq model consists of two sub-networks, the encoder and the decoder. The encoder, on the left hand, receives sequences from the source language as inputs and produces as a result a compact representation of the input sequence, trying to summarize or condense all its information. Then that output becomes an input or initial state of the decoder, which can also receive another external input. At each time step, the decoder generates an element of its output sequence based on the input received and its current state, as well as updating its own state for the next time step. </p>
<p>Mention that the input and output sequences are of fixed size but they do not have to match, the length of the input sequence may differ from that of the output sequence.</p>
<p>The critical point of this model is how to get the encoder to provide the most complete and meaningful representation of its input sequence in a single output element to the decoder. Because this vector or state is the only information the decoder will receive from the input to generate the corresponding output. The longer the input, the harder to compress in a single vector.
We will describe in detail the model and build it in a latter section.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Importing-the-libraries-and-initialize-global-variables">
<a class="anchor" href="#Importing-the-libraries-and-initialize-global-variables" aria-hidden="true"><span class="octicon octicon-link"></span></a>Importing the libraries and initialize global variables<a class="anchor-link" href="#Importing-the-libraries-and-initialize-global-variables"> </a>
</h3>
</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">gc</span>
<span class="kn">import</span> <span class="nn">time</span>
<span class="kn">import</span> <span class="nn">re</span>
<span class="kn">import</span> <span class="nn">unicodedata</span>

<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="o">%</span><span class="k">matplotlib</span> inline
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1">#Importing libraries</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras</span> <span class="kn">import</span> <span class="n">backend</span> <span class="k">as</span> <span class="n">K</span>

<span class="kn">from</span> <span class="nn">keras.preprocessing.text</span> <span class="kn">import</span> <span class="n">Tokenizer</span>
<span class="kn">from</span> <span class="nn">keras.preprocessing.sequence</span> <span class="kn">import</span> <span class="n">pad_sequences</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>Using TensorFlow backend.
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We set the variables for data location</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Global parameters</span>
<span class="c1">#root folder</span>
<span class="n">root_folder</span><span class="o">=</span><span class="s1">'.'</span>
<span class="c1">#data_folder='.'</span>
<span class="n">data_folder_name</span><span class="o">=</span><span class="s1">'data'</span>
<span class="n">train_filename</span><span class="o">=</span><span class="s1">'spa.txt'</span>

<span class="c1"># Variable for data directory</span>
<span class="n">DATA_PATH</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">abspath</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">root_folder</span><span class="p">,</span> <span class="n">data_folder_name</span><span class="p">))</span>
<span class="n">train_filenamepath</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">abspath</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">DATA_PATH</span><span class="p">,</span> <span class="n">train_filename</span><span class="p">))</span>

<span class="c1"># Both train and test set are in the root data directory</span>
<span class="n">train_path</span> <span class="o">=</span> <span class="n">DATA_PATH</span>
<span class="n">test_path</span> <span class="o">=</span> <span class="n">DATA_PATH</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The next code cell define the parameters and hyperparameters of our model:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Parameters for our model</span>
<span class="n">INPUT_COLUMN</span> <span class="o">=</span> <span class="s1">'input'</span>
<span class="n">TARGET_COLUMN</span> <span class="o">=</span> <span class="s1">'target'</span>
<span class="n">TARGET_FOR_INPUT</span> <span class="o">=</span> <span class="s1">'target_for_input'</span>
<span class="n">NUM_SAMPLES</span> <span class="o">=</span> <span class="mi">20000</span> <span class="c1">#40000</span>
<span class="n">MAX_VOCAB_SIZE</span> <span class="o">=</span> <span class="mi">20000</span>
<span class="n">EMBEDDING_DIM</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">HIDDEN_DIM</span><span class="o">=</span><span class="mi">1024</span> <span class="c1">#512</span>

<span class="n">BATCH_SIZE</span> <span class="o">=</span> <span class="mi">64</span>  <span class="c1"># Batch size for training.</span>
<span class="n">EPOCHS</span> <span class="o">=</span> <span class="mi">10</span>  <span class="c1"># Number of epochs to train for.</span>

<span class="n">ATTENTION_FUNC</span><span class="o">=</span><span class="s1">'general'</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="The-dataset-and-text-processing">
<a class="anchor" href="#The-dataset-and-text-processing" aria-hidden="true"><span class="octicon octicon-link"></span></a>The dataset and text processing<a class="anchor-link" href="#The-dataset-and-text-processing"> </a>
</h2>
<p>For this exercise we will use pairs of simple sentences, the source in English and target in Spanish, from the Tatoeba project where people contribute adding translations every day. This is the <a href="http://www.manythings.org/anki/">link</a> to some traslations in different languages. There you can download the Spanish - English spa_eng.zip file, it contains 124457 pairs of sentences.</p>
<p>The text sentences are almost clean, they are simple plain text, so we only need to remove accents, lower case the sentences and replace everything with space except (a-z, A-Z, ".", "?", "!", ","). The code to apply this preprocess has been taken from the Tensorflow tutorial for neural machine translation.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Preprocess-the-text-data">
<a class="anchor" href="#Preprocess-the-text-data" aria-hidden="true"><span class="octicon octicon-link"></span></a>Preprocess the text data<a class="anchor-link" href="#Preprocess-the-text-data"> </a>
</h3>
</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Some function to preprocess the text data, taken from the Neural machine translation with attention tutorial</span>
<span class="c1"># in Tensorflow</span>
<span class="k">def</span> <span class="nf">unicode_to_ascii</span><span class="p">(</span><span class="n">s</span><span class="p">):</span>
    <span class="k">return</span> <span class="s1">''</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">c</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">unicodedata</span><span class="o">.</span><span class="n">normalize</span><span class="p">(</span><span class="s1">'NFD'</span><span class="p">,</span> <span class="n">s</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">unicodedata</span><span class="o">.</span><span class="n">category</span><span class="p">(</span><span class="n">c</span><span class="p">)</span> <span class="o">!=</span> <span class="s1">'Mn'</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">preprocess_sentence</span><span class="p">(</span><span class="n">w</span><span class="p">):</span>
    <span class="sd">''' Preprocess the input text w applying lowercase, removing accents, </span>
<span class="sd">    creating a space between a word and the punctuation following it and </span>
<span class="sd">    replacing everything with space except (a-z, A-Z, ".", "?", "!", ",")</span>
<span class="sd">    Input:</span>
<span class="sd">        - w: a string, input text</span>
<span class="sd">    Output:</span>
<span class="sd">        - a string, the cleaned text</span>
<span class="sd">    '''</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">unicode_to_ascii</span><span class="p">(</span><span class="n">w</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span><span class="o">.</span><span class="n">strip</span><span class="p">())</span>

    <span class="c1"># creating a space between a word and the punctuation following it</span>
    <span class="c1"># eg: "he is a boy." =&gt; "he is a boy ."</span>
    <span class="c1"># Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s2">"([?.!,¿])"</span><span class="p">,</span> <span class="sa">r</span><span class="s2">" \1 "</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s1">'[" "]+'</span><span class="p">,</span> <span class="s2">" "</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span>

    <span class="c1"># replacing everything with space except (a-z, A-Z, ".", "?", "!", ",")</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s2">"[^a-zA-Z?.!,¿]+"</span><span class="p">,</span> <span class="s2">" "</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span>

    <span class="n">w</span> <span class="o">=</span> <span class="n">w</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span>

    <span class="c1"># adding a start and an end token to the sentence</span>
    <span class="c1"># so that the model know when to start and stop predicting.</span>
    <span class="c1">#w = '&lt;start&gt; ' + w + ' &lt;end&gt;'</span>
    
    <span class="k">return</span> <span class="n">w</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Loading-the-datasets">
<a class="anchor" href="#Loading-the-datasets" aria-hidden="true"><span class="octicon octicon-link"></span></a>Loading the datasets<a class="anchor-link" href="#Loading-the-datasets"> </a>
</h3>
<p>Load the dataset into a pandas dataframe and apply the preprocess function to the input and target columns.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Load the dataset: sentence in english, sentence in spanish </span>
<span class="n">df</span><span class="o">=</span><span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">train_filenamepath</span><span class="p">,</span> <span class="n">sep</span><span class="o">=</span><span class="s2">"</span><span class="se">\t</span><span class="s2">"</span><span class="p">,</span> <span class="n">header</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">names</span><span class="o">=</span><span class="p">[</span><span class="n">INPUT_COLUMN</span><span class="p">,</span><span class="n">TARGET_COLUMN</span><span class="p">],</span> <span class="n">usecols</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> 
               <span class="n">nrows</span><span class="o">=</span><span class="n">NUM_SAMPLES</span><span class="p">)</span>
<span class="c1"># Preprocess the input data</span>
<span class="n">input_data</span><span class="o">=</span><span class="n">df</span><span class="p">[</span><span class="n">INPUT_COLUMN</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span> <span class="p">:</span> <span class="n">preprocess_sentence</span><span class="p">(</span><span class="n">x</span><span class="p">))</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
<span class="c1"># Preprocess and include the end of sentence token to the target text</span>
<span class="n">target_data</span><span class="o">=</span><span class="n">df</span><span class="p">[</span><span class="n">TARGET_COLUMN</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span> <span class="p">:</span> <span class="n">preprocess_sentence</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">+</span> <span class="s1">' &lt;eos&gt;'</span><span class="p">)</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
<span class="c1"># Preprocess and include a start of setence token to the input text to the decoder, it is rigth shifted</span>
<span class="n">target_input_data</span><span class="o">=</span><span class="n">df</span><span class="p">[</span><span class="n">TARGET_COLUMN</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span> <span class="p">:</span> <span class="s1">'&lt;sos&gt; '</span><span class="o">+</span> <span class="n">preprocess_sentence</span><span class="p">(</span><span class="n">x</span><span class="p">))</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="n">input_data</span><span class="p">[:</span><span class="mi">5</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">target_data</span><span class="p">[:</span><span class="mi">5</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">target_input_data</span><span class="p">[:</span><span class="mi">5</span><span class="p">])</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>['go .', 'go .', 'go .', 'go .', 'hi .']
['ve . &lt;eos&gt;', 'vete . &lt;eos&gt;', 'vaya . &lt;eos&gt;', 'vayase . &lt;eos&gt;', 'hola . &lt;eos&gt;']
['&lt;sos&gt; ve .', '&lt;sos&gt; vete .', '&lt;sos&gt; vaya .', '&lt;sos&gt; vayase .', '&lt;sos&gt; hola .']
</pre>
</div>
</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1">#Delete the dataframe and release the memory (if it is possible)</span>
<span class="k">del</span> <span class="n">df</span>
<span class="n">gc</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>22</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Tokenize-and-process-the-text-data">
<a class="anchor" href="#Tokenize-and-process-the-text-data" aria-hidden="true"><span class="octicon octicon-link"></span></a>Tokenize and process the text data<a class="anchor-link" href="#Tokenize-and-process-the-text-data"> </a>
</h2>
<p>Next, let's see how to prepare the data for our model. It is very simple and the steps are the following:</p>
<ul>
<li>Tokenize the data, to convert the raw text into a sequence of integers. First, we create a Tokenizer object from the keras library and fit it to our text (one tokenizer for the input and another one for the output).</li>
<li>Extract sequence of integers from the text: we call the text_to_sequence method of the tokenizer for every input and output text.</li>
<li>Calculate the maximum length of the input and output sequences.</li>
</ul>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Create a tokenizer for the input texts and fit it to them </span>
<span class="n">tokenizer_inputs</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">(</span><span class="n">num_words</span><span class="o">=</span><span class="n">MAX_VOCAB_SIZE</span><span class="p">,</span> <span class="n">filters</span><span class="o">=</span><span class="s1">''</span><span class="p">)</span>
<span class="n">tokenizer_inputs</span><span class="o">.</span><span class="n">fit_on_texts</span><span class="p">(</span><span class="n">input_data</span><span class="p">)</span>
<span class="c1"># Tokenize and transform input texts to sequence of integers</span>
<span class="n">input_sequences</span> <span class="o">=</span> <span class="n">tokenizer_inputs</span><span class="o">.</span><span class="n">texts_to_sequences</span><span class="p">(</span><span class="n">input_data</span><span class="p">)</span>
<span class="c1"># Claculate the max length</span>
<span class="n">input_max_len</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">s</span><span class="p">)</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">input_sequences</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'Max Input Length: '</span><span class="p">,</span> <span class="n">input_max_len</span><span class="p">)</span>
<span class="c1"># Show some example of tokenize sentences, useful to check the tokenization</span>
<span class="nb">print</span><span class="p">(</span><span class="n">input_data</span><span class="p">[</span><span class="mi">1000</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">input_sequences</span><span class="p">[</span><span class="mi">1000</span><span class="p">])</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Max Input Length:  8
are you in ?
[21, 4, 36, 5]
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now we repeat the steps for the output texts but now we do not want to filter special characters otherwise eos and sos token will be removed</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># tokenize the outputs</span>
<span class="c1"># don't filter out special characters (filters = '')</span>
<span class="c1"># otherwise &lt;sos&gt; and &lt;eos&gt; won't appear</span>
<span class="c1"># By default, Keras’ Tokenizer will trim out all the punctuations, which is not what we want. </span>
<span class="c1"># we can just set filters as blank here.</span>

<span class="c1"># Create a tokenizer for the output texts and fit it to them </span>
<span class="n">tokenizer_outputs</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">(</span><span class="n">num_words</span><span class="o">=</span><span class="n">MAX_VOCAB_SIZE</span><span class="p">,</span> <span class="n">filters</span><span class="o">=</span><span class="s1">''</span><span class="p">)</span>
<span class="n">tokenizer_outputs</span><span class="o">.</span><span class="n">fit_on_texts</span><span class="p">(</span><span class="n">target_data</span><span class="p">)</span>
<span class="n">tokenizer_outputs</span><span class="o">.</span><span class="n">fit_on_texts</span><span class="p">(</span><span class="n">target_input_data</span><span class="p">)</span>
<span class="c1"># Tokenize and transform output texts to sequence of integers</span>
<span class="n">target_sequences</span> <span class="o">=</span> <span class="n">tokenizer_outputs</span><span class="o">.</span><span class="n">texts_to_sequences</span><span class="p">(</span><span class="n">target_data</span><span class="p">)</span>
<span class="n">target_sequences_inputs</span> <span class="o">=</span> <span class="n">tokenizer_outputs</span><span class="o">.</span><span class="n">texts_to_sequences</span><span class="p">(</span><span class="n">target_input_data</span><span class="p">)</span>

<span class="c1"># determine maximum length output sequence</span>
<span class="n">target_max_len</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">s</span><span class="p">)</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">target_sequences</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'Max Target Length: '</span><span class="p">,</span> <span class="n">target_max_len</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">target_data</span><span class="p">[</span><span class="mi">1000</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">target_sequences</span><span class="p">[</span><span class="mi">1000</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">target_input_data</span><span class="p">[</span><span class="mi">1000</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">target_sequences_inputs</span><span class="p">[</span><span class="mi">1000</span><span class="p">])</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Max Target Length:  15
¿ estas dentro ? &lt;eos&gt;
[5, 45, 401, 4, 2]
&lt;sos&gt; ¿ estas dentro ?
[3, 5, 45, 401, 4]
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Create-the-vocabularies">
<a class="anchor" href="#Create-the-vocabularies" aria-hidden="true"><span class="octicon octicon-link"></span></a>Create the vocabularies<a class="anchor-link" href="#Create-the-vocabularies"> </a>
</h2>
<p>Using the tokenizer we have created previously we can retrieve the vocabularies, one to match word to integer (word2idx) and a second one to match the integer to the corresponding word (idx2word).</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># get the word to index mapping for input language</span>
<span class="n">word2idx_inputs</span> <span class="o">=</span> <span class="n">tokenizer_inputs</span><span class="o">.</span><span class="n">word_index</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'Found </span><span class="si">%s</span><span class="s1"> unique input tokens.'</span> <span class="o">%</span> <span class="nb">len</span><span class="p">(</span><span class="n">word2idx_inputs</span><span class="p">))</span>

<span class="c1"># get the word to index mapping for output language</span>
<span class="n">word2idx_outputs</span> <span class="o">=</span> <span class="n">tokenizer_outputs</span><span class="o">.</span><span class="n">word_index</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'Found </span><span class="si">%s</span><span class="s1"> unique output tokens.'</span> <span class="o">%</span> <span class="nb">len</span><span class="p">(</span><span class="n">word2idx_outputs</span><span class="p">))</span>

<span class="c1"># store number of output and input words for later</span>
<span class="c1"># remember to add 1 since indexing starts at 1</span>
<span class="n">num_words_output</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">word2idx_outputs</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>
<span class="n">num_words_inputs</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">word2idx_inputs</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>

<span class="c1"># map indexes back into real words</span>
<span class="c1"># so we can view the results</span>
<span class="n">idx2word_inputs</span> <span class="o">=</span> <span class="p">{</span><span class="n">v</span><span class="p">:</span><span class="n">k</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">word2idx_inputs</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
<span class="n">idx2word_outputs</span> <span class="o">=</span> <span class="p">{</span><span class="n">v</span><span class="p">:</span><span class="n">k</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">word2idx_outputs</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Found 3664 unique input tokens.
Found 7168 unique output tokens.
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Padding-the-sentences">
<a class="anchor" href="#Padding-the-sentences" aria-hidden="true"><span class="octicon octicon-link"></span></a>Padding the sentences<a class="anchor-link" href="#Padding-the-sentences"> </a>
</h2>
<ul>
<li>Padding the sentences: we need to pad zeros at the end of the sequences so that all sequences have the same length. Otherwise, we won't be able train the model on batches</li>
</ul>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># pad the input sequences</span>
<span class="n">encoder_inputs</span> <span class="o">=</span> <span class="n">pad_sequences</span><span class="p">(</span><span class="n">input_sequences</span><span class="p">,</span> <span class="n">maxlen</span><span class="o">=</span><span class="n">input_max_len</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s1">'post'</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"encoder_inputs.shape:"</span><span class="p">,</span> <span class="n">encoder_inputs</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"encoder_inputs[0]:"</span><span class="p">,</span> <span class="n">encoder_inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="c1"># pad the decoder input sequences</span>
<span class="n">decoder_inputs</span> <span class="o">=</span> <span class="n">pad_sequences</span><span class="p">(</span><span class="n">target_sequences_inputs</span><span class="p">,</span> <span class="n">maxlen</span><span class="o">=</span><span class="n">target_max_len</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s1">'post'</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"decoder_inputs[0]:"</span><span class="p">,</span> <span class="n">decoder_inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"decoder_inputs.shape:"</span><span class="p">,</span> <span class="n">decoder_inputs</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="c1"># pad the target output sequences</span>
<span class="n">decoder_targets</span> <span class="o">=</span> <span class="n">pad_sequences</span><span class="p">(</span><span class="n">target_sequences</span><span class="p">,</span> <span class="n">maxlen</span><span class="o">=</span><span class="n">target_max_len</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s1">'post'</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>encoder_inputs.shape: (20000, 8)
encoder_inputs[0]: [31  1  0  0  0  0  0  0]
decoder_inputs[0]: [ 3 99  1  0  0  0  0  0  0  0  0  0  0  0  0]
decoder_inputs.shape: (20000, 15)
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Create-the-batch-data-generator">
<a class="anchor" href="#Create-the-batch-data-generator" aria-hidden="true"><span class="octicon octicon-link"></span></a>Create the batch data generator<a class="anchor-link" href="#Create-the-batch-data-generator"> </a>
</h3>
<ul>
<li>Create a batch data generator: we want to train the model on batches, group of sentences, so we need to create a Dataset using the tf.data library and the function batch_on_slices on the input and output sequences.</li>
</ul>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Define a dataset </span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Dataset</span><span class="o">.</span><span class="n">from_tensor_slices</span><span class="p">(</span>
    <span class="p">(</span><span class="n">encoder_inputs</span><span class="p">,</span> <span class="n">decoder_inputs</span><span class="p">,</span> <span class="n">decoder_targets</span><span class="p">))</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">input_data</span><span class="p">))</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span>
    <span class="n">BATCH_SIZE</span><span class="p">,</span> <span class="n">drop_remainder</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Build-an-Encoder-Decoder-model-with-Recurrent-Neural-Networks">
<a class="anchor" href="#Build-an-Encoder-Decoder-model-with-Recurrent-Neural-Networks" aria-hidden="true"><span class="octicon octicon-link"></span></a>Build an Encoder-Decoder model with Recurrent Neural Networks<a class="anchor-link" href="#Build-an-Encoder-Decoder-model-with-Recurrent-Neural-Networks"> </a>
</h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>For a better understanding, we can divide the model in three basic components:</p>
<p><img src="/BlogEms/images/copied_from_nb/images/encoder_decoder_RNN.jpeg" alt="Alt" title="From Understanding Encoder-Decoder Sequence to Sequence Model by Simeon Kostadinov [3]"></p>
<ul>
<li>The <strong>encoder</strong>: Layers of recurrent units where in each time step, receive a an input token, collects relevant information and produce a hidden state. Depends on the type of RNN, in our example a LSTM, the unit "mixes" the current hidden state and the input and return an output, discarded, and a new hidden state. You can read my post … for more information.</li>
</ul>
<ul>
<li>The <strong>encoder vector</strong>: it is the last hidden state of the encoder and it tries to contain as much of the useful input information as possible to help the decoder get the best results. It is only information from the input that the decoder will get.</li>
</ul>
<ul>
<li>The <strong>decoder</strong>: Layers of recurrent units, i.e. LSTMs, where each unit produces an output at a time step t. The hidden state of the first unit is the encoder vector and the rest of units accept the hidden state from the previous unit. The output is calculated using a softmax function to obtain a  probability for every token in the output vocabulary.</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Encoder-class">
<a class="anchor" href="#Encoder-class" aria-hidden="true"><span class="octicon octicon-link"></span></a>Encoder class<a class="anchor-link" href="#Encoder-class"> </a>
</h3>
</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">Encoder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Model</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Encoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_dim</span> <span class="o">=</span> <span class="n">hidden_dim</span>
        <span class="c1"># Define the embedding layer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">)</span>
        <span class="c1"># Define the RNN layer, LSTM</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lstm</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">LSTM</span><span class="p">(</span>
            <span class="n">hidden_dim</span><span class="p">,</span> <span class="n">return_sequences</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">return_state</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_sequence</span><span class="p">,</span> <span class="n">states</span><span class="p">):</span>
        <span class="c1"># Embed the input</span>
        <span class="n">embed</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span><span class="p">(</span><span class="n">input_sequence</span><span class="p">)</span>
        <span class="c1"># Call the LSTM unit</span>
        <span class="n">output</span><span class="p">,</span> <span class="n">state_h</span><span class="p">,</span> <span class="n">state_c</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lstm</span><span class="p">(</span><span class="n">embed</span><span class="p">,</span> <span class="n">initial_state</span><span class="o">=</span><span class="n">states</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="n">state_h</span><span class="p">,</span> <span class="n">state_c</span>

    <span class="k">def</span> <span class="nf">init_states</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
        <span class="c1"># Return a all 0s initial states</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">]),</span>
                <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">]))</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Decoder-class">
<a class="anchor" href="#Decoder-class" aria-hidden="true"><span class="octicon octicon-link"></span></a>Decoder class<a class="anchor-link" href="#Decoder-class"> </a>
</h3>
</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">Decoder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Model</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Decoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_dim</span> <span class="o">=</span> <span class="n">hidden_dim</span>
        <span class="c1"># Define the embedding layer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">)</span>
        <span class="c1"># Define the RNN layer, LSTM</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lstm</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">LSTM</span><span class="p">(</span>
            <span class="n">hidden_dim</span><span class="p">,</span> <span class="n">return_sequences</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">return_state</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dense</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_sequence</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="c1"># Embed the input</span>
        <span class="n">embed</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span><span class="p">(</span><span class="n">input_sequence</span><span class="p">)</span>
        <span class="c1"># Call the LSTM unit</span>
        <span class="n">lstm_out</span><span class="p">,</span> <span class="n">state_h</span><span class="p">,</span> <span class="n">state_c</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lstm</span><span class="p">(</span><span class="n">embed</span><span class="p">,</span> <span class="n">state</span><span class="p">)</span>
        <span class="c1"># Dense layer to predict output token</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">lstm_out</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">logits</span><span class="p">,</span> <span class="n">state_h</span><span class="p">,</span> <span class="n">state_c</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Once our encoder and decoder are defined we can init them and set the initial hidden state. We have included a simple test, calling the encoder and decoder to check they works fine.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1">#Set the length of the input and output vocabulary</span>
<span class="n">num_words_inputs</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">word2idx_inputs</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>
<span class="n">num_words_output</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">word2idx_outputs</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>
<span class="c1">#Create the encoder</span>
<span class="n">encoder</span> <span class="o">=</span> <span class="n">Encoder</span><span class="p">(</span><span class="n">num_words_inputs</span><span class="p">,</span> <span class="n">EMBEDDING_DIM</span><span class="p">,</span> <span class="n">HIDDEN_DIM</span><span class="p">)</span>
<span class="c1"># Get the initial states</span>
<span class="n">initial_state</span> <span class="o">=</span> <span class="n">encoder</span><span class="o">.</span><span class="n">init_states</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="c1"># Call the encoder for testing</span>
<span class="n">test_encoder_output</span> <span class="o">=</span> <span class="n">encoder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span>
    <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">23</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]]),</span> <span class="n">initial_state</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">test_encoder_output</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="c1"># Create the decoder</span>
<span class="n">decoder</span> <span class="o">=</span> <span class="n">Decoder</span><span class="p">(</span><span class="n">num_words_output</span><span class="p">,</span> <span class="n">EMBEDDING_DIM</span><span class="p">,</span> <span class="n">HIDDEN_DIM</span><span class="p">)</span>
<span class="c1"># Get the initial states</span>
<span class="n">de_initial_state</span> <span class="o">=</span> <span class="n">test_encoder_output</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>
<span class="c1"># Call the decoder for testing</span>
<span class="n">test_decoder_output</span> <span class="o">=</span> <span class="n">decoder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span>
    <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]]),</span> <span class="n">de_initial_state</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">test_decoder_output</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>(1, 6, 512)
(1, 8, 11239)
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Create-the-loss-function-and-metrics">
<a class="anchor" href="#Create-the-loss-function-and-metrics" aria-hidden="true"><span class="octicon octicon-link"></span></a>Create the loss function and metrics<a class="anchor-link" href="#Create-the-loss-function-and-metrics"> </a>
</h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now we need to define a custom loss function to avoid taking into account the 0 values, padding values, when calculating the loss. And also we have to define a custom accuracy function.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">loss_func</span><span class="p">(</span><span class="n">targets</span><span class="p">,</span> <span class="n">logits</span><span class="p">):</span>
    <span class="n">crossentropy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">SparseCategoricalCrossentropy</span><span class="p">(</span>
        <span class="n">from_logits</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="c1"># Mask padding values, they do not have to compute for loss</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">logical_not</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">targets</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
    <span class="c1"># Calculate the loss value</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">crossentropy</span><span class="p">(</span><span class="n">targets</span><span class="p">,</span> <span class="n">logits</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="n">mask</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">loss</span>

<span class="k">def</span> <span class="nf">accuracy_fn</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">):</span>
    <span class="c1"># y_pred shape is batch_size, seq length, vocab size</span>
    <span class="c1"># y_true shape is batch_size, seq length</span>
    <span class="n">pred_values</span> <span class="o">=</span> <span class="n">K</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">K</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s1">'int32'</span><span class="p">)</span>
    <span class="n">correct</span> <span class="o">=</span> <span class="n">K</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">K</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">pred_values</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s1">'float32'</span><span class="p">)</span>

    <span class="c1"># 0 is padding, don't include those</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="n">K</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">K</span><span class="o">.</span><span class="n">greater</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s1">'float32'</span><span class="p">)</span>
    <span class="n">n_correct</span> <span class="o">=</span> <span class="n">K</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">mask</span> <span class="o">*</span> <span class="n">correct</span><span class="p">)</span>
    <span class="n">n_total</span> <span class="o">=</span> <span class="n">K</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">mask</span><span class="p">)</span>
  
    <span class="k">return</span> <span class="n">n_correct</span> <span class="o">/</span> <span class="n">n_total</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Train-the-model">
<a class="anchor" href="#Train-the-model" aria-hidden="true"><span class="octicon octicon-link"></span></a>Train the model<a class="anchor-link" href="#Train-the-model"> </a>
</h2>
<p>As we mentioned before, we are interested in training the network in batches, therefore, we create a function that carries out the training of a batch of the data:</p>
<ul>
<li>Call the encoder for the batch input sequence, the output is the encoded vector.</li>
<li>Set the decoder initial states to the encoded vector</li>
<li>Call the decoder, taking the right shifted target sequence as input. The output are the logits (the softmax function is applied in the loss function)</li>
<li>Calculate the loss and accuracy of the batch data</li>
<li>Update the learnable parameters of the encoder and the decoder</li>
<li>update the optimizer</li>
</ul>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Use the @tf.function decorator to take advance of static graph computation</span>
<span class="nd">@tf</span><span class="o">.</span><span class="n">function</span>
<span class="k">def</span> <span class="nf">train_step</span><span class="p">(</span><span class="n">input_seq</span><span class="p">,</span> <span class="n">target_seq_in</span><span class="p">,</span> <span class="n">target_seq_out</span><span class="p">,</span> <span class="n">en_initial_states</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">):</span>
    <span class="sd">''' A training step, train a batch of the data and return the loss value reached</span>
<span class="sd">        Input:</span>
<span class="sd">        - input_seq: array of integers, shape [batch_size, max_seq_len, embedding dim].</span>
<span class="sd">            the input sequence</span>
<span class="sd">        - target_seq_out: array of integers, shape [batch_size, max_seq_len, embedding dim].</span>
<span class="sd">            the target seq, our target sequence</span>
<span class="sd">        - target_seq_in: array of integers, shape [batch_size, max_seq_len, embedding dim].</span>
<span class="sd">            the input sequence to the decoder, we use Teacher Forcing</span>
<span class="sd">        - en_initial_states: tuple of arrays of shape [batch_size, hidden_dim].</span>
<span class="sd">            the initial state of the encoder</span>
<span class="sd">        - optimizer: a tf.keras.optimizers.</span>
<span class="sd">        Output:</span>
<span class="sd">        - loss: loss value</span>
<span class="sd">        </span>
<span class="sd">    '''</span>
    <span class="c1"># Network’s computations need to be put under tf.GradientTape() to keep track of gradients</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">tape</span><span class="p">:</span>
        <span class="c1"># Get the encoder outputs</span>
        <span class="n">en_outputs</span> <span class="o">=</span> <span class="n">encoder</span><span class="p">(</span><span class="n">input_seq</span><span class="p">,</span> <span class="n">en_initial_states</span><span class="p">)</span>
        <span class="c1"># Set the encoder and decoder states</span>
        <span class="n">en_states</span> <span class="o">=</span> <span class="n">en_outputs</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>
        <span class="n">de_states</span> <span class="o">=</span> <span class="n">en_states</span>
        <span class="c1"># Get the encoder outputs</span>
        <span class="n">de_outputs</span> <span class="o">=</span> <span class="n">decoder</span><span class="p">(</span><span class="n">target_seq_in</span><span class="p">,</span> <span class="n">de_states</span><span class="p">)</span>
        <span class="c1"># Take the actual output</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="n">de_outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="c1"># Calculate the loss function</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_func</span><span class="p">(</span><span class="n">target_seq_out</span><span class="p">,</span> <span class="n">logits</span><span class="p">)</span>
        <span class="n">acc</span> <span class="o">=</span> <span class="n">accuracy_fn</span><span class="p">(</span><span class="n">target_seq_out</span><span class="p">,</span> <span class="n">logits</span><span class="p">)</span>

    <span class="n">variables</span> <span class="o">=</span> <span class="n">encoder</span><span class="o">.</span><span class="n">trainable_variables</span> <span class="o">+</span> <span class="n">decoder</span><span class="o">.</span><span class="n">trainable_variables</span>
    <span class="c1"># Calculate the gradients for the variables</span>
    <span class="n">gradients</span> <span class="o">=</span> <span class="n">tape</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">variables</span><span class="p">)</span>
    <span class="c1"># Apply the gradients and update the optimizer</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">apply_gradients</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">gradients</span><span class="p">,</span> <span class="n">variables</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">loss</span><span class="p">,</span> <span class="n">acc</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>As you can observe, our train function receives three sequences:</p>
<ul>
<li>
<p><strong>Input</strong> sequence: array of integers of shape [batch_size, max_seq_len, embedding dim]. It is the input sequence to the encoder.</p>
</li>
<li>
<p><strong>target</strong> sequence: array of integers of shape [batch_size, max_seq_len, embedding dim]. It is the target of our model, the output that we want for our model.</p>
</li>
<li>
<p><strong>Target input</strong> sequence: array of integers of shape [batch_size, max_seq_len, embedding dim]. It is the input sequence to the decoder because we use <em>Teacher Forcing</em>.</p>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Teacher-Forcing">
<a class="anchor" href="#Teacher-Forcing" aria-hidden="true"><span class="octicon octicon-link"></span></a>Teacher Forcing<a class="anchor-link" href="#Teacher-Forcing"> </a>
</h3>
<p>Teacher forcing is a training method critical to the development of deep learning models in NLP. It is a way for quickly and efficiently training recurrent neural network models that use the ground truth from a prior time step as input.</p>
<p>In a recurrent network usually the input to a RNN at the time step t is the output of the RNN in the previous time step, t-1. But with teacher forcing we can use the actual output to improve the learning capabilities of the model.</p>
<p><em>"Teacher forcing works by using the actual or expected output from the training dataset at the current time step y(t) as input in the next time step X(t+1), rather than the output generated by the network.
So, in our example, the input to the decoder is the target sequence right-shifted, the target output at time step t is the decoder input at time step t+1."</em></p>
<p>When our model output do not vary from what was seen by the model during training, teacher forcing is very effective. But if we need a more "creative" model, where given an input sequence there can be several possible outputs, we should avoid this technique or apply it randomly (only in some random time steps).</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now, we can code the whole training process:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Create the main train function</span>
<span class="k">def</span> <span class="nf">main_train</span><span class="p">(</span><span class="n">encoder</span><span class="p">,</span> <span class="n">decoder</span><span class="p">,</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">n_epochs</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">checkpoint</span><span class="p">,</span> <span class="n">checkpoint_prefix</span><span class="p">):</span>
    
    <span class="n">losses</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">accuracies</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_epochs</span><span class="p">):</span>
        <span class="c1"># Get the initial time</span>
        <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
        <span class="c1"># Get the initial state for the encoder</span>
        <span class="n">en_initial_states</span> <span class="o">=</span> <span class="n">encoder</span><span class="o">.</span><span class="n">init_states</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span>
        <span class="c1"># For every batch data</span>
        <span class="k">for</span> <span class="n">batch</span><span class="p">,</span> <span class="p">(</span><span class="n">input_seq</span><span class="p">,</span> <span class="n">target_seq_in</span><span class="p">,</span> <span class="n">target_seq_out</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">dataset</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)):</span>
            <span class="c1"># Train and get the loss value </span>
            <span class="n">loss</span><span class="p">,</span> <span class="n">accuracy</span> <span class="o">=</span> <span class="n">train_step</span><span class="p">(</span><span class="n">input_seq</span><span class="p">,</span> <span class="n">target_seq_in</span><span class="p">,</span> <span class="n">target_seq_out</span><span class="p">,</span> <span class="n">en_initial_states</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">)</span>
        
            <span class="k">if</span> <span class="n">batch</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="c1"># Store the loss and accuracy values</span>
                <span class="n">losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
                <span class="n">accuracies</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">accuracy</span><span class="p">)</span>
                <span class="nb">print</span><span class="p">(</span><span class="s1">'Epoch </span><span class="si">{}</span><span class="s1"> Batch </span><span class="si">{}</span><span class="s1"> Loss </span><span class="si">{:.4f}</span><span class="s1"> Acc:</span><span class="si">{:.4f}</span><span class="s1">'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">e</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">loss</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">accuracy</span><span class="o">.</span><span class="n">numpy</span><span class="p">()))</span>
                
        <span class="c1"># saving (checkpoint) the model every 2 epochs</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">e</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">checkpoint</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">file_prefix</span> <span class="o">=</span> <span class="n">checkpoint_prefix</span><span class="p">)</span>
    
        <span class="nb">print</span><span class="p">(</span><span class="s1">'Time taken for 1 epoch </span><span class="si">{:.4f}</span><span class="s1"> sec</span><span class="se">\n</span><span class="s1">'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span><span class="p">))</span>
        
    <span class="k">return</span> <span class="n">losses</span><span class="p">,</span> <span class="n">accuracies</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We are almost ready, our last step include a call to the main train function and we create a checkpoint object to save our model. Because the training process require a long time to run, every two epochs we save it. Later we can restore it and use it to make predictions.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Create an Adam optimizer and clips gradients by norm</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">clipnorm</span><span class="o">=</span><span class="mf">5.0</span><span class="p">)</span>
<span class="c1"># Create a checkpoint object to save the model</span>
<span class="n">checkpoint_dir</span> <span class="o">=</span> <span class="s1">'./training_ckpt_seq2seq'</span>
<span class="n">checkpoint_prefix</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">checkpoint_dir</span><span class="p">,</span> <span class="s2">"ckpt"</span><span class="p">)</span>
<span class="n">checkpoint</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">Checkpoint</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">,</span>
                                 <span class="n">encoder</span><span class="o">=</span><span class="n">encoder</span><span class="p">,</span>
                                 <span class="n">decoder</span><span class="o">=</span><span class="n">decoder</span><span class="p">)</span>

<span class="n">losses</span><span class="p">,</span> <span class="n">accuracies</span> <span class="o">=</span> <span class="n">main_train</span><span class="p">(</span><span class="n">encoder</span><span class="p">,</span> <span class="n">decoder</span><span class="p">,</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">EPOCHS</span><span class="p">,</span> <span class="n">BATCH_SIZE</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">checkpoint</span><span class="p">,</span> <span class="n">checkpoint_prefix</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Epoch 1 Batch 0 Loss 3.6920 Acc:0.0000
Epoch 1 Batch 100 Loss 1.8457 Acc:0.3483
Epoch 1 Batch 200 Loss 1.8571 Acc:0.3507
Epoch 1 Batch 300 Loss 1.7594 Acc:0.3544
Epoch 1 Batch 400 Loss 1.7331 Acc:0.3711
Epoch 1 Batch 500 Loss 1.6858 Acc:0.4112
Epoch 1 Batch 600 Loss 1.5133 Acc:0.4176
Time taken for 1 epoch 412.1442 sec

Epoch 2 Batch 0 Loss 1.4640 Acc:0.4237
Epoch 2 Batch 100 Loss 1.3850 Acc:0.4479
Epoch 2 Batch 200 Loss 1.3766 Acc:0.4496
Epoch 2 Batch 300 Loss 1.3028 Acc:0.4920
Epoch 2 Batch 400 Loss 1.2801 Acc:0.4804
Epoch 2 Batch 500 Loss 1.2633 Acc:0.4809
Epoch 2 Batch 600 Loss 1.1715 Acc:0.5307
Time taken for 1 epoch 407.9001 sec

Epoch 3 Batch 0 Loss 1.0933 Acc:0.5026
Epoch 3 Batch 100 Loss 0.9902 Acc:0.5368
Epoch 3 Batch 200 Loss 1.1020 Acc:0.5166
Epoch 3 Batch 300 Loss 0.9828 Acc:0.5831
Epoch 3 Batch 400 Loss 1.0007 Acc:0.5851
Epoch 3 Batch 500 Loss 1.0448 Acc:0.5623
Epoch 3 Batch 600 Loss 1.0433 Acc:0.5434
Time taken for 1 epoch 410.5108 sec

Epoch 4 Batch 0 Loss 0.8128 Acc:0.5749
Epoch 4 Batch 100 Loss 0.8484 Acc:0.5805
Epoch 4 Batch 200 Loss 0.8766 Acc:0.6042
Epoch 4 Batch 300 Loss 0.8591 Acc:0.5722
Epoch 4 Batch 400 Loss 0.9052 Acc:0.5625
Epoch 4 Batch 500 Loss 0.8957 Acc:0.5607
Epoch 4 Batch 600 Loss 0.7913 Acc:0.6230
Time taken for 1 epoch 408.0038 sec

Epoch 5 Batch 0 Loss 0.7634 Acc:0.6171
Epoch 5 Batch 100 Loss 0.7738 Acc:0.6005
Epoch 5 Batch 200 Loss 0.7116 Acc:0.6011
Epoch 5 Batch 300 Loss 0.7309 Acc:0.6133
Epoch 5 Batch 400 Loss 0.7092 Acc:0.6168
Epoch 5 Batch 500 Loss 0.6972 Acc:0.6475
Epoch 5 Batch 600 Loss 0.7019 Acc:0.6544
Time taken for 1 epoch 407.3742 sec

Epoch 6 Batch 0 Loss 0.5252 Acc:0.6979
Epoch 6 Batch 100 Loss 0.5881 Acc:0.6987
Epoch 6 Batch 200 Loss 0.5753 Acc:0.6832
Epoch 6 Batch 300 Loss 0.5651 Acc:0.7018
Epoch 6 Batch 400 Loss 0.6628 Acc:0.6524
Epoch 6 Batch 500 Loss 0.6011 Acc:0.6794
Epoch 6 Batch 600 Loss 0.5782 Acc:0.6613
Time taken for 1 epoch 407.0541 sec

Epoch 7 Batch 0 Loss 0.4506 Acc:0.7228
Epoch 7 Batch 100 Loss 0.4577 Acc:0.7179
Epoch 7 Batch 200 Loss 0.4120 Acc:0.7560
Epoch 7 Batch 300 Loss 0.4673 Acc:0.7092
Epoch 7 Batch 400 Loss 0.4652 Acc:0.7084
Epoch 7 Batch 500 Loss 0.5468 Acc:0.7000
Epoch 7 Batch 600 Loss 0.5556 Acc:0.6779
Time taken for 1 epoch 405.3848 sec

Epoch 8 Batch 0 Loss 0.4024 Acc:0.7581
Epoch 8 Batch 100 Loss 0.3794 Acc:0.7715
Epoch 8 Batch 200 Loss 0.3802 Acc:0.7486
Epoch 8 Batch 300 Loss 0.3644 Acc:0.7425
Epoch 8 Batch 400 Loss 0.4295 Acc:0.7378
Epoch 8 Batch 500 Loss 0.3921 Acc:0.7418
Epoch 8 Batch 600 Loss 0.4412 Acc:0.7280
Time taken for 1 epoch 406.3167 sec

Epoch 9 Batch 0 Loss 0.3029 Acc:0.8434
Epoch 9 Batch 100 Loss 0.3544 Acc:0.7959
Epoch 9 Batch 200 Loss 0.3183 Acc:0.7900
Epoch 9 Batch 300 Loss 0.3036 Acc:0.8057
Epoch 9 Batch 400 Loss 0.2649 Acc:0.7955
Epoch 9 Batch 500 Loss 0.3325 Acc:0.8065
Epoch 9 Batch 600 Loss 0.3186 Acc:0.7851
Time taken for 1 epoch 406.4001 sec

Epoch 10 Batch 0 Loss 0.2331 Acc:0.8437
Epoch 10 Batch 100 Loss 0.2692 Acc:0.8189
Epoch 10 Batch 200 Loss 0.2705 Acc:0.8256
Epoch 10 Batch 300 Loss 0.2724 Acc:0.8333
Epoch 10 Batch 400 Loss 0.3233 Acc:0.7969
Epoch 10 Batch 500 Loss 0.2822 Acc:0.8115
Epoch 10 Batch 600 Loss 0.2861 Acc:0.7984
Time taken for 1 epoch 404.6103 sec

Epoch 11 Batch 0 Loss 0.2095 Acc:0.8633
Epoch 11 Batch 100 Loss 0.2155 Acc:0.8743
Epoch 11 Batch 200 Loss 0.2788 Acc:0.8182
Epoch 11 Batch 300 Loss 0.2359 Acc:0.8660
Epoch 11 Batch 400 Loss 0.2279 Acc:0.8285
Epoch 11 Batch 500 Loss 0.2795 Acc:0.8436
Epoch 11 Batch 600 Loss 0.2480 Acc:0.8238
Time taken for 1 epoch 405.8043 sec

Epoch 12 Batch 0 Loss 0.1880 Acc:0.8763
Epoch 12 Batch 100 Loss 0.2248 Acc:0.8665
Epoch 12 Batch 200 Loss 0.2108 Acc:0.8656
Epoch 12 Batch 300 Loss 0.1966 Acc:0.8590
Epoch 12 Batch 400 Loss 0.2039 Acc:0.8560
Epoch 12 Batch 500 Loss 0.2260 Acc:0.8429
Epoch 12 Batch 600 Loss 0.2418 Acc:0.8442
Time taken for 1 epoch 404.0757 sec

</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Evaluate-the-model">
<a class="anchor" href="#Evaluate-the-model" aria-hidden="true"><span class="octicon octicon-link"></span></a>Evaluate the model<a class="anchor-link" href="#Evaluate-the-model"> </a>
</h3>
<p>When training is done, we get back the history and results, so we can explore them and plot our relevant metrics:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="p">(</span><span class="n">ax1</span><span class="p">,</span> <span class="n">ax2</span><span class="p">)</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>
<span class="c1"># plot some data</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">losses</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">'loss'</span><span class="p">)</span>
<span class="c1">#plt.plot(results.history['val_loss'], label='val_loss')</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">'Training Loss'</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="c1"># accuracies</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">accuracies</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">'acc'</span><span class="p">)</span>
<span class="c1">#plt.plot(results.history['val_accuracy_fn'], label='val_acc')</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">'Training Accuracy'</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,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%0A">
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Make-predictions">
<a class="anchor" href="#Make-predictions" aria-hidden="true"><span class="octicon octicon-link"></span></a>Make predictions<a class="anchor-link" href="#Make-predictions"> </a>
</h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>To restore the lastest checkpoint, saved model, you can run the following cell:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># restoring the latest checkpoint in checkpoint_dir</span>
<span class="n">checkpoint_dir</span> <span class="o">=</span> <span class="s1">'./training_ckpt_seq2seq'</span>
<span class="n">checkpoint_prefix</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">checkpoint_dir</span><span class="p">,</span> <span class="s2">"ckpt"</span><span class="p">)</span>
<span class="n">checkpoint</span><span class="o">.</span><span class="n">restore</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">latest_checkpoint</span><span class="p">(</span><span class="n">checkpoint_dir</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In the prediction step, our input is a secuence of length one, the sos token, then we call the encoder and decoder repeatedly until we get the eos token or reach the maximum length defined.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="n">input_text</span><span class="p">,</span> <span class="n">encoder</span><span class="p">,</span> <span class="n">input_max_len</span><span class="p">,</span> <span class="n">tokenizer_inputs</span><span class="p">,</span> <span class="n">word2idx_outputs</span><span class="p">,</span> <span class="n">idx2word_outputs</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">input_text</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">input_text</span> <span class="o">=</span> <span class="n">input_data</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">input_data</span><span class="p">))]</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">input_text</span><span class="p">)</span>
    <span class="c1"># Tokenize the input sequence</span>
    <span class="n">input_seq</span> <span class="o">=</span> <span class="n">tokenizer_inputs</span><span class="o">.</span><span class="n">texts_to_sequences</span><span class="p">([</span><span class="n">input_text</span><span class="p">])</span>
    <span class="c1"># Pad the sentence</span>
    <span class="n">input_seq</span> <span class="o">=</span> <span class="n">pad_sequences</span><span class="p">(</span><span class="n">input_seq</span><span class="p">,</span> <span class="n">maxlen</span><span class="o">=</span><span class="n">input_max_len</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s1">'post'</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">input_seq</span><span class="p">)</span>
    <span class="c1"># Set the encoder initial state</span>
    <span class="n">en_initial_states</span> <span class="o">=</span> <span class="n">encoder</span><span class="o">.</span><span class="n">init_states</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">en_outputs</span> <span class="o">=</span> <span class="n">encoder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="n">input_seq</span><span class="p">),</span> <span class="n">en_initial_states</span><span class="p">)</span>
    <span class="c1"># Create the decoder input, the sos token</span>
    <span class="n">de_input</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([[</span><span class="n">word2idx_outputs</span><span class="p">[</span><span class="s1">'&lt;sos&gt;'</span><span class="p">]]])</span>
    <span class="c1"># Set the decoder states to the encoder vector or encoder hidden state</span>
    <span class="n">de_state_h</span><span class="p">,</span> <span class="n">de_state_c</span> <span class="o">=</span> <span class="n">en_outputs</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>
    
    <span class="n">out_words</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
        <span class="c1"># Decode and get the output probabilities</span>
        <span class="n">de_output</span><span class="p">,</span> <span class="n">de_state_h</span><span class="p">,</span> <span class="n">de_state_c</span> <span class="o">=</span> <span class="n">decoder</span><span class="p">(</span>
            <span class="n">de_input</span><span class="p">,</span> <span class="p">(</span><span class="n">de_state_h</span><span class="p">,</span> <span class="n">de_state_c</span><span class="p">))</span>
        <span class="c1"># Select the word with the highest probability</span>
        <span class="n">de_input</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">de_output</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="c1"># Append the word to the predicted output</span>
        <span class="n">out_words</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">idx2word_outputs</span><span class="p">[</span><span class="n">de_input</span><span class="o">.</span><span class="n">numpy</span><span class="p">()[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]])</span>
        <span class="c1"># Finish when eos token is found or the max length is reached</span>
        <span class="k">if</span> <span class="n">out_words</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="s1">'&lt;eos&gt;'</span> <span class="ow">or</span> <span class="nb">len</span><span class="p">(</span><span class="n">out_words</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="mi">20</span><span class="p">:</span>
            <span class="k">break</span>

    <span class="nb">print</span><span class="p">(</span><span class="s1">' '</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">out_words</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>It is time to show how our model works with some simple examples:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">test_sents</span> <span class="o">=</span> <span class="p">[</span><span class="n">input_data</span><span class="p">[</span><span class="mi">10003</span><span class="p">],</span> <span class="n">input_data</span><span class="p">[</span><span class="mi">10120</span><span class="p">]]</span>
<span class="c1">#test_sents = [encoder_inputs[1000]]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">test_sents</span><span class="p">)</span>
<span class="k">for</span> <span class="n">test_sent</span> <span class="ow">in</span> <span class="n">test_sents</span><span class="p">:</span>
    <span class="n">predict</span><span class="p">(</span><span class="n">test_sent</span><span class="p">,</span> <span class="n">encoder</span><span class="p">,</span> <span class="n">input_max_len</span><span class="p">,</span> <span class="n">tokenizer_inputs</span><span class="p">,</span> <span class="n">word2idx_outputs</span><span class="p">,</span> <span class="n">idx2word_outputs</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>['we re not going .', 'why are you sad ?']
[[ 17  24  33 111   1   0   0   0   0   0]]
no vamos . &lt;eos&gt;
[[ 69  22   3 326   5   0   0   0   0   0]]
¿ por que estas triste ? &lt;eos&gt;
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="The-Attention-Mechanism">
<a class="anchor" href="#The-Attention-Mechanism" aria-hidden="true"><span class="octicon octicon-link"></span></a>The Attention Mechanism<a class="anchor-link" href="#The-Attention-Mechanism"> </a>
</h2>
<p>The previously described model based on RNNs has a serious problem when working with long sequences, because the information of the first tokens is lost or diluted as more tokens are processed. The context vector has been given the responsibility of encoding all the information in a given source sentence in to a vector of few hundred elements. it made it challenging for the models to deal with long sentences. A solution was proposed in Bahdanau et al., 2014 [4] and Luong et al., 2015,[5].</p>
<p>They introduce a technique called "Attention", which highly improved the quality of machine translation systems. Attention allows the model to focus on the relevant parts of the input sequence as needed, accessing to all the past hidden states of the encoder, instead of just the last one. At each decoding step, the decoder gets to look at any particular state of the encoder and can selectively pick out specific elements from that sequence to produce the output. We will focus on the Luong perspective.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Loung-Attention-layer">
<a class="anchor" href="#Loung-Attention-layer" aria-hidden="true"><span class="octicon octicon-link"></span></a>Loung Attention layer<a class="anchor-link" href="#Loung-Attention-layer"> </a>
</h3>
<p><img src="/BlogEms/images/copied_from_nb/images/luong_attention.PNG" alt="Alt" title="Attention Mechanism by Gabriel Loye [6]"></p>
<p>There are two relevant points to focus on:</p>
<ul>
<li>
<p>The <strong>alignment vector</strong>: is a vector with the same length that the input or source sequence and is computed at every time step of the decoder. Each of its values is the score (or the probability) of the corresponding word within the source sequence, they tell the decoder what to focus on at each time step.</p>
<p>There are three ways to calculate the alingment scores:</p>
<ul>
<li>
<em>Dot product</em>: we only need to take the hidden states of the encoder and multiply them by the hidden state of the decoder</li>
<li>
<em>General</em>: very similar to dot product but a weight matrix is included.</li>
<li>
<em>Concat</em>: the decoder hidden state and encoder hidden states are added together first before being passed through a Linear layer with an tanh activation function and finally multiply by a weight matrix.</li>
</ul>
</li>
</ul>
<p><img src="/BlogEms/images/copied_from_nb/images/formula_luong_attention.PNG" alt="Alt" title="Decoder output"></p>
<p>The alignment scores are softmaxed so that the weights will be between 0 to 1.</p>
<ul>
<li>The context vector: It's the weighted average sum of the encoder's output, the dot product of the alignment vector and the encoder's output.</li>
</ul>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">LuongAttention</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Model</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">rnn_size</span><span class="p">,</span> <span class="n">attention_func</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">LuongAttention</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attention_func</span> <span class="o">=</span> <span class="n">attention_func</span>

        <span class="k">if</span> <span class="n">attention_func</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">'dot'</span><span class="p">,</span> <span class="s1">'general'</span><span class="p">,</span> <span class="s1">'concat'</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s1">'Attention score must be either dot, general or concat.'</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">attention_func</span> <span class="o">==</span> <span class="s1">'general'</span><span class="p">:</span>
            <span class="c1"># General score function</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">wa</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">rnn_size</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">attention_func</span> <span class="o">==</span> <span class="s1">'concat'</span><span class="p">:</span>
            <span class="c1"># Concat score function</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">wa</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">rnn_size</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'tanh'</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">va</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">decoder_output</span><span class="p">,</span> <span class="n">encoder_output</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention_func</span> <span class="o">==</span> <span class="s1">'dot'</span><span class="p">:</span>
            <span class="c1"># Dot score function: decoder_output (dot) encoder_output</span>
            <span class="c1"># decoder_output has shape: (batch_size, 1, rnn_size)</span>
            <span class="c1"># encoder_output has shape: (batch_size, max_len, rnn_size)</span>
            <span class="c1"># =&gt; score has shape: (batch_size, 1, max_len)</span>
            <span class="n">score</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">decoder_output</span><span class="p">,</span> <span class="n">encoder_output</span><span class="p">,</span> <span class="n">transpose_b</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="c1"># (batch_size, 1, max_len)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention_func</span> <span class="o">==</span> <span class="s1">'general'</span><span class="p">:</span>
            <span class="c1"># General score function: decoder_output (dot) (Wa (dot) encoder_output)</span>
            <span class="c1"># decoder_output has shape: (batch_size, 1, rnn_size)</span>
            <span class="c1"># encoder_output has shape: (batch_size, max_len, rnn_size)</span>
            <span class="c1"># =&gt; score has shape: (batch_size, 1, max_len)</span>
            <span class="n">score</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">decoder_output</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">wa</span><span class="p">(</span>
                <span class="n">encoder_output</span><span class="p">),</span> <span class="n">transpose_b</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="c1">#(batch_size, 1, max_len)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention_func</span> <span class="o">==</span> <span class="s1">'concat'</span><span class="p">:</span>
            <span class="c1"># Concat score function: va (dot) tanh(Wa (dot) concat(decoder_output + encoder_output))</span>
            <span class="c1"># Decoder output must be broadcasted to encoder output's shape first</span>
            <span class="n">decoder_output</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span>
                <span class="n">decoder_output</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="n">encoder_output</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">1</span><span class="p">])</span> <span class="c1">#shape (batch size, max len,hidden_dim)</span>

            <span class="c1"># Concat =&gt; Wa =&gt; va</span>
            <span class="c1"># (batch_size, max_len, 2 * rnn_size) =&gt; (batch_size, max_len, rnn_size) =&gt; (batch_size, max_len, 1)</span>
            <span class="n">score</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">va</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">wa</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">((</span><span class="n">decoder_output</span><span class="p">,</span> <span class="n">encoder_output</span><span class="p">),</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)))</span> <span class="c1"># (batch_size, max len, 1)</span>

            <span class="c1"># Transpose score vector to have the same shape as other two above</span>
            <span class="c1"># (batch_size, max_len, 1) =&gt; (batch_size, 1, max_len)</span>
            <span class="n">score</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">score</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span> <span class="c1">#(batch_size, 1, max_len)</span>

        <span class="c1"># alignment a_t = softmax(score)</span>
        <span class="n">alignment</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">activations</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">score</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span> <span class="c1">#(batch_size, 1, max_len)</span>
        
        <span class="c1"># context vector c_t is the weighted average sum of encoder output</span>
        <span class="n">context</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">alignment</span><span class="p">,</span> <span class="n">encoder_output</span><span class="p">)</span> <span class="c1"># (batch_size, 1, hidden_dim)</span>

        <span class="k">return</span> <span class="n">context</span><span class="p">,</span> <span class="n">alignment</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Decoder-with-Attention">
<a class="anchor" href="#Decoder-with-Attention" aria-hidden="true"><span class="octicon octicon-link"></span></a>Decoder with Attention<a class="anchor-link" href="#Decoder-with-Attention"> </a>
</h3>
<p>Once our Attention Class has been defined, we can create the decoder. The complete sequence of steps when calling the decoder are:</p>
<ul>
<li>Generate the encoder hidden states as usual, one for every input token</li>
<li>Apply a RNN to produce a new hidden state, taking its previous hidden state and the target output from the previous time step</li>
<li>Calculate the alignment scores as described previously</li>
<li>Calculate the context vector</li>
<li>In the last operation, the context vector is concatenated with the decoder hidden state we generated previously, then it is passed through a linear layer which acts as a classifier for us to obtain the probability scores of the next predicted word</li>
</ul>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">Decoder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Model</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">,</span> <span class="n">attention_func</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Decoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attention</span> <span class="o">=</span> <span class="n">LuongAttention</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">attention_func</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_dim</span> <span class="o">=</span> <span class="n">hidden_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lstm</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">LSTM</span><span class="p">(</span>
            <span class="n">hidden_dim</span><span class="p">,</span> <span class="n">return_sequences</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">return_state</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">wc</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'tanh'</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ws</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_sequence</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">encoder_output</span><span class="p">):</span>
        <span class="c1"># Remember that the input to the decoder</span>
        <span class="c1"># is now a batch of one-word sequences,</span>
        <span class="c1"># which means that its shape is (batch_size, 1)</span>
        <span class="n">embed</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span><span class="p">(</span><span class="n">input_sequence</span><span class="p">)</span>

        <span class="c1"># Therefore, the lstm_out has shape (batch_size, 1, hidden_dim)</span>
        <span class="n">lstm_out</span><span class="p">,</span> <span class="n">state_h</span><span class="p">,</span> <span class="n">state_c</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lstm</span><span class="p">(</span><span class="n">embed</span><span class="p">,</span> <span class="n">initial_state</span><span class="o">=</span><span class="n">state</span><span class="p">)</span>

        <span class="c1"># Use self.attention to compute the context and alignment vectors</span>
        <span class="c1"># context vector's shape: (batch_size, 1, hidden_dim)</span>
        <span class="c1"># alignment vector's shape: (batch_size, 1, source_length)</span>
        <span class="n">context</span><span class="p">,</span> <span class="n">alignment</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="p">(</span><span class="n">lstm_out</span><span class="p">,</span> <span class="n">encoder_output</span><span class="p">)</span>

        <span class="c1"># Combine the context vector and the LSTM output</span>
        <span class="c1"># Before combined, both have shape of (batch_size, 1, hidden_dim),</span>
        <span class="c1"># so let's squeeze the axis 1 first</span>
        <span class="c1"># After combined, it will have shape of (batch_size, 2 * hidden_dim)</span>
        <span class="n">lstm_out</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span>
            <span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">context</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">tf</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">lstm_out</span><span class="p">,</span> <span class="mi">1</span><span class="p">)],</span> <span class="mi">1</span><span class="p">)</span>

        <span class="c1"># lstm_out now has shape (batch_size, hidden_dim)</span>
        <span class="n">lstm_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">wc</span><span class="p">(</span><span class="n">lstm_out</span><span class="p">)</span>

        <span class="c1"># Finally, it is converted back to vocabulary space: (batch_size, vocab_size)</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ws</span><span class="p">(</span><span class="n">lstm_out</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">logits</span><span class="p">,</span> <span class="n">state_h</span><span class="p">,</span> <span class="n">state_c</span><span class="p">,</span> <span class="n">alignment</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>For testing purposes, we create a decoder and call it to check the output shapes:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1">#Set the length of the input and output vocabulary</span>
<span class="n">num_words_inputs</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">word2idx_inputs</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>
<span class="n">num_words_output</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">word2idx_outputs</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>
<span class="c1">#Create the encoder</span>
<span class="n">encoder</span> <span class="o">=</span> <span class="n">Encoder</span><span class="p">(</span><span class="n">num_words_inputs</span><span class="p">,</span> <span class="n">EMBEDDING_DIM</span><span class="p">,</span> <span class="n">HIDDEN_DIM</span><span class="p">)</span>
<span class="n">decoder</span> <span class="o">=</span> <span class="n">Decoder</span><span class="p">(</span><span class="n">num_words_output</span><span class="p">,</span> <span class="n">EMBEDDING_DIM</span><span class="p">,</span> <span class="n">HIDDEN_DIM</span><span class="p">,</span> <span class="n">ATTENTION_FUNC</span><span class="p">)</span>

<span class="c1"># Call the encoder and then the decoder</span>
<span class="n">initial_state</span> <span class="o">=</span> <span class="n">encoder</span><span class="o">.</span><span class="n">init_states</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">encoder_outputs</span> <span class="o">=</span> <span class="n">encoder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([[</span><span class="mi">1</span><span class="p">]]),</span> <span class="n">initial_state</span><span class="p">)</span>
<span class="n">decoder_outputs</span> <span class="o">=</span> <span class="n">decoder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span>
    <span class="p">[[</span><span class="mi">1</span><span class="p">]]),</span> <span class="n">encoder_outputs</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="n">encoder_outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Train-step-function">
<a class="anchor" href="#Train-step-function" aria-hidden="true"><span class="octicon octicon-link"></span></a>Train step function<a class="anchor-link" href="#Train-step-function"> </a>
</h3>
<p>Now we can define our step train function, to train a batch data. It is very similar to the one we coded for the seq2seq model without attention but this time we pass all the hidden states returned by the encoder to the decoder. And we need to create a loop to iterate through the target sequences, calling the decoder for each one and calculating the loss function comparing the decoder output to the expected target.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nd">@tf</span><span class="o">.</span><span class="n">function</span>
<span class="k">def</span> <span class="nf">train_step</span><span class="p">(</span><span class="n">input_seq</span><span class="p">,</span> <span class="n">target_seq_in</span><span class="p">,</span> <span class="n">target_seq_out</span><span class="p">,</span> <span class="n">en_initial_states</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">):</span>
    <span class="sd">''' A training step, train a batch of the data and return the loss value reached</span>
<span class="sd">        Input:</span>
<span class="sd">        - input_seq: array of integers, shape [batch_size, max_seq_len, embedding dim].</span>
<span class="sd">            the input sequence</span>
<span class="sd">        - target_seq_out: array of integers, shape [batch_size, max_seq_len, embedding dim].</span>
<span class="sd">            the target seq, our target sequence</span>
<span class="sd">        - target_seq_in: array of integers, shape [batch_size, max_seq_len, embedding dim].</span>
<span class="sd">            the input sequence to the decoder, we use Teacher Forcing</span>
<span class="sd">        - en_initial_states: tuple of arrays of shape [batch_size, hidden_dim].</span>
<span class="sd">            the initial state of the encoder</span>
<span class="sd">        - optimizer: a tf.keras.optimizers.</span>
<span class="sd">        Output:</span>
<span class="sd">        - loss: loss value</span>
<span class="sd">        </span>
<span class="sd">    '''</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="mf">0.</span>
    <span class="n">acc</span> <span class="o">=</span> <span class="mf">0.</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="kc">None</span>
    
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">tape</span><span class="p">:</span>
        <span class="n">en_outputs</span> <span class="o">=</span> <span class="n">encoder</span><span class="p">(</span><span class="n">input_seq</span><span class="p">,</span> <span class="n">en_initial_states</span><span class="p">)</span>
        <span class="n">en_states</span> <span class="o">=</span> <span class="n">en_outputs</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>
        <span class="n">de_state_h</span><span class="p">,</span> <span class="n">de_state_c</span> <span class="o">=</span> <span class="n">en_states</span>

        <span class="c1"># We need to create a loop to iterate through the target sequences</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">target_seq_out</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
            <span class="c1"># Input to the decoder must have shape of (batch_size, length)</span>
            <span class="c1"># so we need to expand one dimension</span>
            <span class="n">decoder_in</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">target_seq_in</span><span class="p">[:,</span> <span class="n">i</span><span class="p">],</span> <span class="mi">1</span><span class="p">)</span>
            <span class="n">logit</span><span class="p">,</span> <span class="n">de_state_h</span><span class="p">,</span> <span class="n">de_state_c</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">decoder</span><span class="p">(</span>
                <span class="n">decoder_in</span><span class="p">,</span> <span class="p">(</span><span class="n">de_state_h</span><span class="p">,</span> <span class="n">de_state_c</span><span class="p">),</span> <span class="n">en_outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

            <span class="c1"># The loss is now accumulated through the whole batch</span>
            <span class="n">loss</span> <span class="o">+=</span> <span class="n">loss_func</span><span class="p">(</span><span class="n">target_seq_out</span><span class="p">[:,</span> <span class="n">i</span><span class="p">],</span> <span class="n">logit</span><span class="p">)</span>
            <span class="c1"># Store the logits to calculate the accuracy</span>
            <span class="n">logit</span> <span class="o">=</span> <span class="n">K</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">logit</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">logits</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">logits</span> <span class="o">=</span> <span class="n">logit</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">logits</span> <span class="o">=</span> <span class="n">K</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">logits</span><span class="p">,</span><span class="n">logit</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="c1"># Calculate the accuracy for the batch data        </span>
        <span class="n">acc</span> <span class="o">=</span> <span class="n">accuracy_fn</span><span class="p">(</span><span class="n">target_seq_out</span><span class="p">,</span> <span class="n">logits</span><span class="p">)</span>
    <span class="c1"># Update the parameters and the optimizer</span>
    <span class="n">variables</span> <span class="o">=</span> <span class="n">encoder</span><span class="o">.</span><span class="n">trainable_variables</span> <span class="o">+</span> <span class="n">decoder</span><span class="o">.</span><span class="n">trainable_variables</span>
    <span class="n">gradients</span> <span class="o">=</span> <span class="n">tape</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">variables</span><span class="p">)</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">apply_gradients</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">gradients</span><span class="p">,</span> <span class="n">variables</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">loss</span> <span class="o">/</span> <span class="n">target_seq_out</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">acc</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Main-train">
<a class="anchor" href="#Main-train" aria-hidden="true"><span class="octicon octicon-link"></span></a>Main train<a class="anchor-link" href="#Main-train"> </a>
</h3>
</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Create an Adam optimizer and clips gradients by norm</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">clipnorm</span><span class="o">=</span><span class="mf">5.0</span><span class="p">)</span>
<span class="c1"># Create a checkpoint object to save the model</span>
<span class="n">checkpoint_dir</span> <span class="o">=</span> <span class="s1">'./training_ckpt_seq2seq_att'</span>
<span class="n">checkpoint_prefix</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">checkpoint_dir</span><span class="p">,</span> <span class="s2">"ckpt"</span><span class="p">)</span>
<span class="n">checkpoint</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">Checkpoint</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">,</span>
                                 <span class="n">encoder</span><span class="o">=</span><span class="n">encoder</span><span class="p">,</span>
                                 <span class="n">decoder</span><span class="o">=</span><span class="n">decoder</span><span class="p">)</span>

<span class="n">losses</span><span class="p">,</span> <span class="n">accuracies</span> <span class="o">=</span> <span class="n">main_train</span><span class="p">(</span><span class="n">encoder</span><span class="p">,</span> <span class="n">decoder</span><span class="p">,</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">EPOCHS</span><span class="p">,</span> <span class="n">BATCH_SIZE</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">checkpoint</span><span class="p">,</span> <span class="n">checkpoint_prefix</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Epoch 1 Batch 0 Loss 3.0794 Acc:0.0000
Epoch 1 Batch 100 Loss 1.5153 Acc:0.4106
Epoch 1 Batch 200 Loss 1.3571 Acc:0.4490
Epoch 1 Batch 300 Loss 1.1166 Acc:0.5296
Time taken for 1 epoch 682.8766 sec

Epoch 2 Batch 0 Loss 1.1873 Acc:0.4848
Epoch 2 Batch 100 Loss 0.9923 Acc:0.5562
Epoch 2 Batch 200 Loss 1.0025 Acc:0.5470
Epoch 2 Batch 300 Loss 1.0277 Acc:0.5510
Time taken for 1 epoch 656.4251 sec

Epoch 3 Batch 0 Loss 0.8298 Acc:0.5680
Epoch 3 Batch 100 Loss 0.7965 Acc:0.6073
Epoch 3 Batch 200 Loss 0.8324 Acc:0.6023
Epoch 3 Batch 300 Loss 0.8243 Acc:0.6059
Time taken for 1 epoch 650.9733 sec

Epoch 4 Batch 0 Loss 0.6563 Acc:0.6423
Epoch 4 Batch 100 Loss 0.6277 Acc:0.6714
Epoch 4 Batch 200 Loss 0.6086 Acc:0.6497
Epoch 4 Batch 300 Loss 0.5977 Acc:0.6833
Time taken for 1 epoch 657.5127 sec

Epoch 5 Batch 0 Loss 0.4388 Acc:0.7050
Epoch 5 Batch 100 Loss 0.5328 Acc:0.6919
Epoch 5 Batch 200 Loss 0.5073 Acc:0.6619
Epoch 5 Batch 300 Loss 0.4655 Acc:0.6946
Time taken for 1 epoch 655.9225 sec

Epoch 6 Batch 0 Loss 0.3079 Acc:0.7917
Epoch 6 Batch 100 Loss 0.4277 Acc:0.7337
Epoch 6 Batch 200 Loss 0.4378 Acc:0.7390
Epoch 6 Batch 300 Loss 0.3456 Acc:0.7735
Time taken for 1 epoch 658.4363 sec

Epoch 7 Batch 0 Loss 0.2302 Acc:0.8353
Epoch 7 Batch 100 Loss 0.2826 Acc:0.7977
Epoch 7 Batch 200 Loss 0.2620 Acc:0.7975
Epoch 7 Batch 300 Loss 0.3166 Acc:0.7826
Time taken for 1 epoch 644.7999 sec

Epoch 8 Batch 0 Loss 0.2196 Acc:0.8529
Epoch 8 Batch 100 Loss 0.2533 Acc:0.8128
Epoch 8 Batch 200 Loss 0.2885 Acc:0.7924
Epoch 8 Batch 300 Loss 0.3511 Acc:0.8093
Time taken for 1 epoch 650.8827 sec

Epoch 9 Batch 0 Loss 0.1434 Acc:0.8690
Epoch 9 Batch 100 Loss 0.2054 Acc:0.8629
Epoch 9 Batch 200 Loss 0.2266 Acc:0.8220
Epoch 9 Batch 300 Loss 0.1910 Acc:0.8663
Time taken for 1 epoch 645.2892 sec

Epoch 10 Batch 0 Loss 0.1330 Acc:0.8950
Epoch 10 Batch 100 Loss 0.1828 Acc:0.8692
Epoch 10 Batch 200 Loss 0.1890 Acc:0.8563
Epoch 10 Batch 300 Loss 0.1998 Acc:0.8806
Time taken for 1 epoch 650.5875 sec

</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Evaluate-the-model">
<a class="anchor" href="#Evaluate-the-model" aria-hidden="true"><span class="octicon octicon-link"></span></a>Evaluate the model<a class="anchor-link" href="#Evaluate-the-model"> </a>
</h3>
<p>When training is done, we can plot the losses and accuracies obtained during training:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="p">(</span><span class="n">ax1</span><span class="p">,</span> <span class="n">ax2</span><span class="p">)</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>
<span class="c1"># plot some data</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">losses</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">'loss'</span><span class="p">)</span>
<span class="c1">#plt.plot(results.history['val_loss'], label='val_loss')</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">'Training Loss'</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="c1"># accuracies</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">accuracies</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">'acc'</span><span class="p">)</span>
<span class="c1">#plt.plot(results.history['val_accuracy_fn'], label='val_acc')</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">'Training Accuracy'</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,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%0A">
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Prediction-or-inference">
<a class="anchor" href="#Prediction-or-inference" aria-hidden="true"><span class="octicon octicon-link"></span></a>Prediction or inference<a class="anchor-link" href="#Prediction-or-inference"> </a>
</h3>
<p>We can restore the latest checkpoint of our model before making some predictions:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Create an Adam optimizer and clips gradients by norm</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">clipnorm</span><span class="o">=</span><span class="mf">5.0</span><span class="p">)</span>
<span class="c1"># Create a checkpoint object to save the model</span>
<span class="n">checkpoint_dir</span> <span class="o">=</span> <span class="s1">'./training_ckpt_seq2seq_att'</span>
<span class="n">checkpoint_prefix</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">checkpoint_dir</span><span class="p">,</span> <span class="s2">"ckpt"</span><span class="p">)</span>
<span class="n">checkpoint</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">Checkpoint</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">,</span>
                                 <span class="n">encoder</span><span class="o">=</span><span class="n">encoder</span><span class="p">,</span>
                                 <span class="n">decoder</span><span class="o">=</span><span class="n">decoder</span><span class="p">)</span>

<span class="n">checkpoint</span><span class="o">.</span><span class="n">restore</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">latest_checkpoint</span><span class="p">(</span><span class="n">checkpoint_dir</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>&lt;tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f5916e835f8&gt;</pre>
</div>

</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">predict_seq2seq_att</span><span class="p">(</span><span class="n">input_text</span><span class="p">,</span> <span class="n">input_max_len</span><span class="p">,</span> <span class="n">tokenizer_inputs</span><span class="p">,</span> <span class="n">word2idx_outputs</span><span class="p">,</span> <span class="n">idx2word_outputs</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">input_text</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">input_text</span> <span class="o">=</span> <span class="n">input_data</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">input_data</span><span class="p">))]</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">input_text</span><span class="p">)</span>
    <span class="c1"># Tokenize the input text</span>
    <span class="n">input_seq</span> <span class="o">=</span> <span class="n">tokenizer_inputs</span><span class="o">.</span><span class="n">texts_to_sequences</span><span class="p">([</span><span class="n">input_text</span><span class="p">])</span>
    <span class="c1"># Pad the sentence</span>
    <span class="n">input_seq</span> <span class="o">=</span> <span class="n">pad_sequences</span><span class="p">(</span><span class="n">input_seq</span><span class="p">,</span> <span class="n">maxlen</span><span class="o">=</span><span class="n">input_max_len</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s1">'post'</span><span class="p">)</span>
    <span class="c1"># Get the encoder initial states</span>
    <span class="n">en_initial_states</span> <span class="o">=</span> <span class="n">encoder</span><span class="o">.</span><span class="n">init_states</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="c1"># Get the encoder outputs or hidden states</span>
    <span class="n">en_outputs</span> <span class="o">=</span> <span class="n">encoder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="n">input_seq</span><span class="p">),</span> <span class="n">en_initial_states</span><span class="p">)</span>
    <span class="c1"># Set the decoder input to the sos token</span>
    <span class="n">de_input</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([[</span><span class="n">word2idx_outputs</span><span class="p">[</span><span class="s1">'&lt;sos&gt;'</span><span class="p">]]])</span>
    <span class="c1"># Set the initial hidden states of the decoder to the hidden states of the encoder</span>
    <span class="n">de_state_h</span><span class="p">,</span> <span class="n">de_state_c</span> <span class="o">=</span> <span class="n">en_outputs</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>
    
    <span class="n">out_words</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">alignments</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
        <span class="c1"># Get the decoder with attention output</span>
        <span class="n">de_output</span><span class="p">,</span> <span class="n">de_state_h</span><span class="p">,</span> <span class="n">de_state_c</span><span class="p">,</span> <span class="n">alignment</span> <span class="o">=</span> <span class="n">decoder</span><span class="p">(</span>
            <span class="n">de_input</span><span class="p">,</span> <span class="p">(</span><span class="n">de_state_h</span><span class="p">,</span> <span class="n">de_state_c</span><span class="p">),</span> <span class="n">en_outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">de_input</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">de_output</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="mi">0</span><span class="p">)</span>
        <span class="c1"># Detokenize the output</span>
        <span class="n">out_words</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">idx2word_outputs</span><span class="p">[</span><span class="n">de_input</span><span class="o">.</span><span class="n">numpy</span><span class="p">()[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]])</span>
        <span class="c1"># Save the aligment matrix</span>
        <span class="n">alignments</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">alignment</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>

        <span class="k">if</span> <span class="n">out_words</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="s1">'&lt;eos&gt;'</span> <span class="ow">or</span> <span class="nb">len</span><span class="p">(</span><span class="n">out_words</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="mi">20</span><span class="p">:</span>
            <span class="k">break</span>
    <span class="c1"># Join the output words</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">' '</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">out_words</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">alignments</span><span class="p">),</span> <span class="n">input_text</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">' '</span><span class="p">),</span> <span class="n">out_words</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>It is time to test out model, making some predictions or doing some translation from english to spanish</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">n_predictions</span><span class="o">=</span><span class="mi">1</span>
<span class="n">test_sents</span> <span class="o">=</span> <span class="n">input_data</span><span class="p">[</span><span class="mi">15005</span><span class="p">:(</span><span class="mi">15005</span><span class="o">+</span><span class="n">n_predictions</span><span class="p">)]</span>

<span class="c1"># Create the figure to plot in</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">test_sent</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">test_sents</span><span class="p">):</span>
    <span class="c1"># Call the predict function to get the translation</span>
    <span class="n">alignments</span><span class="p">,</span> <span class="n">source</span><span class="p">,</span> <span class="n">prediction</span> <span class="o">=</span> <span class="n">predict_seq2seq_att</span><span class="p">(</span><span class="n">test_sent</span><span class="p">,</span> <span class="n">input_max_len</span><span class="p">,</span> <span class="n">tokenizer_inputs</span><span class="p">,</span> 
                                                     <span class="n">word2idx_outputs</span><span class="p">,</span> <span class="n">idx2word_outputs</span><span class="p">)</span>
    <span class="n">attention</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">alignments</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
    <span class="c1"># Create a subplot</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_predictions</span><span class="p">,</span> <span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">matshow</span><span class="p">(</span><span class="n">attention</span><span class="p">[:</span><span class="nb">len</span><span class="p">(</span><span class="n">prediction</span><span class="p">),</span> <span class="p">:</span><span class="nb">len</span><span class="p">(</span><span class="n">source</span><span class="p">)],</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">'viridis'</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">([</span><span class="s1">''</span><span class="p">]</span> <span class="o">+</span> <span class="n">source</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">90</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_yticklabels</span><span class="p">([</span><span class="s1">''</span><span class="p">]</span> <span class="o">+</span> <span class="n">prediction</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>she got him drunk .
ella le emborracho . &lt;eos&gt;
</pre>
</div>
</div>

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,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%0A">
</div>

</div>

</div>
</div>

</div>
    

</div>



  </div><a class="u-url" href="/BlogEms/fastpages/jupyter/encoder-decoder/lstm/attention/tensorflow%202/2020/10/07/Intro-seq2seq-Encoder-Decoder-ENG-SPA-translator-tf2.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/BlogEms/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/BlogEms/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/BlogEms/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>An easy to use blogging platform with support for Jupyter Notebooks.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/edumunozsala" title="edumunozsala"><svg class="svg-icon grey"><use xlink:href="/BlogEms/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/emunozsala" title="emunozsala"><svg class="svg-icon grey"><use xlink:href="/BlogEms/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
